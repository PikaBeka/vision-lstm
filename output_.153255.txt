MASTER_ADDR: gpu-07
CUDA_VISIBLE_DEVICES=0,1,2,3
Wed Nov  5 16:55:11 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   25C    P0             50W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   26C    P0             51W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   25C    P0             53W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   26C    P0             51W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
torch: 2.5.1+cu121 cuda: 12.1 cuda available: True
11-05 16:55:26 I initializing rank=0 local_rank=0 nodes=1 hostname=gpu-07 master_addr=gpu-07 master_port=55555 (waiting for all 4 processes to connect)
11-05 16:55:26 I initializing rank=2 local_rank=2 nodes=1 hostname=gpu-07 master_addr=gpu-07 master_port=55555 (waiting for all 4 processes to connect)
11-05 16:55:26 I initializing rank=3 local_rank=3 nodes=1 hostname=gpu-07 master_addr=gpu-07 master_port=55555 (waiting for all 4 processes to connect)
11-05 16:55:26 I initializing rank=1 local_rank=1 nodes=1 hostname=gpu-07 master_addr=gpu-07 master_port=55555 (waiting for all 4 processes to connect)
[rank1]:[W1105 16:55:26.647842406 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W1105 16:55:26.647850406 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W1105 16:55:26.647960644 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W1105 16:55:26.648001174 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
11-05 16:55:27 I initialized process rank=0 local_rank=0 pid=808912
11-05 16:55:27 I initialized process rank=1 local_rank=1 pid=808913
11-05 16:55:27 I initialized process rank=3 local_rank=3 pid=808915
11-05 16:55:27 I initialized process rank=2 local_rank=2 pid=808914
11-05 16:55:27 I initialized 4 processes
11-05 16:55:27 W disabled cudnn benchmark
11-05 16:55:27 W enabled cudnn deterministic
11-05 16:55:27 I log file: /home/beknur.kalmakhanbet/save/in1k/1bv42fx3/log.txt
11-05 16:55:27 I no seed specified -> using seed=0
11-05 16:55:27 I ------------------
11-05 16:55:27 I initializing wandb (mode=online)
11-05 16:55:27 I logging into wandb (host=https://api.wandb.ai/ rank=0)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
wandb: Currently logged in as: beka-kalmahanbet (ml710_project) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
11-05 16:55:28 I logged into wandb (host=https://api.wandb.ai/)
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/beknur.kalmakhanbet/save/in1k/1bv42fx3/wandb/run-20251105_165528-1bv42fx3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ddgjaz5t--e5res224/in1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/beka-kalmahanbet-mbzuai/minLSTM
wandb: üöÄ View run at https://wandb.ai/beka-kalmahanbet-mbzuai/minLSTM/runs/1bv42fx3
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
11-05 16:55:29 I ------------------
11-05 16:55:29 I stage_id: 1bv42fx3
11-05 16:55:29 I python main_train.py --hp src/vislstm/yamls/pretrain/vil/80M_res224finetuning_e5.yaml --num_workers 5
11-05 16:55:29 I ------------------
11-05 16:55:29 I VERSION CHECK
11-05 16:55:29 I executable: /home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/bin/python
11-05 16:55:29 I python version: 3.9.21
11-05 16:55:29 I torch version: 2.5.1+cu121
11-05 16:55:29 I torch.cuda version: 12.1
11-05 16:55:29 I torchvision.version: 0.20.1+cu121
11-05 16:55:30 I torchmetrics version: 1.6.2
11-05 16:55:30 I kappaschedules version: 0.0.31
11-05 16:55:30 I kappamodules version: 0.1.76
11-05 16:55:30 I ------------------
11-05 16:55:30 I SYSTEM INFO
11-05 16:55:30 I host name: gpu-07
11-05 16:55:30 I OS: Linux-5.15.161-ql-generic-13.0-14-x86_64-with-glibc2.35
11-05 16:55:30 I OS version: #1 SMP Wed Jun 26 16:19:39 UTC 2024
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
11-05 16:55:31 I initialized process rank=2 local_rank=2 pid=808914 hostname=gpu-07
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
11-05 16:55:31 I initialized process rank=1 local_rank=1 pid=808913 hostname=gpu-07
11-05 16:55:31 I initialized process rank=3 local_rank=3 pid=808915 hostname=gpu-07
11-05 16:55:32 I CUDA version: 12.4
11-05 16:55:32 I current commit hash: 879894a2c4205819466aaff45b583fe3b517c036
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
11-05 16:55:32 I latest git tag: 
11-05 16:55:32 I initialized process rank=0 local_rank=0 pid=808912 hostname=gpu-07
11-05 16:55:32 I total_cpu_count: 64
11-05 16:55:32 I ------------------
11-05 16:55:32 I STATIC CONFIG
11-05 16:55:32 I account_name: beknur.kalmakhanbet
11-05 16:55:32 I output_path: /home/beknur.kalmakhanbet/save
11-05 16:55:32 I ------------------
11-05 16:55:32 I CLI ARGS
11-05 16:55:32 I hp: src/vislstm/yamls/pretrain/vil/80M_res224finetuning_e5.yaml
11-05 16:55:32 I accelerator: gpu
11-05 16:55:32 I num_workers: 5
11-05 16:55:32 I testrun: False
11-05 16:55:32 I minmodelrun: False
11-05 16:55:32 I mindatarun: False
11-05 16:55:32 I mindurationrun: False
11-05 16:55:32 I static_config_uri: static_config.yaml
11-05 16:55:32 I ------------------
11-05 16:55:32 I DIST CONFIG
11-05 16:55:32 I rank: 0
11-05 16:55:32 I local_rank: 0
11-05 16:55:32 I world_size: 4
11-05 16:55:32 I nodes: 1
11-05 16:55:32 I backend: nccl
11-05 16:55:32 I slurm job id: 153255
11-05 16:55:32 I hostnames: gpu-07
11-05 16:55:32 I ------------------
master_factory_base_path: vislstm
stage_name: in1k
datasets:
  train:
    kind: imagenet1k
    split: train
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: random_resized_crop
        size: 224
        scale:
        - 0.08
        - 1.0
        interpolation: bicubic
      - kind: random_horizontal_flip
      - kind: transforms.three_augment
        blur_sigma:
        - 0.1
        - 2.0
      - kind: color_jitter
        brightness: 0.3
        contrast: 0.3
        saturation: 0.3
        hue: 0.0
      - kind: imagenet1k_norm
    - kind: one_hot_wrapper
    collators:
    - kind: mix_collator
      mixup_alpha: 0.8
      cutmix_alpha: 1.0
      mixup_p: 0.5
      cutmix_p: 0.5
      apply_mode: batch
      lamb_mode: batch
      shuffle_mode: flip
  val:
    kind: imagenet1k
    split: val
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: resize
        size: 224
        interpolation: bicubic
      - kind: center_crop
        size: 224
      - kind: imagenet1k_norm
model:
  initializers:
  - kind: previous_run_initializer
    stage_id: ddgjaz5t
    stage_name: in1k
    model_name: vislstm
    checkpoint: last
    use_checkpoint_kwargs: true
  optim:
    kind: adamw
    lr: 1.0e-05
    betas:
    - 0.9
    - 0.999
    weight_decay: 0.05
    schedule:
      kind: linear_warmup_cosine_decay_schedule
      warmup_epochs: 1
      end_value: 1.0e-06
    lr_scaler:
      kind: linear_lr_scaler
      divisor: 1024
trainer:
  kind: classification_trainer
  precision: bfloat16
  backup_precision: float16
  max_epochs: 5
  effective_batch_size: 256
  log_every_n_epochs: 1
  callbacks:
  - kind: checkpoint_callback
  - kind: checkpoint_callback
    every_n_epochs: 10
    save_latest_weights: true
    save_latest_optim: true
  - kind: offline_accuracy_callback
    every_n_epochs: 1
    dataset_key: val
11-05 16:55:32 I copied unresolved hp to /home/beknur.kalmakhanbet/save/in1k/1bv42fx3/hp_unresolved.yaml
11-05 16:55:32 I dumped resolved hp to /home/beknur.kalmakhanbet/save/in1k/1bv42fx3/hp_resolved.yaml
11-05 16:55:32 I ------------------
11-05 16:55:32 I training stage 'in1k'
11-05 16:55:32 I using different seeds per process (seed+rank)
11-05 16:55:32 I set seed to 0
11-05 16:55:32 I ------------------
11-05 16:55:32 I initializing datasets
11-05 16:55:32 I initializing train
11-05 16:55:36 I instantiating sample_wrapper x_transform_wrapper
11-05 16:55:36 I instantiating sample_wrapper one_hot_wrapper
11-05 16:55:36 I initializing val
11-05 16:55:38 I instantiating sample_wrapper x_transform_wrapper
11-05 16:55:38 I ------------------
11-05 16:55:38 I initializing trainer
11-05 16:55:38 I using precision: torch.bfloat16 (desired=bfloat16 backup=float16)
11-05 16:55:38 I main_sampler: DistributedSampler(num_repeats=1, shuffle=True)
11-05 16:55:38 I ------------------
11-05 16:55:38 I creating model
11-05 16:55:38 I input_shape: (3, 224, 224)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/base/checkpoint_initializer.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=torch.device("cpu"))
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/base/checkpoint_initializer.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=torch.device("cpu"))
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/base/checkpoint_initializer.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=torch.device("cpu"))
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/base/checkpoint_initializer.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=torch.device("cpu"))
11-05 16:55:39 I loaded model kwargs from /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=last model.th
11-05 16:55:39 I loaded model kwargs: {'patch_size': 16, 'dim': 768, 'depth': 24, 'bidirectional': False, 'alternation': 'bidirectional', 'conv1d_kernel_size': 3, 'use_conv2d': True, 'bias': True, 'pos_embed_mode': 'learnable', 'drop_path_rate': 0.2, 'drop_path_decay': False, 'mode': 'classifier', 'pooling': {'kind': 'bilateral', 'aggregate': 'flatten'}, 'kind': 'models.single.vislstm'}
11-05 16:55:39 I postprocessed checkpoint kwargs:
initializers:
- kind: previous_run_initializer
  stage_id: ddgjaz5t
  stage_name: in1k
  model_name: vislstm
  checkpoint: last
optim:
  kind: adamw
  lr: 1.0e-05
  betas:
  - 0.9
  - 0.999
  weight_decay: 0.05
  schedule:
    kind: linear_warmup_cosine_decay_schedule
    warmup_epochs: 1
    end_value: 1.0e-06
  lr_scaler:
    kind: linear_lr_scaler
    divisor: 1024
patch_size: 16
dim: 768
depth: 24
bidirectional: false
alternation: bidirectional
conv1d_kernel_size: 3
use_conv2d: true
bias: true
pos_embed_mode: learnable
drop_path_rate: 0.2
drop_path_decay: false
mode: classifier
pooling:
  kind: bilateral
  aggregate: flatten
11-05 16:55:39 I pos_embed.is_learnable=True
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
11-05 16:55:41 I drop_path_rate: 0.2
11-05 16:55:41 I model:
VisLSTM(
  (pooling): Bilateral(aggregate=flatten)
  (patch_embed): VitPatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_embed): VitPosEmbed2d()
  (xlstm): xLSTMBlockStack(
    (blocks): ModuleList(
      (0-23): 24 x mLSTMBlock(
        (drop_path1): DropPath(drop_prob=0.200)
        (xlstm_norm): LayerNorm()
        (xlstm): mLSTMLayer(
          (proj_up): Linear(in_features=768, out_features=3072, bias=True)
          (conv1d): SequenceConv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
          (conv_act_fn): SiLU()
          (mlstm_cell): mLSTMCell(
            (linear_h): FeedForward(
              (linear1): Linear(in_features=1536, out_features=8, bias=True)
              (activation): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=8, out_features=1536, bias=True)
            )
            (linear_i): FeedForward(
              (linear1): Linear(in_features=1536, out_features=8, bias=True)
              (activation): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=8, out_features=1536, bias=True)
            )
            (linear_f): FeedForward(
              (linear1): Linear(in_features=1536, out_features=8, bias=True)
              (activation): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=8, out_features=1536, bias=True)
            )
          )
          (ogate_act_fn): SiLU()
          (proj_down): Linear(in_features=1536, out_features=768, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layerscale): Identity()
        )
      )
    )
    (post_blocks_norm): LayerNorm()
  )
  (head): Sequential(
    (0): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
    (1): Linear(in_features=1536, out_features=1000, bias=True)
  )
)
11-05 16:55:41 I vislstm initialize optimizer
11-05 16:55:41 I base lr: 1e-5
11-05 16:55:41 I scaled lr: 2.5e-6
11-05 16:55:41 I lr_scaler=LinearLrScaler(divisor=1024)
11-05 16:55:41 I lr_scale_factor=256
11-05 16:55:41 I exclude_bias_from_wd=True exclude_norm_from_wd=True param_group_modifiers=[WeightDecayByNameModifier(name=pos_embed.embed)]
11-05 16:55:41 I using 2 param groups:
11-05 16:55:41 I len(params)=218
11-05 16:55:41 I weight_decay=0.0 len(params)=295
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/base/checkpoint_initializer.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/base/checkpoint_initializer.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/base/checkpoint_initializer.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/base/checkpoint_initializer.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
11-05 16:55:41 I interpolate pos_embed: torch.Size([1, 12, 12, 768]) -> torch.Size([1, 14, 14, 768])
11-05 16:55:41 I loaded weights of vislstm from /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=last model.th
11-05 16:55:41 I added default DatasetStatsCallback
11-05 16:55:41 I added default ParamCountCallback
11-05 16:55:41 I added default CopyPreviousConfigCallback
11-05 16:55:41 I added default CopyPreviousSummaryCallback
11-05 16:55:41 I added default ProgressCallback(every_n_epochs=1)
11-05 16:55:41 I added default TrainTimeCallback(every_n_epochs=1)
11-05 16:55:41 I added default OnlineLossCallback(every_n_epochs=1)
11-05 16:55:41 I added default LrCallback(every_n_updates=50)
11-05 16:55:41 I added default FreezerCallback(every_n_updates=50)
11-05 16:55:41 I added default OnlineLossCallback(every_n_updates=50)
11-05 16:55:41 I replacing BatchNorm layers with SyncBatchNorm
11-05 16:55:41 I torch.compile not used (use_torch_compile == False)
11-05 16:55:41 I ------------------
11-05 16:55:41 I PREPARE TRAINER
11-05 16:55:41 I calculating batch_size and accumulation_steps (effective_batch_size=256)
11-05 16:55:41 I effective_batch_size: 256
11-05 16:55:41 I effective_batch_size_per_device: 64
11-05 16:55:41 I world_size: 4
11-05 16:55:41 I calculating automatic max_batch_size
11-05 16:55:41 I trying batch_size 64
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
11-05 16:55:43 I automatic max_batch_size: 64
11-05 16:55:43 I batch_size: 64
11-05 16:55:43 I accumulation_steps: 1
11-05 16:55:43 I train_batches per epoch: 5004 (world_size=4 batch_size=64)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
11-05 16:55:43 I initializing dataloader
11-05 16:55:43 I OfflineAccuracyCallback(every_n_epochs=1) registered InterleavedSamplerConfig(every_n_epochs=1) dataset_mode='x class'
11-05 16:55:43 I created dataloader (batch_size=64 num_workers=5 pin_memory=True total_cpu_count=64 prefetch_factor=2)
11-05 16:55:43 I concatenated dataset properties:
11-05 16:55:43 I - mode='index x class' len=1281167 root_dataset=<vislstm.datasets.imagenet1k.Imagenet1k object at 0x14f498e70e50>
11-05 16:55:43 I - mode='x class' len=50000 root_dataset=<vislstm.datasets.imagenet1k.Imagenet1k object at 0x14f498fb5040>
11-05 16:55:43 I ------------------
11-05 16:55:43 I BEFORE TRAINING
11-05 16:55:43 I train: 1281167 samples
11-05 16:55:43 I val: 50000 samples
11-05 16:55:43 I parameter counts (trainable | frozen)
11-05 16:55:43 I 89,632,552 | 0 | vislstm
11-05 16:55:43 I estimated checkpoint size: 1.0GB
11-05 16:55:43 I estimated weight checkpoint size: 358.5MB
11-05 16:55:43 I estimated optim checkpoint size: 717.0MB
11-05 16:55:43 I estimated size for 1 checkpoints: 358.5MB
11-05 16:55:43 I estimated checkpoint size: 1.0GB
11-05 16:55:43 I estimated weight checkpoint size: 358.5MB
11-05 16:55:43 I estimated optim checkpoint size: 717.0MB
11-05 16:55:43 I estimated size for 1 checkpoints: 358.5MB
11-05 16:55:43 I ------------------
11-05 16:55:43 I DatasetStatsCallback
11-05 16:55:43 I ParamCountCallback
11-05 16:55:43 I CopyPreviousConfigCallback
11-05 16:55:43 I CopyPreviousSummaryCallback
11-05 16:55:43 I ProgressCallback(every_n_epochs=1)
11-05 16:55:43 I TrainTimeCallback(every_n_epochs=1)
11-05 16:55:43 I OnlineLossCallback(every_n_epochs=1)
11-05 16:55:43 I LrCallback(every_n_updates=50)
11-05 16:55:43 I FreezerCallback(every_n_updates=50)
11-05 16:55:43 I OnlineLossCallback(every_n_updates=50)
11-05 16:55:43 I OnlineAccuracyCallback(every_n_updates=50)
11-05 16:55:43 I OnlineAccuracyCallback(every_n_epochs=1)
11-05 16:55:43 I CheckpointCallback()
11-05 16:55:43 I CheckpointCallback(every_n_epochs=10)
11-05 16:55:43 I OfflineAccuracyCallback(every_n_epochs=1)
11-05 16:55:43 I ------------------
11-05 16:55:43 I START TRAINING
11-05 16:55:43 I initializing dataloader workers
11-05 16:55:43 I initialized dataloader workers
11-05 16:55:44 I 0 unused parameters
11-05 17:23:47 I ------------------
11-05 17:23:47 I Epoch 1/5 (E1_U5004_S1281024)
11-05 17:23:47 I ETA: 11.05 19.16.03 estimated_duration: 02:20:20.28 time_since_last_log: 00:28:04.05 time_per_update: 00:00:00.33 
11-05 17:23:47 I data=[0.00, 0.00, 0.00, 0.00] update=[0.33, 0.33, 0.33, 0.33]
11-05 17:23:47 I loss/online/main/E1: 2.045051097869873
11-05 17:23:47 I loss/online/total/E1: 2.045051097869873
11-05 17:23:47 I accuracy1/online/main/E1: 0.672908
11-05 17:24:12 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.12
11-05 17:24:12 I accuracy1/val/main: 0.815940
11-05 17:24:12 I loss/val/main: 0.74609375
11-05 17:52:14 I ------------------
11-05 17:52:14 I Epoch 2/5 (E2_U10008_S2562048)
11-05 17:52:14 I ETA: 11.05 19.17.35 estimated_duration: 01:53:48.04 time_since_last_log: 00:28:27.01 time_per_update: 00:00:00.34 
11-05 17:52:14 I data=[0.00, 0.00, 0.00, 0.00] update=[0.33, 0.33, 0.33, 0.33]
11-05 17:52:14 I loss/online/main/E1: 2.0286779403686523
11-05 17:52:14 I loss/online/total/E1: 2.0286779403686523
11-05 17:52:14 I accuracy1/online/main/E1: 0.676905
11-05 17:52:38 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.12
11-05 17:52:38 I accuracy1/val/main: 0.814820
11-05 17:52:38 I loss/val/main: 0.74609375
11-05 18:20:42 I ------------------
11-05 18:20:42 I Epoch 3/5 (E3_U15012_S3843072)
11-05 18:20:42 I ETA: 11.05 19.17.37 estimated_duration: 01:53:49.95 time_since_last_log: 00:28:27.96 time_per_update: 00:00:00.34 
11-05 18:20:42 I data=[0.00, 0.00, 0.00, 0.00] update=[0.33, 0.33, 0.33, 0.33]
11-05 18:20:42 I loss/online/main/E1: 2.041469097137451
11-05 18:20:42 I loss/online/total/E1: 2.041469097137451
11-05 18:20:42 I accuracy1/online/main/E1: 0.675231
11-05 18:21:06 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.12
11-05 18:21:06 I accuracy1/val/main: 0.815120
11-05 18:21:06 I loss/val/main: 0.75
11-05 18:49:10 I ------------------
11-05 18:49:10 I Epoch 4/5 (E4_U20016_S5124096)
11-05 18:49:10 I ETA: 11.05 19.17.37 estimated_duration: 01:53:50.09 time_since_last_log: 00:28:27.59 time_per_update: 00:00:00.34 
11-05 18:49:10 I data=[0.00, 0.00, 0.00, 0.00] update=[0.33, 0.33, 0.33, 0.33]
11-05 18:49:10 I loss/online/main/E1: 2.027181625366211
11-05 18:49:10 I loss/online/total/E1: 2.027181625366211
11-05 18:49:10 I accuracy1/online/main/E1: 0.677595
11-05 18:49:34 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.12
11-05 18:49:34 I accuracy1/val/main: 0.815220
11-05 18:49:34 I loss/val/main: 0.74609375
11-05 19:17:37 I ------------------
11-05 19:17:37 I Epoch 5/5 (E5_U25020_S6405120)
11-05 19:17:37 I ETA: 11.05 19.17.37 estimated_duration: 01:53:50.26 time_since_last_log: 00:28:27.69 time_per_update: 00:00:00.34 
11-05 19:17:37 I data=[0.00, 0.00, 0.00, 0.00] update=[0.33, 0.33, 0.33, 0.33]
11-05 19:17:37 I loss/online/main/E1: 2.0276904106140137
11-05 19:17:38 I loss/online/total/E1: 2.0276904106140137
11-05 19:17:38 I accuracy1/online/main/E1: 0.677670
11-05 19:18:02 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.12
11-05 19:18:02 I accuracy1/val/main: 0.814760
11-05 19:18:02 I loss/val/main: 0.74609375
11-05 19:18:02 I ------------------
11-05 19:18:02 I AFTER TRAINING
11-05 19:18:02 I ------------------
11-05 19:18:02 I total_train_data_time:   [8.12, 7.50, 8.16, 7.75]
11-05 19:18:02 I total_update_time: [8216.28, 8228.07, 8219.31, 8221.74]
11-05 19:18:03 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/1bv42fx3/checkpoints/vislstm cp=last model.th
11-05 19:18:03 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/1bv42fx3/checkpoints/trainer cp=last.th
11-05 19:18:04 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/1bv42fx3/checkpoints/vislstm cp=last model.th
11-05 19:18:05 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/1bv42fx3/checkpoints/vislstm cp=latest model.th
11-05 19:18:06 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/1bv42fx3/checkpoints/vislstm cp=latest optim.th
11-05 19:18:06 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/1bv42fx3/checkpoints/trainer cp=last.th
11-05 19:18:06 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/1bv42fx3/checkpoints/trainer cp=latest.th
11-05 19:18:06 I ------------------
11-05 19:18:06 I offline_accuracy_callback dataset_key=val.x.class
11-05 19:18:06 I total_data_time:    [0.00, 0.00, 0.00, 0.00]
11-05 19:18:06 I total_forward_time: [0.61, 0.61, 0.61, 0.61]
11-05 19:18:06 I writing 506 log entries to /home/beknur.kalmakhanbet/save/in1k/1bv42fx3/primitive/entries.th
11-05 19:18:06 I ------------------
11-05 19:18:06 I summarize logvalues
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/providers/summary_providers/primitive_summary_provider.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  entries = torch.load(entries_uri)
11-05 19:18:06 I loss/online/main/U50/min: 1.8979291915893555
11-05 19:18:06 I loss/online/total/U50/min: 1.8979291915893555
11-05 19:18:06 I accuracy1/online/main/U50/max: 0.7109375
11-05 19:18:06 I loss/online/main/E1/min: 2.027181625366211
11-05 19:18:06 I loss/online/total/E1/min: 2.027181625366211
11-05 19:18:06 I accuracy1/online/main/E1/max: 0.6776703596115112
11-05 19:18:06 I accuracy1/val/main/max: 0.8159400224685669
11-05 19:18:06 I loss/val/main/min: 0.74609375
11-05 19:18:06 I pushing summarized logvalues to wandb
11-05 19:18:06 W cuda profiling is not activated -> all cuda calls are executed asynchronously -> this will result in inaccurate profiling times where the time for all asynchronous cuda operation will be attributed to the first synchronous cuda operation https://github.com/BenediktAlkin/KappaProfiler?tab=readme-ov-file#time-async-operations
11-05 19:18:06 I full profiling times:
  8545.43 train
     0.00 train.DatasetStatsCallback.before_training
     0.00 train.ParamCountCallback.before_training
     0.03 train.CopyPreviousConfigCallback.before_training
     0.01 train.CopyPreviousSummaryCallback.before_training
     0.00 train.ProgressCallback(every_n_epochs=1).before_training
     0.00 train.OnlineAccuracyCallback(every_n_updates=50).before_training
     0.00 train.OnlineAccuracyCallback(every_n_epochs=1).before_training
     0.00 train.CheckpointCallback().before_training
     0.00 train.CheckpointCallback(every_n_epochs=10).before_training
     0.00 train.OfflineAccuracyCallback(every_n_epochs=1).before_training
     0.27 train.iterator
     8.12 train.data_loading
  8216.28 train.update
     0.23 train.OnlineLossCallback(every_n_epochs=1).track_after_accumulation_step
     0.14 train.OnlineLossCallback(every_n_updates=50).track_after_accumulation_step
    12.02 train.OnlineAccuracyCallback(every_n_updates=50).track_after_accumulation_step
     7.91 train.OnlineAccuracyCallback(every_n_epochs=1).track_after_accumulation_step
     0.11 train.TrainTimeCallback(every_n_epochs=1).track_after_update_step
     0.02 train.LrCallback(every_n_updates=50).after_update
     0.00 train.FreezerCallback(every_n_updates=50).after_update
    10.12 train.OnlineLossCallback(every_n_updates=50).after_update
     0.15 train.OnlineAccuracyCallback(every_n_updates=50).after_update
     0.02 train.ProgressCallback(every_n_epochs=1).after_epoch
     0.10 train.TrainTimeCallback(every_n_epochs=1).after_epoch
     0.14 train.OnlineLossCallback(every_n_epochs=1).after_epoch
     0.09 train.OnlineAccuracyCallback(every_n_epochs=1).after_epoch
   121.21 train.OfflineAccuracyCallback(every_n_epochs=1).after_epoch
     0.00 train.TrainTimeCallback(every_n_epochs=1).after_training
     0.79 train.CheckpointCallback().after_training
     3.13 train.CheckpointCallback(every_n_epochs=10).after_training
11-05 19:18:06 I ------------------
11-05 19:18:06 I CLEANUP
11-05 19:18:06 I ------------------
11-05 19:18:06 I encountered 1 warnings
11-05 19:18:06 I encountered 0 errors
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:          accuracy1/online/main/E1 ‚ñÅ‚ñá‚ñÑ‚ñà‚ñà
wandb:         accuracy1/online/main/U50 ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÜ
wandb:                accuracy1/val/main ‚ñà‚ñÅ‚ñÉ‚ñÑ‚ñÅ
wandb:                             epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:               loss/online/main/E1 ‚ñà‚ñÇ‚ñá‚ñÅ‚ñÅ
wandb:              loss/online/main/U50 ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñà‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñá‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÜ‚ñÑ‚ñÖ
wandb:              loss/online/total/E1 ‚ñà‚ñÇ‚ñá‚ñÅ‚ñÅ
wandb:             loss/online/total/U50 ‚ñÇ‚ñá‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ
wandb:                     loss/val/main ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ
wandb:          optim/lr/vislstm/default ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb: optim/lr/vislstm/weight_decay=0e0 ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:                            sample ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:                            update ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:            accuracy1/online/main/E1 0.67767
wandb:       accuracy1/online/main/E1/last 0.67767
wandb:        accuracy1/online/main/E1/max 0.67767
wandb:           accuracy1/online/main/U50 0.6732
wandb:      accuracy1/online/main/U50/last 0.6732
wandb:       accuracy1/online/main/U50/max 0.71094
wandb:                  accuracy1/val/main 0.81476
wandb:             accuracy1/val/main/last 0.81476
wandb:              accuracy1/val/main/max 0.81594
wandb:                  ds_stats/train/len 1281167
wandb:                    ds_stats/val/len 50000
wandb:                               epoch 5
wandb:       in1k/accuracy1/online/main/E1 0.66577
wandb:  in1k/accuracy1/online/main/E1/last 0.66577
wandb:   in1k/accuracy1/online/main/E1/max 0.66819
wandb:      in1k/accuracy1/online/main/U50 0.66617
wandb: in1k/accuracy1/online/main/U50/last 0.66617
wandb:  in1k/accuracy1/online/main/U50/max 0.69723
wandb:             in1k/accuracy1/val/main 0.8047
wandb:        in1k/accuracy1/val/main/last 0.8047
wandb:         in1k/accuracy1/val/main/max 0.8047
wandb:             in1k/ds_stats/train/len 1281167
wandb:               in1k/ds_stats/val/len 50000
wandb:                          in1k/epoch 400
wandb:            in1k/loss/online/main/E1 2.12527
wandb:       in1k/loss/online/main/E1/last 2.12527
wandb:        in1k/loss/online/main/E1/min 2.11142
wandb:           in1k/loss/online/main/U50 2.12294
wandb:      in1k/loss/online/main/U50/last 2.12294
wandb:       in1k/loss/online/main/U50/min 1.98818
wandb:           in1k/loss/online/total/E1 2.12527
wandb:      in1k/loss/online/total/E1/last 2.12527
wandb:       in1k/loss/online/total/E1/min 2.11142
wandb:          in1k/loss/online/total/U50 2.12294
wandb:     in1k/loss/online/total/U50/last 2.12294
wandb:      in1k/loss/online/total/U50/min 1.98818
wandb:                  in1k/loss/val/main 0.79688
wandb:             in1k/loss/val/main/last 0.79688
wandb:              in1k/loss/val/main/min 0.79297
wandb:     in1k/param_count/vislstm/frozen 0
wandb:  in1k/param_count/vislstm/trainable 89592616
wandb:                         in1k/sample 512409600
wandb:                 loss/online/main/E1 2.02769
wandb:            loss/online/main/E1/last 2.02769
wandb:             loss/online/main/E1/min 2.02718
wandb:                loss/online/main/U50 2.02395
wandb:           loss/online/main/U50/last 2.02395
wandb:            loss/online/main/U50/min 1.89793
wandb:                loss/online/total/E1 2.02769
wandb:           loss/online/total/E1/last 2.02769
wandb:            loss/online/total/E1/min 2.02718
wandb:               loss/online/total/U50 2.02395
wandb:          loss/online/total/U50/last 2.02395
wandb:           loss/online/total/U50/min 1.89793
wandb:                       loss/val/main 0.74609
wandb:                  loss/val/main/last 0.74609
wandb:                   loss/val/main/min 0.74609
wandb:            optim/lr/vislstm/default 0.0
wandb:   optim/lr/vislstm/weight_decay=0e0 0.0
wandb:          param_count/vislstm/frozen 0
wandb:       param_count/vislstm/trainable 89632552
wandb:                              sample 6405120
wandb:                              update 25020
wandb: 
wandb: üöÄ View run ddgjaz5t--e5res224/in1k at: https://wandb.ai/beka-kalmahanbet-mbzuai/minLSTM/runs/1bv42fx3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/beka-kalmahanbet-mbzuai/minLSTM
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /home/beknur.kalmakhanbet/save/in1k/1bv42fx3/wandb/run-20251105_165528-1bv42fx3/logs
