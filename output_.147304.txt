MASTER_ADDR: gpu-04
CUDA_VISIBLE_DEVICES=0,1,2,3
Fri Oct 17 04:21:44 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   26C    P0             50W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   27C    P0             52W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   26C    P0             56W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   25C    P0             53W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
torch: 2.5.1+cu121 cuda: 12.1 cuda available: True
10-17 04:21:59 I initializing rank=0 local_rank=0 nodes=1 hostname=gpu-04 master_addr=gpu-04 master_port=55555 (waiting for all 4 processes to connect)
10-17 04:21:59 I initializing rank=3 local_rank=3 nodes=1 hostname=gpu-04 master_addr=gpu-04 master_port=55555 (waiting for all 4 processes to connect)
10-17 04:21:59 I initializing rank=1 local_rank=1 nodes=1 hostname=gpu-04 master_addr=gpu-04 master_port=55555 (waiting for all 4 processes to connect)
10-17 04:21:59 I initializing rank=2 local_rank=2 nodes=1 hostname=gpu-04 master_addr=gpu-04 master_port=55555 (waiting for all 4 processes to connect)
[rank1]:[W1017 04:21:59.073723571 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W1017 04:21:59.073734671 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W1017 04:21:59.073908188 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W1017 04:21:59.074055266 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
10-17 04:22:00 I initialized process rank=1 local_rank=1 pid=1941059
10-17 04:22:00 I initialized process rank=3 local_rank=3 pid=1941061
10-17 04:22:00 I initialized process rank=2 local_rank=2 pid=1941060
10-17 04:22:00 I initialized process rank=0 local_rank=0 pid=1941058
10-17 04:22:00 I initialized 4 processes
10-17 04:22:00 W disabled cudnn benchmark
10-17 04:22:00 W enabled cudnn deterministic
10-17 04:22:00 I log file: /home/beknur.kalmakhanbet/save/in1k/3vej3j35/log.txt
10-17 04:22:00 I no seed specified -> using seed=0
10-17 04:22:00 I ------------------
10-17 04:22:00 I initializing wandb (mode=disabled)
fatal: No annotated tags can describe '0a017ac64ba77e9fa6f34eaa0f9fd5f82cd32537'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe '0a017ac64ba77e9fa6f34eaa0f9fd5f82cd32537'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe '0a017ac64ba77e9fa6f34eaa0f9fd5f82cd32537'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe '0a017ac64ba77e9fa6f34eaa0f9fd5f82cd32537'.
However, there were unannotated tags: try --tags.
10-17 04:22:01 I ------------------
10-17 04:22:01 I stage_id: 3vej3j35
10-17 04:22:01 I python main_train.py --hp src/vislstm/yamls/pretrain/vil/80M_res224finetuning_e5.yaml --resume_stage_id 697ylce2 --resume_checkpoint latest --num_workers 5
10-17 04:22:01 I ------------------
10-17 04:22:01 I VERSION CHECK
10-17 04:22:01 I executable: /home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/bin/python
10-17 04:22:01 I python version: 3.9.21
10-17 04:22:01 I torch version: 2.5.1+cu121
10-17 04:22:01 I torch.cuda version: 12.1
10-17 04:22:01 I torchvision.version: 0.20.1+cu121
10-17 04:22:03 I torchmetrics version: 1.6.2
10-17 04:22:03 I kappaschedules version: 0.0.31
10-17 04:22:03 I kappamodules version: 0.1.76
10-17 04:22:03 I ------------------
10-17 04:22:03 I SYSTEM INFO
10-17 04:22:03 I host name: gpu-04
10-17 04:22:03 I OS: Linux-5.15.161-ql-generic-13.0-14-x86_64-with-glibc2.35
10-17 04:22:03 I OS version: #1 SMP Wed Jun 26 16:19:39 UTC 2024
fatal: No annotated tags can describe '0a017ac64ba77e9fa6f34eaa0f9fd5f82cd32537'.
However, there were unannotated tags: try --tags.
10-17 04:22:03 I initialized process rank=2 local_rank=2 pid=1941060 hostname=gpu-04
10-17 04:22:04 I CUDA version: 12.4
10-17 04:22:04 I current commit hash: 07d72a6204b22292b3b8f6ff749fea2a6a875230
fatal: No annotated tags can describe '0a017ac64ba77e9fa6f34eaa0f9fd5f82cd32537'.
However, there were unannotated tags: try --tags.
10-17 04:22:04 I latest git tag: 
10-17 04:22:04 I initialized process rank=0 local_rank=0 pid=1941058 hostname=gpu-04
10-17 04:22:04 I total_cpu_count: 64
10-17 04:22:04 I ------------------
10-17 04:22:04 I STATIC CONFIG
10-17 04:22:04 I account_name: beknur.kalmakhanbet
10-17 04:22:04 I output_path: /home/beknur.kalmakhanbet/save
10-17 04:22:04 I ------------------
10-17 04:22:04 I CLI ARGS
10-17 04:22:04 I hp: src/vislstm/yamls/pretrain/vil/80M_res224finetuning_e5.yaml
10-17 04:22:04 I accelerator: gpu
10-17 04:22:04 I num_workers: 5
10-17 04:22:04 I testrun: False
10-17 04:22:04 I minmodelrun: False
10-17 04:22:04 I mindatarun: False
10-17 04:22:04 I mindurationrun: False
10-17 04:22:04 I static_config_uri: static_config.yaml
10-17 04:22:04 I resume_stage_id: 697ylce2
10-17 04:22:04 I resume_checkpoint: latest
10-17 04:22:04 I ------------------
10-17 04:22:04 I DIST CONFIG
10-17 04:22:04 I rank: 0
10-17 04:22:04 I local_rank: 0
10-17 04:22:04 I world_size: 4
10-17 04:22:04 I nodes: 1
10-17 04:22:04 I backend: nccl
10-17 04:22:04 I slurm job id: 147304
10-17 04:22:04 I hostnames: gpu-04
10-17 04:22:04 I ------------------
master_factory_base_path: vislstm
stage_name: in1k
datasets:
  train:
    kind: imagenet1k
    split: train
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: random_resized_crop
        size: 224
        scale:
        - 0.08
        - 1.0
        interpolation: bicubic
      - kind: random_horizontal_flip
      - kind: transforms.three_augment
        blur_sigma:
        - 0.1
        - 2.0
      - kind: color_jitter
        brightness: 0.3
        contrast: 0.3
        saturation: 0.3
        hue: 0.0
      - kind: imagenet1k_norm
    - kind: one_hot_wrapper
    collators:
    - kind: mix_collator
      mixup_alpha: 0.8
      cutmix_alpha: 1.0
      mixup_p: 0.5
      cutmix_p: 0.5
      apply_mode: batch
      lamb_mode: batch
      shuffle_mode: flip
  val:
    kind: imagenet1k
    split: val
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: resize
        size: 224
        interpolation: bicubic
      - kind: center_crop
        size: 224
      - kind: imagenet1k_norm
model:
  initializers:
  - kind: previous_run_initializer
    stage_id: a0gelq34
    stage_name: in1k
    model_name: vislstm
    checkpoint: last
    use_checkpoint_kwargs: true
  optim:
    kind: adamw
    lr: 1.0e-05
    betas:
    - 0.9
    - 0.999
    weight_decay: 0.05
    schedule:
      kind: linear_warmup_cosine_decay_schedule
      warmup_epochs: 1
      end_value: 1.0e-06
    lr_scaler:
      kind: linear_lr_scaler
      divisor: 1024
trainer:
  kind: classification_trainer
  precision: bfloat16
  backup_precision: float16
  max_epochs: 5
  effective_batch_size: 256
  log_every_n_epochs: 1
  callbacks:
  - kind: checkpoint_callback
  - kind: checkpoint_callback
    every_n_epochs: 10
    save_latest_weights: true
    save_latest_optim: true
  - kind: offline_accuracy_callback
    every_n_epochs: 1
    dataset_key: val
  initializer:
    kind: resume_initializer
    stage_id: 697ylce2
    checkpoint: latest
10-17 04:22:04 I copied unresolved hp to /home/beknur.kalmakhanbet/save/in1k/3vej3j35/hp_unresolved.yaml
10-17 04:22:04 I dumped resolved hp to /home/beknur.kalmakhanbet/save/in1k/3vej3j35/hp_resolved.yaml
10-17 04:22:04 I ------------------
10-17 04:22:04 I training stage 'in1k'
10-17 04:22:04 I using different seeds per process (seed+rank)
10-17 04:22:04 I set seed to 0
10-17 04:22:04 I ------------------
10-17 04:22:04 I initializing datasets
10-17 04:22:04 I initializing train
fatal: No annotated tags can describe '0a017ac64ba77e9fa6f34eaa0f9fd5f82cd32537'.
However, there were unannotated tags: try --tags.
10-17 04:22:04 I initialized process rank=3 local_rank=3 pid=1941061 hostname=gpu-04
fatal: No annotated tags can describe '0a017ac64ba77e9fa6f34eaa0f9fd5f82cd32537'.
However, there were unannotated tags: try --tags.
10-17 04:22:05 I initialized process rank=1 local_rank=1 pid=1941059 hostname=gpu-04
10-17 04:22:10 I instantiating sample_wrapper x_transform_wrapper
10-17 04:22:10 I instantiating sample_wrapper one_hot_wrapper
10-17 04:22:10 I initializing val
10-17 04:22:11 I instantiating sample_wrapper x_transform_wrapper
10-17 04:22:11 I ------------------
10-17 04:22:11 I initializing trainer
10-17 04:22:11 I using precision: torch.bfloat16 (desired=bfloat16 backup=float16)
10-17 04:22:11 I main_sampler: DistributedSampler(num_repeats=1, shuffle=True)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
10-17 04:22:11 I loaded checkpoint from trainer_state_dict: {'epoch': 400, 'update': 1000800, 'sample': 512409600, 'callback_state_dicts': [None, None, None]}
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/main_train.py", line 9, in <module>
[rank0]:     main()
[rank0]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/main_train.py", line 5, in main
[rank0]:     Runner().run()
[rank0]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/runners/runner.py", line 86, in run
[rank0]:     run_managed(
[rank0]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/distributed/run/managed.py", line 47, in run_managed
[rank0]:     _run_managed_multiprocess(accelerator, main)
[rank0]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/distributed/run/managed.py", line 78, in _run_managed_multiprocess
[rank0]:     main(device=device)
[rank0]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/runners/runner.py", line 319, in main
[rank0]:     trainer = MasterFactory.get("trainer").instantiate(
[rank0]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/factory/base/single_factory.py", line 71, in instantiate
[rank0]:     return ctor(**kwargs, **optional_kwargs)
[rank0]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/trainers/classification_trainer.py", line 12, in __init__
[rank0]:     super().__init__(**kwargs)
[rank0]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/trainers/base/sgd_trainer.py", line 172, in __init__
[rank0]:     self.update_counter = UpdateCounter(
[rank0]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/utils/update_counter.py", line 23, in __init__
[rank0]:     assert self.start_checkpoint == Checkpoint(epoch=self.start_checkpoint.epoch).to_fully_specified(
[rank0]: AssertionError
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/main_train.py", line 9, in <module>
[rank1]:     main()
[rank1]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/main_train.py", line 5, in main
[rank1]:     Runner().run()
[rank1]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/runners/runner.py", line 86, in run
[rank1]:     run_managed(
[rank1]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/distributed/run/managed.py", line 47, in run_managed
[rank1]:     _run_managed_multiprocess(accelerator, main)
[rank1]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/distributed/run/managed.py", line 78, in _run_managed_multiprocess
[rank1]:     main(device=device)
[rank1]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/runners/runner.py", line 319, in main
[rank1]:     trainer = MasterFactory.get("trainer").instantiate(
[rank1]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/factory/base/single_factory.py", line 71, in instantiate
[rank1]:     return ctor(**kwargs, **optional_kwargs)
[rank1]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/trainers/classification_trainer.py", line 12, in __init__
[rank1]:     super().__init__(**kwargs)
[rank1]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/trainers/base/sgd_trainer.py", line 172, in __init__
[rank1]:     self.update_counter = UpdateCounter(
[rank1]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/utils/update_counter.py", line 23, in __init__
[rank1]:     assert self.start_checkpoint == Checkpoint(epoch=self.start_checkpoint.epoch).to_fully_specified(
[rank1]: AssertionError
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/main_train.py", line 9, in <module>
[rank2]:     main()
[rank2]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/main_train.py", line 5, in main
[rank2]:     Runner().run()
[rank2]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/runners/runner.py", line 86, in run
[rank2]:     run_managed(
[rank2]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/distributed/run/managed.py", line 47, in run_managed
[rank2]:     _run_managed_multiprocess(accelerator, main)
[rank2]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/distributed/run/managed.py", line 78, in _run_managed_multiprocess
[rank2]:     main(device=device)
[rank2]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/runners/runner.py", line 319, in main
[rank2]:     trainer = MasterFactory.get("trainer").instantiate(
[rank2]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/factory/base/single_factory.py", line 71, in instantiate
[rank2]:     return ctor(**kwargs, **optional_kwargs)
[rank2]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/trainers/classification_trainer.py", line 12, in __init__
[rank2]:     super().__init__(**kwargs)
[rank2]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/trainers/base/sgd_trainer.py", line 172, in __init__
[rank2]:     self.update_counter = UpdateCounter(
[rank2]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/utils/update_counter.py", line 23, in __init__
[rank2]:     assert self.start_checkpoint == Checkpoint(epoch=self.start_checkpoint.epoch).to_fully_specified(
[rank2]: AssertionError
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/main_train.py", line 9, in <module>
[rank3]:     main()
[rank3]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/main_train.py", line 5, in main
[rank3]:     Runner().run()
[rank3]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/runners/runner.py", line 86, in run
[rank3]:     run_managed(
[rank3]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/distributed/run/managed.py", line 47, in run_managed
[rank3]:     _run_managed_multiprocess(accelerator, main)
[rank3]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/distributed/run/managed.py", line 78, in _run_managed_multiprocess
[rank3]:     main(device=device)
[rank3]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/runners/runner.py", line 319, in main
[rank3]:     trainer = MasterFactory.get("trainer").instantiate(
[rank3]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/factory/base/single_factory.py", line 71, in instantiate
[rank3]:     return ctor(**kwargs, **optional_kwargs)
[rank3]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/trainers/classification_trainer.py", line 12, in __init__
[rank3]:     super().__init__(**kwargs)
[rank3]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/trainers/base/sgd_trainer.py", line 172, in __init__
[rank3]:     self.update_counter = UpdateCounter(
[rank3]:   File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/utils/update_counter.py", line 23, in __init__
[rank3]:     assert self.start_checkpoint == Checkpoint(epoch=self.start_checkpoint.epoch).to_fully_specified(
[rank3]: AssertionError
[rank2]:[W1017 04:22:11.091855048 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W1017 04:22:11.097364293 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]:[W1017 04:22:11.099364258 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1017 04:22:11.103622955 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
srun: error: gpu-04: tasks 0-1,3: Exited with exit code 1
srun: error: gpu-04: task 2: Exited with exit code 1
slurmstepd-gpu-04: error: *** JOB 147304 ON gpu-04 CANCELLED AT 2025-10-17T09:39:26 ***
