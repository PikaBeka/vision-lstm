#! /bin/bash
#SBATCH --job-name=minLSTM # Job name
#SBATCH --output=/home/beknur.kalmakhanbet/vision-lstm/output_.%A.txt # Standard output and error.
#SBATCH --nodes=1 # Run all processes on a single node
#SBATCH --ntasks=4 # Run on a single CPU
#SBATCH --mem=40G # Total RAM to be used
#SBATCH --cpus-per-task=64 # Number of CPU cores
#SBATCH --gres=gpu:4 # Number of GPUs (per node)
#SBATCH -p cscc-gpu-p # Use the gpu partition
#SBATCH --time=72:00:00 # Specify the time needed for you job
#SBATCH -q cscc-gpu-qos # To enable the use of up to 8 GPUs

# ACTIVATE ENV
source /home/beknur.kalmakhanbet/miniconda3/etc/profile.d/conda.sh
conda activate minLSTM

sleep infinity

# master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
# export MASTER_ADDR=$master_addr          # each rank sees one GPU
# echo "MASTER_ADDR: $MASTER_ADDR"
# export MASTER_PORT=55555
# # add all hostnames info for logging
# export ALL_HOST_NAMES=$(scontrol show hostnames "$SLURM_JOB_NODELIST")

srun python src/main_train.py --hp src/vislstm/yamls/pretrain/vil/80M_res224finetuning_e5.yaml
    # --accelerator gpu
# python src/main_sbatch.py --time 24:00:00 --nodes 1 --hp src/vislstm/yamls/pretrain/vil/lstm_6M16_e800_bialter_bilatflat_conv2d3_lr1e3_res192_bias.yaml # You job script or command