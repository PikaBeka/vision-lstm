MASTER_ADDR: gpu-06
CUDA_VISIBLE_DEVICES=0,1,2,3
Thu Oct 30 14:27:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   28C    P0             56W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   26C    P0             50W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   26C    P0             50W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   25C    P0             52W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
torch: 2.5.1+cu121 cuda: 12.1 cuda available: True
10-30 14:27:15 I initializing rank=1 local_rank=1 nodes=1 hostname=gpu-06 master_addr=gpu-06 master_port=55555 (waiting for all 4 processes to connect)
10-30 14:27:15 I initializing rank=3 local_rank=3 nodes=1 hostname=gpu-06 master_addr=gpu-06 master_port=55555 (waiting for all 4 processes to connect)
10-30 14:27:15 I initializing rank=0 local_rank=0 nodes=1 hostname=gpu-06 master_addr=gpu-06 master_port=55555 (waiting for all 4 processes to connect)
10-30 14:27:15 I initializing rank=2 local_rank=2 nodes=1 hostname=gpu-06 master_addr=gpu-06 master_port=55555 (waiting for all 4 processes to connect)
[rank2]:[W1030 14:27:15.361799920 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W1030 14:27:15.687718290 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W1030 14:27:16.869949901 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W1030 14:27:16.873087084 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
10-30 14:27:16 I initialized process rank=1 local_rank=1 pid=991781
10-30 14:27:16 I initialized process rank=3 local_rank=3 pid=991783
10-30 14:27:16 I initialized process rank=0 local_rank=0 pid=991780
10-30 14:27:16 I initialized process rank=2 local_rank=2 pid=991782
10-30 14:27:16 I initialized 4 processes
10-30 14:27:16 W disabled cudnn benchmark
10-30 14:27:16 W enabled cudnn deterministic
10-30 14:27:17 I log file: /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/log.txt
10-30 14:27:17 I no seed specified -> using seed=0
10-30 14:27:17 I ------------------
10-30 14:27:17 I initializing wandb (mode=online)
10-30 14:27:17 I logging into wandb (host=https://api.wandb.ai/ rank=0)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
wandb: Currently logged in as: beka-kalmahanbet (ml710_project) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
10-30 14:27:18 I logged into wandb (host=https://api.wandb.ai/)
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/wandb/run-20251030_142718-6xt8x4ip
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run in1k-lstm-80m16-e400res192-bialter-bilatflat-lr1e3-conv2d3-bias/in1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/beka-kalmahanbet-mbzuai/minLSTM
wandb: üöÄ View run at https://wandb.ai/beka-kalmahanbet-mbzuai/minLSTM/runs/6xt8x4ip
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
10-30 14:27:19 I ------------------
10-30 14:27:19 I stage_id: 6xt8x4ip
10-30 14:27:19 I python main_train.py --hp src/vislstm/yamls/pretrain/vil/lstm_80M16_e400_bialter_bilatflat_conv2d3_lr1e3_res192_bias.yaml --resume_stage_id fkiz3se6 --resume_checkpoint latest --num_workers 5
10-30 14:27:19 I ------------------
10-30 14:27:19 I VERSION CHECK
10-30 14:27:19 I executable: /home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/bin/python
10-30 14:27:19 I python version: 3.9.21
10-30 14:27:19 I torch version: 2.5.1+cu121
10-30 14:27:19 I torch.cuda version: 12.1
10-30 14:27:19 I torchvision.version: 0.20.1+cu121
10-30 14:27:20 I torchmetrics version: 1.6.2
10-30 14:27:20 I kappaschedules version: 0.0.31
10-30 14:27:20 I kappamodules version: 0.1.76
10-30 14:27:20 I ------------------
10-30 14:27:20 I SYSTEM INFO
10-30 14:27:20 I host name: gpu-06
10-30 14:27:20 I OS: Linux-5.15.161-ql-generic-13.0-14-x86_64-with-glibc2.35
10-30 14:27:20 I OS version: #1 SMP Wed Jun 26 16:19:39 UTC 2024
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
10-30 14:27:20 I initialized process rank=3 local_rank=3 pid=991783 hostname=gpu-06
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
10-30 14:27:20 I initialized process rank=1 local_rank=1 pid=991781 hostname=gpu-06
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
10-30 14:27:21 I initialized process rank=2 local_rank=2 pid=991782 hostname=gpu-06
10-30 14:27:21 I CUDA version: 12.4
10-30 14:27:21 I current commit hash: 879894a2c4205819466aaff45b583fe3b517c036
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
10-30 14:27:21 I latest git tag: 
10-30 14:27:21 I initialized process rank=0 local_rank=0 pid=991780 hostname=gpu-06
10-30 14:27:21 I total_cpu_count: 64
10-30 14:27:21 I ------------------
10-30 14:27:21 I STATIC CONFIG
10-30 14:27:21 I account_name: beknur.kalmakhanbet
10-30 14:27:21 I output_path: /home/beknur.kalmakhanbet/save
10-30 14:27:21 I ------------------
10-30 14:27:21 I CLI ARGS
10-30 14:27:21 I hp: src/vislstm/yamls/pretrain/vil/lstm_80M16_e400_bialter_bilatflat_conv2d3_lr1e3_res192_bias.yaml
10-30 14:27:21 I accelerator: gpu
10-30 14:27:21 I num_workers: 5
10-30 14:27:21 I testrun: False
10-30 14:27:21 I minmodelrun: False
10-30 14:27:21 I mindatarun: False
10-30 14:27:21 I mindurationrun: False
10-30 14:27:21 I static_config_uri: static_config.yaml
10-30 14:27:21 I resume_stage_id: fkiz3se6
10-30 14:27:21 I resume_checkpoint: latest
10-30 14:27:21 I ------------------
10-30 14:27:21 I DIST CONFIG
10-30 14:27:21 I rank: 0
10-30 14:27:21 I local_rank: 0
10-30 14:27:21 I world_size: 4
10-30 14:27:21 I nodes: 1
10-30 14:27:21 I backend: nccl
10-30 14:27:21 I slurm job id: 150080
10-30 14:27:21 I hostnames: gpu-06
10-30 14:27:21 I ------------------
master_factory_base_path: vislstm
stage_name: in1k
datasets:
  train:
    kind: imagenet1k
    split: train
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: random_resized_crop
        size: 192
        scale:
        - 0.08
        - 1.0
        interpolation: bicubic
      - kind: random_horizontal_flip
      - kind: transforms.three_augment
        blur_sigma:
        - 0.1
        - 2.0
      - kind: color_jitter
        brightness: 0.3
        contrast: 0.3
        saturation: 0.3
        hue: 0.0
      - kind: imagenet1k_norm
    - kind: one_hot_wrapper
    collators:
    - kind: mix_collator
      mixup_alpha: 0.8
      cutmix_alpha: 1.0
      mixup_p: 0.5
      cutmix_p: 0.5
      apply_mode: batch
      lamb_mode: batch
      shuffle_mode: flip
  val:
    kind: imagenet1k
    split: val
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: resize
        size: 192
        interpolation: bicubic
      - kind: center_crop
        size: 192
      - kind: imagenet1k_norm
model:
  kind: models.single.vislstm
  patch_size: 16
  dim: 768
  depth: 24
  bidirectional: false
  alternation: bidirectional
  conv1d_kernel_size: 3
  use_conv2d: true
  bias: true
  pos_embed_mode: learnable
  drop_path_rate: 0.2
  drop_path_decay: false
  mode: classifier
  pooling:
    kind: bilateral
    aggregate: flatten
  optim:
    kind: adamw
    lr: 0.001
    betas:
    - 0.9
    - 0.999
    weight_decay: 0.05
    clip_grad_norm: 1.0
    schedule:
      kind: linear_warmup_cosine_decay_schedule
      warmup_epochs: 5
      end_value: 1.0e-06
    lr_scaler:
      kind: linear_lr_scaler
      divisor: 1024
trainer:
  kind: classification_trainer
  precision: bfloat16
  backup_precision: float16
  max_epochs: 400
  effective_batch_size: 512
  log_every_n_epochs: 1
  use_torch_compile: true
  callbacks:
  - kind: checkpoint_callback
  - kind: checkpoint_callback
    every_n_epochs: 10
    save_weights: false
    save_latest_weights: true
    save_latest_optim: true
  - kind: offline_accuracy_callback
    every_n_epochs: 5
    dataset_key: val
  initializer:
    kind: resume_initializer
    stage_id: fkiz3se6
    checkpoint: latest
10-30 14:27:21 I copied unresolved hp to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/hp_unresolved.yaml
10-30 14:27:21 I dumped resolved hp to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/hp_resolved.yaml
10-30 14:27:21 I ------------------
10-30 14:27:21 I training stage 'in1k'
10-30 14:27:21 I using different seeds per process (seed+rank)
10-30 14:27:21 I set seed to 0
10-30 14:27:21 I ------------------
10-30 14:27:21 I initializing datasets
10-30 14:27:21 I initializing train
10-30 14:27:26 I instantiating sample_wrapper x_transform_wrapper
10-30 14:27:26 I instantiating sample_wrapper one_hot_wrapper
10-30 14:27:26 I initializing val
10-30 14:27:27 I instantiating sample_wrapper x_transform_wrapper
10-30 14:27:27 I ------------------
10-30 14:27:27 I initializing trainer
10-30 14:27:27 I using precision: torch.bfloat16 (desired=bfloat16 backup=float16)
10-30 14:27:27 I main_sampler: DistributedSampler(num_repeats=1, shuffle=True)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
10-30 14:27:27 I loaded checkpoint from trainer_state_dict: {'epoch': 80, 'update': 200160, 'sample': 102481920, 'callback_state_dicts': [None, None, None]}
10-30 14:27:27 I ------------------
10-30 14:27:27 I creating model
10-30 14:27:27 I input_shape: (3, 192, 192)
10-30 14:27:27 I pos_embed.is_learnable=True
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
10-30 14:27:28 I drop_path_rate: 0.2
10-30 14:27:29 I model:
VisLSTM(
  (pooling): Bilateral(aggregate=flatten)
  (patch_embed): VitPatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_embed): VitPosEmbed2d()
  (xlstm): xLSTMBlockStack(
    (blocks): ModuleList(
      (0-23): 24 x mLSTMBlock(
        (drop_path1): DropPath(drop_prob=0.200)
        (xlstm_norm): LayerNorm()
        (xlstm): mLSTMLayer(
          (proj_up): Linear(in_features=768, out_features=3072, bias=True)
          (conv1d): SequenceConv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
          (conv_act_fn): SiLU()
          (mlstm_cell): mLSTMCell(
            (linear_h): FeedForward(
              (linear1): Linear(in_features=1536, out_features=8, bias=True)
              (activation): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=8, out_features=1536, bias=True)
            )
            (linear_i): FeedForward(
              (linear1): Linear(in_features=1536, out_features=8, bias=True)
              (activation): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=8, out_features=1536, bias=True)
            )
            (linear_f): FeedForward(
              (linear1): Linear(in_features=1536, out_features=8, bias=True)
              (activation): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=8, out_features=1536, bias=True)
            )
          )
          (ogate_act_fn): SiLU()
          (proj_down): Linear(in_features=1536, out_features=768, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layerscale): Identity()
        )
      )
    )
    (post_blocks_norm): LayerNorm()
  )
  (head): Sequential(
    (0): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
    (1): Linear(in_features=1536, out_features=1000, bias=True)
  )
)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
10-30 14:27:29 I vislstm initialize optimizer
10-30 14:27:29 I base lr: 1e-3
10-30 14:27:29 I scaled lr: 5e-4
10-30 14:27:29 I lr_scaler=LinearLrScaler(divisor=1024)
10-30 14:27:29 I lr_scale_factor=512
10-30 14:27:29 I exclude_bias_from_wd=True exclude_norm_from_wd=True param_group_modifiers=[WeightDecayByNameModifier(name=pos_embed.embed)]
10-30 14:27:29 I using 2 param groups:
10-30 14:27:29 I len(params)=218
10-30 14:27:29 I weight_decay=0.0 len(params)=295
10-30 14:27:29 I ------------------
10-30 14:27:29 I loading trainer/model state for resuming
10-30 14:27:29 I loading state from checkpoint fkiz3se6/in1k/latest
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
10-30 14:27:29 I loaded trainer checkpoint /home/beknur.kalmakhanbet/save/in1k/fkiz3se6/checkpoints/trainer cp=latest.th
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
10-30 14:27:30 I loaded weights of vislstm from /home/beknur.kalmakhanbet/save/in1k/fkiz3se6/checkpoints/vislstm cp=latest model.th
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
10-30 14:27:33 I loaded optimizer of vislstm from /home/beknur.kalmakhanbet/save/in1k/fkiz3se6/checkpoints/vislstm cp=latest optim.th
10-30 14:27:33 I added default DatasetStatsCallback
10-30 14:27:33 I added default ParamCountCallback
10-30 14:27:33 I added default CopyPreviousConfigCallback
10-30 14:27:33 I added default CopyPreviousSummaryCallback
10-30 14:27:33 I added default ProgressCallback(every_n_epochs=1)
10-30 14:27:33 I added default TrainTimeCallback(every_n_epochs=1)
10-30 14:27:33 I added default OnlineLossCallback(every_n_epochs=1)
10-30 14:27:33 I added default LrCallback(every_n_updates=50)
10-30 14:27:33 I added default FreezerCallback(every_n_updates=50)
10-30 14:27:33 I added default OnlineLossCallback(every_n_updates=50)
10-30 14:27:33 I replacing BatchNorm layers with SyncBatchNorm
10-30 14:27:33 I wrapping model with torch.compile
10-30 14:27:34 I ------------------
10-30 14:27:34 I PREPARE TRAINER
10-30 14:27:34 I calculating batch_size and accumulation_steps (effective_batch_size=512)
10-30 14:27:34 I torch.compile is used -> automatic batchsize not supported
10-30 14:27:34 I train_batches per epoch: 2502 (world_size=4 batch_size=128)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
10-30 14:27:34 I initializing dataloader
10-30 14:27:34 I OfflineAccuracyCallback(every_n_epochs=5) registered InterleavedSamplerConfig(every_n_epochs=5) dataset_mode='x class'
10-30 14:27:34 I created dataloader (batch_size=128 num_workers=5 pin_memory=True total_cpu_count=64 prefetch_factor=2)
10-30 14:27:34 I concatenated dataset properties:
10-30 14:27:34 I - mode='index x class' len=1281167 root_dataset=<vislstm.datasets.imagenet1k.Imagenet1k object at 0x151be586fe50>
10-30 14:27:34 I - mode='x class' len=50000 root_dataset=<vislstm.datasets.imagenet1k.Imagenet1k object at 0x151be59b7b50>
10-30 14:27:34 I ------------------
10-30 14:27:34 I BEFORE TRAINING
10-30 14:27:34 I train: 1281167 samples
10-30 14:27:34 I val: 50000 samples
10-30 14:27:34 I parameter counts (trainable | frozen)
10-30 14:27:34 I 89,592,616 | 0 | vislstm
10-30 14:27:34 I estimated checkpoint size: 1.0GB
10-30 14:27:34 I estimated weight checkpoint size: 358.3MB
10-30 14:27:34 I estimated optim checkpoint size: 716.7MB
10-30 14:27:34 I estimated size for 1 checkpoints: 358.3MB
10-30 14:27:34 I estimated checkpoint size: 1.0GB
10-30 14:27:34 I estimated weight checkpoint size: 358.3MB
10-30 14:27:34 I estimated optim checkpoint size: 716.7MB
10-30 14:27:34 I estimated size for 41 checkpoints: 0.0B
10-30 14:27:34 I ------------------
10-30 14:27:34 I DatasetStatsCallback
10-30 14:27:34 I ParamCountCallback
10-30 14:27:34 I CopyPreviousConfigCallback
10-30 14:27:34 I CopyPreviousSummaryCallback
10-30 14:27:34 I ProgressCallback(every_n_epochs=1)
10-30 14:27:34 I TrainTimeCallback(every_n_epochs=1)
10-30 14:27:34 I OnlineLossCallback(every_n_epochs=1)
10-30 14:27:34 I LrCallback(every_n_updates=50)
10-30 14:27:34 I FreezerCallback(every_n_updates=50)
10-30 14:27:34 I OnlineLossCallback(every_n_updates=50)
10-30 14:27:34 I OnlineAccuracyCallback(every_n_updates=50)
10-30 14:27:34 I OnlineAccuracyCallback(every_n_epochs=1)
10-30 14:27:34 I CheckpointCallback()
10-30 14:27:34 I CheckpointCallback(every_n_epochs=10)
10-30 14:27:34 I OfflineAccuracyCallback(every_n_epochs=5)
10-30 14:27:34 I ------------------
10-30 14:27:34 I START TRAINING
10-30 14:27:34 I initializing dataloader workers
10-30 14:27:34 I initialized dataloader workers
[rank3]:W1030 14:27:35.851209 991783 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W1030 14:27:35.897855 991780 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W1030 14:27:35.925764 991781 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:W1030 14:27:35.971337 991782 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
10-30 14:28:58 I 0 unused parameters
10-30 14:37:56 I ------------------
10-30 14:37:56 I Epoch 81/400 (E81_U202662_S103762944)
10-30 14:37:56 I ETA: 10.30 15.18.43 estimated_duration: 00:51:08.95 time_since_last_log: 00:10:21.46 time_per_update: 00:00:00.00 
10-30 14:37:56 I data=[0.00, 0.00, 0.00, 0.00] update=[0.24, 0.24, 0.24, 0.24]
10-30 14:37:56 I loss/online/main/E1: 3.5561814308166504
10-30 14:37:56 I loss/online/total/E1: 3.5561814308166504
10-30 14:37:56 I accuracy1/online/main/E1: 0.426862
10-30 14:46:51 I ------------------
10-30 14:46:51 I Epoch 82/400 (E82_U205164_S105043968)
10-30 14:46:51 I ETA: 10.30 15.21.55 estimated_duration: 00:43:59.71 time_since_last_log: 00:08:55.88 time_per_update: 00:00:00.21 
10-30 14:46:51 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 14:46:51 I loss/online/main/E1: 3.548985719680786
10-30 14:46:51 I loss/online/total/E1: 3.548985719680786
10-30 14:46:51 I accuracy1/online/main/E1: 0.427758
10-30 14:55:49 I ------------------
10-30 14:55:49 I Epoch 83/400 (E83_U207666_S106324992)
10-30 14:55:49 I ETA: 10.30 16.04.57 estimated_duration: 01:27:01.42 time_since_last_log: 00:08:57.19 time_per_update: 00:00:00.21 
10-30 14:55:49 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 14:55:49 I loss/online/main/E1: 3.5509462356567383
10-30 14:55:49 I loss/online/total/E1: 3.5509462356567383
10-30 14:55:49 I accuracy1/online/main/E1: 0.429633
10-30 15:04:45 I ------------------
10-30 15:04:45 I Epoch 84/400 (E84_U210168_S107606016)
10-30 15:04:45 I ETA: 10.30 16.46.54 estimated_duration: 02:08:58.73 time_since_last_log: 00:08:56.73 time_per_update: 00:00:00.21 
10-30 15:04:45 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 15:04:45 I loss/online/main/E1: 3.539947509765625
10-30 15:04:45 I loss/online/total/E1: 3.539947509765625
10-30 15:04:45 I accuracy1/online/main/E1: 0.429412
10-30 15:13:40 I ------------------
10-30 15:13:40 I Epoch 85/400 (E85_U212670_S108887040)
10-30 15:13:40 I ETA: 10.30 17.27.43 estimated_duration: 02:49:47.17 time_since_last_log: 00:08:54.85 time_per_update: 00:00:00.21 
10-30 15:13:40 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 15:13:40 I loss/online/main/E1: 3.5416457653045654
10-30 15:13:40 I loss/online/total/E1: 3.5416457653045654
10-30 15:13:40 I accuracy1/online/main/E1: 0.431118
10-30 15:13:58 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.18
10-30 15:13:58 I accuracy1/val/main: 0.674640
10-30 15:13:58 I loss/val/main: 1.3671875
10-30 15:22:53 I ------------------
10-30 15:22:53 I Epoch 86/400 (E86_U215172_S110168064)
10-30 15:22:53 I ETA: 10.30 18.08.57 estimated_duration: 03:31:01.25 time_since_last_log: 00:09:12.59 time_per_update: 00:00:00.22 
10-30 15:22:53 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 15:22:53 I loss/online/main/E1: 3.5329320430755615
10-30 15:22:53 I loss/online/total/E1: 3.5329320430755615
10-30 15:22:53 I accuracy1/online/main/E1: 0.431122
10-30 15:31:49 I ------------------
10-30 15:31:49 I Epoch 87/400 (E87_U217674_S111449088)
10-30 15:31:49 I ETA: 10.30 18.47.56 estimated_duration: 04:09:59.96 time_since_last_log: 00:08:55.81 time_per_update: 00:00:00.21 
10-30 15:31:49 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 15:31:49 I loss/online/main/E1: 3.5309391021728516
10-30 15:31:49 I loss/online/total/E1: 3.5309391021728516
10-30 15:31:49 I accuracy1/online/main/E1: 0.431055
10-30 15:40:46 I ------------------
10-30 15:40:46 I Epoch 88/400 (E88_U220176_S112730112)
10-30 15:40:46 I ETA: 10.30 19.26.06 estimated_duration: 04:48:10.21 time_since_last_log: 00:08:56.97 time_per_update: 00:00:00.21 
10-30 15:40:46 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 15:40:46 I loss/online/main/E1: 3.518096446990967
10-30 15:40:46 I loss/online/total/E1: 3.518096446990967
10-30 15:40:46 I accuracy1/online/main/E1: 0.434267
10-30 15:49:40 I ------------------
10-30 15:49:40 I Epoch 89/400 (E89_U222678_S114011136)
10-30 15:49:40 I ETA: 10.30 20.03.14 estimated_duration: 05:25:18.56 time_since_last_log: 00:08:54.79 time_per_update: 00:00:00.21 
10-30 15:49:40 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 15:49:40 I loss/online/main/E1: 3.513458251953125
10-30 15:49:40 I loss/online/total/E1: 3.513458251953125
10-30 15:49:40 I accuracy1/online/main/E1: 0.435433
10-30 15:58:36 I ------------------
10-30 15:58:36 I Epoch 90/400 (E90_U225180_S115292160)
10-30 15:58:36 I ETA: 10.30 20.39.37 estimated_duration: 06:01:41.31 time_since_last_log: 00:08:55.79 time_per_update: 00:00:00.21 
10-30 15:58:36 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 15:58:36 I loss/online/main/E1: 3.50333309173584
10-30 15:58:36 I loss/online/total/E1: 3.50333309173584
10-30 15:58:36 I accuracy1/online/main/E1: 0.435264
10-30 15:58:37 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest model.th
10-30 15:58:39 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest optim.th
10-30 15:58:39 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/trainer cp=latest.th
10-30 15:58:56 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-30 15:58:56 I accuracy1/val/main: 0.673900
10-30 15:58:56 I loss/val/main: 1.3671875
10-30 16:07:54 I ------------------
10-30 16:07:54 I Epoch 91/400 (E91_U227682_S116573184)
10-30 16:07:54 I ETA: 10.30 21.16.49 estimated_duration: 06:38:53.22 time_since_last_log: 00:09:17.82 time_per_update: 00:00:00.22 
10-30 16:07:54 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 16:07:54 I loss/online/main/E1: 3.5069267749786377
10-30 16:07:54 I loss/online/total/E1: 3.5069267749786377
10-30 16:07:54 I accuracy1/online/main/E1: 0.436268
10-30 16:16:52 I ------------------
10-30 16:16:52 I Epoch 92/400 (E92_U230184_S117854208)
10-30 16:16:52 I ETA: 10.30 21.51.44 estimated_duration: 07:13:48.84 time_since_last_log: 00:08:57.93 time_per_update: 00:00:00.21 
10-30 16:16:52 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 16:16:52 I loss/online/main/E1: 3.506927728652954
10-30 16:16:52 I loss/online/total/E1: 3.506927728652954
10-30 16:16:52 I accuracy1/online/main/E1: 0.437198
10-30 16:25:47 I ------------------
10-30 16:25:47 I Epoch 93/400 (E93_U232686_S119135232)
10-30 16:25:47 I ETA: 10.30 22.25.42 estimated_duration: 07:47:46.82 time_since_last_log: 00:08:55.14 time_per_update: 00:00:00.21 
10-30 16:25:47 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 16:25:47 I loss/online/main/E1: 3.4882569313049316
10-30 16:25:47 I loss/online/total/E1: 3.4882569313049316
10-30 16:25:47 I accuracy1/online/main/E1: 0.438562
10-30 16:34:44 I ------------------
10-30 16:34:44 I Epoch 94/400 (E94_U235188_S120416256)
10-30 16:34:44 I ETA: 10.30 22.59.02 estimated_duration: 08:21:06.88 time_since_last_log: 00:08:56.52 time_per_update: 00:00:00.21 
10-30 16:34:44 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 16:34:44 I loss/online/main/E1: 3.4803121089935303
10-30 16:34:44 I loss/online/total/E1: 3.4803121089935303
10-30 16:34:44 I accuracy1/online/main/E1: 0.440463
10-30 16:43:40 I ------------------
10-30 16:43:40 I Epoch 95/400 (E95_U237690_S121697280)
10-30 16:43:40 I ETA: 10.30 23.31.39 estimated_duration: 08:53:43.68 time_since_last_log: 00:08:56.35 time_per_update: 00:00:00.21 
10-30 16:43:40 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 16:43:40 I loss/online/main/E1: 3.483492374420166
10-30 16:43:40 I loss/online/total/E1: 3.483492374420166
10-30 16:43:40 I accuracy1/online/main/E1: 0.439514
10-30 16:43:57 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-30 16:43:57 I accuracy1/val/main: 0.678860
10-30 16:43:57 I loss/val/main: 1.328125
10-30 16:52:55 I ------------------
10-30 16:52:55 I Epoch 96/400 (E96_U240192_S122978304)
10-30 16:52:55 I ETA: 10.31 00.04.52 estimated_duration: 09:26:56.76 time_since_last_log: 00:09:14.80 time_per_update: 00:00:00.22 
10-30 16:52:55 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 16:52:55 I loss/online/main/E1: 3.4722936153411865
10-30 16:52:55 I loss/online/total/E1: 3.4722936153411865
10-30 16:52:55 I accuracy1/online/main/E1: 0.441434
10-30 17:01:51 I ------------------
10-30 17:01:51 I Epoch 97/400 (E97_U242694_S124259328)
10-30 17:01:51 I ETA: 10.31 00.36.05 estimated_duration: 09:58:09.62 time_since_last_log: 00:08:55.86 time_per_update: 00:00:00.21 
10-30 17:01:51 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 17:01:51 I loss/online/main/E1: 3.4654860496520996
10-30 17:01:51 I loss/online/total/E1: 3.4654860496520996
10-30 17:01:51 I accuracy1/online/main/E1: 0.443376
10-30 17:10:47 I ------------------
10-30 17:10:47 I Epoch 98/400 (E98_U245196_S125540352)
10-30 17:10:47 I ETA: 10.31 01.06.41 estimated_duration: 10:28:45.41 time_since_last_log: 00:08:56.24 time_per_update: 00:00:00.21 
10-30 17:10:47 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 17:10:47 I loss/online/main/E1: 3.4529688358306885
10-30 17:10:47 I loss/online/total/E1: 3.4529688358306885
10-30 17:10:47 I accuracy1/online/main/E1: 0.443588
10-30 17:19:43 I ------------------
10-30 17:19:43 I Epoch 99/400 (E99_U247698_S126821376)
10-30 17:19:43 I ETA: 10.31 01.36.40 estimated_duration: 10:58:44.44 time_since_last_log: 00:08:56.41 time_per_update: 00:00:00.21 
10-30 17:19:43 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 17:19:43 I loss/online/main/E1: 3.4430367946624756
10-30 17:19:43 I loss/online/total/E1: 3.4430367946624756
10-30 17:19:43 I accuracy1/online/main/E1: 0.444597
10-30 17:28:42 I ------------------
10-30 17:28:42 I Epoch 100/400 (E100_U250200_S128102400)
10-30 17:28:42 I ETA: 10.31 02.06.11 estimated_duration: 11:28:15.87 time_since_last_log: 00:08:58.58 time_per_update: 00:00:00.21 
10-30 17:28:42 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 17:28:42 I loss/online/main/E1: 3.450331211090088
10-30 17:28:42 I loss/online/total/E1: 3.450331211090088
10-30 17:28:42 I accuracy1/online/main/E1: 0.444047
10-30 17:28:43 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest model.th
10-30 17:28:44 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest optim.th
10-30 17:28:44 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/trainer cp=latest.th
10-30 17:29:01 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-30 17:29:01 I accuracy1/val/main: 0.679640
10-30 17:29:01 I loss/val/main: 1.328125
10-30 17:37:57 I ------------------
10-30 17:37:57 I Epoch 101/400 (E101_U252702_S129383424)
10-30 17:37:57 I ETA: 10.31 02.36.14 estimated_duration: 11:58:18.60 time_since_last_log: 00:09:15.31 time_per_update: 00:00:00.22 
10-30 17:37:57 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 17:37:57 I loss/online/main/E1: 3.4462859630584717
10-30 17:37:57 I loss/online/total/E1: 3.4462859630584717
10-30 17:37:57 I accuracy1/online/main/E1: 0.446026
10-30 17:46:54 I ------------------
10-30 17:46:54 I Epoch 102/400 (E102_U255204_S130664448)
10-30 17:46:54 I ETA: 10.31 03.04.26 estimated_duration: 12:26:30.42 time_since_last_log: 00:08:56.27 time_per_update: 00:00:00.21 
10-30 17:46:54 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 17:46:54 I loss/online/main/E1: 3.4343414306640625
10-30 17:46:54 I loss/online/total/E1: 3.4343414306640625
10-30 17:46:54 I accuracy1/online/main/E1: 0.447436
10-30 17:55:50 I ------------------
10-30 17:55:50 I Epoch 103/400 (E103_U257706_S131945472)
10-30 17:55:50 I ETA: 10.31 03.32.06 estimated_duration: 12:54:10.64 time_since_last_log: 00:08:56.67 time_per_update: 00:00:00.21 
10-30 17:55:50 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 17:55:50 I loss/online/main/E1: 3.4455881118774414
10-30 17:55:50 I loss/online/total/E1: 3.4455881118774414
10-30 17:55:50 I accuracy1/online/main/E1: 0.444806
10-30 18:04:47 I ------------------
10-30 18:04:47 I Epoch 104/400 (E104_U260208_S133226496)
10-30 18:04:47 I ETA: 10.31 03.59.15 estimated_duration: 13:21:19.87 time_since_last_log: 00:08:56.99 time_per_update: 00:00:00.21 
10-30 18:04:47 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 18:04:47 I loss/online/main/E1: 3.4271466732025146
10-30 18:04:47 I loss/online/total/E1: 3.4271466732025146
10-30 18:04:47 I accuracy1/online/main/E1: 0.448023
10-30 18:13:43 I ------------------
10-30 18:13:43 I Epoch 105/400 (E105_U262710_S134507520)
10-30 18:13:43 I ETA: 10.31 04.25.50 estimated_duration: 13:47:53.98 time_since_last_log: 00:08:56.00 time_per_update: 00:00:00.21 
10-30 18:13:43 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 18:13:43 I loss/online/main/E1: 3.4380416870117188
10-30 18:13:43 I loss/online/total/E1: 3.4380416870117188
10-30 18:13:43 I accuracy1/online/main/E1: 0.446939
10-30 18:14:00 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-30 18:14:00 I accuracy1/val/main: 0.688620
10-30 18:14:00 I loss/val/main: 1.3046875
10-30 18:22:57 I ------------------
10-30 18:22:57 I Epoch 106/400 (E106_U265212_S135788544)
10-30 18:22:57 I ETA: 10.31 04.53.02 estimated_duration: 14:15:06.59 time_since_last_log: 00:09:14.13 time_per_update: 00:00:00.22 
10-30 18:22:57 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 18:22:57 I loss/online/main/E1: 3.4253506660461426
10-30 18:22:57 I loss/online/total/E1: 3.4253506660461426
10-30 18:22:57 I accuracy1/online/main/E1: 0.448903
10-30 18:31:56 I ------------------
10-30 18:31:56 I Epoch 107/400 (E107_U267714_S137069568)
10-30 18:31:56 I ETA: 10.31 05.18.44 estimated_duration: 14:40:48.76 time_since_last_log: 00:08:58.28 time_per_update: 00:00:00.21 
10-30 18:31:56 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 18:31:56 I loss/online/main/E1: 3.4167392253875732
10-30 18:31:56 I loss/online/total/E1: 3.4167392253875732
10-30 18:31:56 I accuracy1/online/main/E1: 0.451059
10-30 18:40:51 I ------------------
10-30 18:40:51 I Epoch 108/400 (E108_U270216_S138350592)
10-30 18:40:51 I ETA: 10.31 05.43.48 estimated_duration: 15:05:52.68 time_since_last_log: 00:08:55.76 time_per_update: 00:00:00.21 
10-30 18:40:51 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 18:40:51 I loss/online/main/E1: 3.419718027114868
10-30 18:40:51 I loss/online/total/E1: 3.419718027114868
10-30 18:40:51 I accuracy1/online/main/E1: 0.450757
10-30 18:49:49 I ------------------
10-30 18:49:49 I Epoch 109/400 (E109_U272718_S139631616)
10-30 18:49:49 I ETA: 10.31 06.08.31 estimated_duration: 15:30:35.69 time_since_last_log: 00:08:57.63 time_per_update: 00:00:00.21 
10-30 18:49:49 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 18:49:49 I loss/online/main/E1: 3.4009017944335938
10-30 18:49:49 I loss/online/total/E1: 3.4009017944335938
10-30 18:49:49 I accuracy1/online/main/E1: 0.453393
10-30 18:58:46 I ------------------
10-30 18:58:46 I Epoch 110/400 (E110_U275220_S140912640)
10-30 18:58:46 I ETA: 10.31 06.32.43 estimated_duration: 15:54:47.92 time_since_last_log: 00:08:56.66 time_per_update: 00:00:00.21 
10-30 18:58:46 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 18:58:46 I loss/online/main/E1: 3.4099807739257812
10-30 18:58:46 I loss/online/total/E1: 3.4099807739257812
10-30 18:58:46 I accuracy1/online/main/E1: 0.451001
10-30 18:58:46 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest model.th
10-30 18:58:48 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest optim.th
10-30 18:58:48 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/trainer cp=latest.th
10-30 18:59:05 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-30 18:59:05 I accuracy1/val/main: 0.687200
10-30 18:59:05 I loss/val/main: 1.28125
10-30 19:08:03 I ------------------
10-30 19:08:03 I Epoch 111/400 (E111_U277722_S142193664)
10-30 19:08:03 I ETA: 10.31 06.57.45 estimated_duration: 16:19:49.16 time_since_last_log: 00:09:17.45 time_per_update: 00:00:00.22 
10-30 19:08:03 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 19:08:03 I loss/online/main/E1: 3.390220880508423
10-30 19:08:03 I loss/online/total/E1: 3.390220880508423
10-30 19:08:03 I accuracy1/online/main/E1: 0.454808
10-30 19:17:01 I ------------------
10-30 19:17:01 I Epoch 112/400 (E112_U280224_S143474688)
10-30 19:17:01 I ETA: 10.31 07.21.09 estimated_duration: 16:43:13.40 time_since_last_log: 00:08:57.99 time_per_update: 00:00:00.21 
10-30 19:17:01 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 19:17:01 I loss/online/main/E1: 3.396958827972412
10-30 19:17:01 I loss/online/total/E1: 3.396958827972412
10-30 19:17:01 I accuracy1/online/main/E1: 0.454436
10-30 19:26:01 I ------------------
10-30 19:26:01 I Epoch 113/400 (E113_U282726_S144755712)
10-30 19:26:01 I ETA: 10.31 07.44.15 estimated_duration: 17:06:19.17 time_since_last_log: 00:08:59.84 time_per_update: 00:00:00.21 
10-30 19:26:01 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 19:26:01 I loss/online/main/E1: 3.382164478302002
10-30 19:26:01 I loss/online/total/E1: 3.382164478302002
10-30 19:26:01 I accuracy1/online/main/E1: 0.456691
10-30 19:34:59 I ------------------
10-30 19:34:59 I Epoch 114/400 (E114_U285228_S146036736)
10-30 19:34:59 I ETA: 10.31 08.06.49 estimated_duration: 17:28:53.64 time_since_last_log: 00:08:57.92 time_per_update: 00:00:00.21 
10-30 19:34:59 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 19:34:59 I loss/online/main/E1: 3.3798675537109375
10-30 19:34:59 I loss/online/total/E1: 3.3798675537109375
10-30 19:34:59 I accuracy1/online/main/E1: 0.456731
10-30 19:43:59 I ------------------
10-30 19:43:59 I Epoch 115/400 (E115_U287730_S147317760)
10-30 19:43:59 I ETA: 10.31 08.29.07 estimated_duration: 17:51:11.21 time_since_last_log: 00:08:59.89 time_per_update: 00:00:00.21 
10-30 19:43:59 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 19:43:59 I loss/online/main/E1: 3.3737294673919678
10-30 19:43:59 I loss/online/total/E1: 3.3737294673919678
10-30 19:43:59 I accuracy1/online/main/E1: 0.457866
10-30 19:44:16 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-30 19:44:16 I accuracy1/val/main: 0.694540
10-30 19:44:16 I loss/val/main: 1.2734375
10-30 19:53:13 I ------------------
10-30 19:53:13 I Epoch 116/400 (E116_U290232_S148598784)
10-30 19:53:13 I ETA: 10.31 08.51.50 estimated_duration: 18:13:54.92 time_since_last_log: 00:09:14.12 time_per_update: 00:00:00.22 
10-30 19:53:13 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 19:53:13 I loss/online/main/E1: 3.372281551361084
10-30 19:53:13 I loss/online/total/E1: 3.372281551361084
10-30 19:53:13 I accuracy1/online/main/E1: 0.457482
10-30 20:02:09 I ------------------
10-30 20:02:09 I Epoch 117/400 (E117_U292734_S149879808)
10-30 20:02:09 I ETA: 10.31 09.13.09 estimated_duration: 18:35:13.76 time_since_last_log: 00:08:56.29 time_per_update: 00:00:00.21 
10-30 20:02:09 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 20:02:09 I loss/online/main/E1: 3.3681979179382324
10-30 20:02:09 I loss/online/total/E1: 3.3681979179382324
10-30 20:02:09 I accuracy1/online/main/E1: 0.457680
10-30 20:11:06 I ------------------
10-30 20:11:06 I Epoch 118/400 (E118_U295236_S151160832)
10-30 20:11:06 I ETA: 10.31 09.34.07 estimated_duration: 18:56:11.67 time_since_last_log: 00:08:56.56 time_per_update: 00:00:00.21 
10-30 20:11:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 20:11:06 I loss/online/main/E1: 3.36767578125
10-30 20:11:06 I loss/online/total/E1: 3.36767578125
10-30 20:11:06 I accuracy1/online/main/E1: 0.459673
10-30 20:20:03 I ------------------
10-30 20:20:03 I Epoch 119/400 (E119_U297738_S152441856)
10-30 20:20:03 I ETA: 10.31 09.54.46 estimated_duration: 19:16:49.93 time_since_last_log: 00:08:57.05 time_per_update: 00:00:00.21 
10-30 20:20:03 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 20:20:03 I loss/online/main/E1: 3.3682565689086914
10-30 20:20:03 I loss/online/total/E1: 3.3682565689086914
10-30 20:20:03 I accuracy1/online/main/E1: 0.459123
10-30 20:29:00 I ------------------
10-30 20:29:00 I Epoch 120/400 (E120_U300240_S153722880)
10-30 20:29:00 I ETA: 10.31 10.15.02 estimated_duration: 19:37:06.14 time_since_last_log: 00:08:56.68 time_per_update: 00:00:00.21 
10-30 20:29:00 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 20:29:00 I loss/online/main/E1: 3.3528811931610107
10-30 20:29:00 I loss/online/total/E1: 3.3528811931610107
10-30 20:29:00 I accuracy1/online/main/E1: 0.461915
10-30 20:29:00 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest model.th
10-30 20:29:02 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest optim.th
10-30 20:29:02 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/trainer cp=latest.th
10-30 20:29:19 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-30 20:29:19 I accuracy1/val/main: 0.694180
10-30 20:29:19 I loss/val/main: 1.234375
10-30 20:38:16 I ------------------
10-30 20:38:16 I Epoch 121/400 (E121_U302742_S155003904)
10-30 20:38:16 I ETA: 10.31 10.36.04 estimated_duration: 19:58:08.42 time_since_last_log: 00:09:16.64 time_per_update: 00:00:00.22 
10-30 20:38:16 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 20:38:16 I loss/online/main/E1: 3.350801706314087
10-30 20:38:16 I loss/online/total/E1: 3.350801706314087
10-30 20:38:16 I accuracy1/online/main/E1: 0.460802
10-30 20:47:13 I ------------------
10-30 20:47:13 I Epoch 122/400 (E122_U305244_S156284928)
10-30 20:47:13 I ETA: 10.31 10.55.41 estimated_duration: 20:17:45.52 time_since_last_log: 00:08:57.13 time_per_update: 00:00:00.21 
10-30 20:47:13 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 20:47:13 I loss/online/main/E1: 3.334214210510254
10-30 20:47:13 I loss/online/total/E1: 3.334214210510254
10-30 20:47:13 I accuracy1/online/main/E1: 0.463312
10-30 20:56:11 I ------------------
10-30 20:56:11 I Epoch 123/400 (E123_U307746_S157565952)
10-30 20:56:11 I ETA: 10.31 11.14.59 estimated_duration: 20:37:03.66 time_since_last_log: 00:08:57.24 time_per_update: 00:00:00.21 
10-30 20:56:11 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 20:56:11 I loss/online/main/E1: 3.3508195877075195
10-30 20:56:11 I loss/online/total/E1: 3.3508195877075195
10-30 20:56:11 I accuracy1/online/main/E1: 0.461909
10-30 21:05:07 I ------------------
10-30 21:05:07 I Epoch 124/400 (E124_U310248_S158846976)
10-30 21:05:07 I ETA: 10.31 11.33.57 estimated_duration: 20:56:01.23 time_since_last_log: 00:08:56.70 time_per_update: 00:00:00.21 
10-30 21:05:07 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 21:05:07 I loss/online/main/E1: 3.331669330596924
10-30 21:05:07 I loss/online/total/E1: 3.331669330596924
10-30 21:05:07 I accuracy1/online/main/E1: 0.464271
10-30 21:14:04 I ------------------
10-30 21:14:04 I Epoch 125/400 (E125_U312750_S160128000)
10-30 21:14:04 I ETA: 10.31 11.52.36 estimated_duration: 21:14:40.25 time_since_last_log: 00:08:56.64 time_per_update: 00:00:00.21 
10-30 21:14:04 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 21:14:04 I loss/online/main/E1: 3.32779860496521
10-30 21:14:04 I loss/online/total/E1: 3.32779860496521
10-30 21:14:04 I accuracy1/online/main/E1: 0.466082
10-30 21:14:21 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-30 21:14:21 I accuracy1/val/main: 0.700040
10-30 21:14:21 I loss/val/main: 1.2109375
10-30 21:23:17 I ------------------
10-30 21:23:17 I Epoch 126/400 (E126_U315252_S161409024)
10-30 21:23:17 I ETA: 10.31 12.11.51 estimated_duration: 21:33:55.56 time_since_last_log: 00:09:13.61 time_per_update: 00:00:00.22 
10-30 21:23:17 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 21:23:17 I loss/online/main/E1: 3.332915782928467
10-30 21:23:18 I loss/online/total/E1: 3.332915782928467
10-30 21:23:18 I accuracy1/online/main/E1: 0.464381
10-30 21:32:14 I ------------------
10-30 21:32:14 I Epoch 127/400 (E127_U317754_S162690048)
10-30 21:32:14 I ETA: 10.31 12.29.55 estimated_duration: 21:51:59.84 time_since_last_log: 00:08:56.97 time_per_update: 00:00:00.21 
10-30 21:32:14 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 21:32:14 I loss/online/main/E1: 3.328967809677124
10-30 21:32:14 I loss/online/total/E1: 3.328967809677124
10-30 21:32:14 I accuracy1/online/main/E1: 0.465531
10-30 21:41:10 I ------------------
10-30 21:41:10 I Epoch 128/400 (E128_U320256_S163971072)
10-30 21:41:10 I ETA: 10.31 12.47.40 estimated_duration: 22:09:43.96 time_since_last_log: 00:08:55.99 time_per_update: 00:00:00.21 
10-30 21:41:10 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 21:41:10 I loss/online/main/E1: 3.3271193504333496
10-30 21:41:10 I loss/online/total/E1: 3.3271193504333496
10-30 21:41:10 I accuracy1/online/main/E1: 0.465644
10-30 21:50:08 I ------------------
10-30 21:50:08 I Epoch 129/400 (E129_U322758_S165252096)
10-30 21:50:08 I ETA: 10.31 13.05.10 estimated_duration: 22:27:14.80 time_since_last_log: 00:08:57.07 time_per_update: 00:00:00.21 
10-30 21:50:08 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 21:50:08 I loss/online/main/E1: 3.3142848014831543
10-30 21:50:08 I loss/online/total/E1: 3.3142848014831543
10-30 21:50:08 I accuracy1/online/main/E1: 0.467394
10-30 21:59:05 I ------------------
10-30 21:59:05 I Epoch 130/400 (E130_U325260_S166533120)
10-30 21:59:05 I ETA: 10.31 13.22.26 estimated_duration: 22:44:30.48 time_since_last_log: 00:08:57.43 time_per_update: 00:00:00.21 
10-30 21:59:05 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 21:59:05 I loss/online/main/E1: 3.2981884479522705
10-30 21:59:05 I loss/online/total/E1: 3.2981884479522705
10-30 21:59:05 I accuracy1/online/main/E1: 0.470537
10-30 21:59:06 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest model.th
10-30 21:59:07 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest optim.th
10-30 21:59:07 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/trainer cp=latest.th
10-30 21:59:24 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-30 21:59:24 I accuracy1/val/main: 0.700020
10-30 21:59:24 I loss/val/main: 1.2109375
10-30 22:08:20 I ------------------
10-30 22:08:20 I Epoch 131/400 (E131_U327762_S167814144)
10-30 22:08:20 I ETA: 10.31 13.40.20 estimated_duration: 23:02:24.45 time_since_last_log: 00:09:15.10 time_per_update: 00:00:00.22 
10-30 22:08:20 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 22:08:20 I loss/online/main/E1: 3.308516502380371
10-30 22:08:20 I loss/online/total/E1: 3.308516502380371
10-30 22:08:20 I accuracy1/online/main/E1: 0.469132
10-30 22:17:16 I ------------------
10-30 22:17:16 I Epoch 132/400 (E132_U330264_S169095168)
10-30 22:17:16 I ETA: 10.31 13.56.59 estimated_duration: 23:19:03.60 time_since_last_log: 00:08:55.92 time_per_update: 00:00:00.21 
10-30 22:17:16 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 22:17:16 I loss/online/main/E1: 3.314453601837158
10-30 22:17:16 I loss/online/total/E1: 3.314453601837158
10-30 22:17:16 I accuracy1/online/main/E1: 0.467157
10-30 22:26:12 I ------------------
10-30 22:26:12 I Epoch 133/400 (E133_U332766_S170376192)
10-30 22:26:12 I ETA: 10.31 14.13.25 estimated_duration: 23:35:29.30 time_since_last_log: 00:08:56.48 time_per_update: 00:00:00.21 
10-30 22:26:12 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 22:26:12 I loss/online/main/E1: 3.3024532794952393
10-30 22:26:12 I loss/online/total/E1: 3.3024532794952393
10-30 22:26:13 I accuracy1/online/main/E1: 0.469210
10-30 22:35:10 I ------------------
10-30 22:35:10 I Epoch 134/400 (E134_U335268_S171657216)
10-30 22:35:10 I ETA: 10.31 14.29.40 estimated_duration: 23:51:44.36 time_since_last_log: 00:08:57.87 time_per_update: 00:00:00.21 
10-30 22:35:10 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 22:35:10 I loss/online/main/E1: 3.300036907196045
10-30 22:35:10 I loss/online/total/E1: 3.300036907196045
10-30 22:35:10 I accuracy1/online/main/E1: 0.470118
10-30 22:44:06 I ------------------
10-30 22:44:06 I Epoch 135/400 (E135_U337770_S172938240)
10-30 22:44:06 I ETA: 10.31 14.45.32 estimated_duration: 1-00:07:36.79 time_since_last_log: 00:08:55.16 time_per_update: 00:00:00.21 
10-30 22:44:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 22:44:06 I loss/online/main/E1: 3.2937421798706055
10-30 22:44:06 I loss/online/total/E1: 3.2937421798706055
10-30 22:44:06 I accuracy1/online/main/E1: 0.471118
10-30 22:44:23 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-30 22:44:23 I accuracy1/val/main: 0.707280
10-30 22:44:23 I loss/val/main: 1.2109375
10-30 22:53:18 I ------------------
10-30 22:53:18 I Epoch 136/400 (E136_U340272_S174219264)
10-30 22:53:18 I ETA: 10.31 15.02.01 estimated_duration: 1-00:24:05.65 time_since_last_log: 00:09:12.26 time_per_update: 00:00:00.22 
10-30 22:53:18 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 22:53:18 I loss/online/main/E1: 3.2882394790649414
10-30 22:53:18 I loss/online/total/E1: 3.2882394790649414
10-30 22:53:18 I accuracy1/online/main/E1: 0.472125
10-30 23:02:13 I ------------------
10-30 23:02:13 I Epoch 137/400 (E137_U342774_S175500288)
10-30 23:02:13 I ETA: 10.31 15.17.26 estimated_duration: 1-00:39:30.67 time_since_last_log: 00:08:55.45 time_per_update: 00:00:00.21 
10-30 23:02:13 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 23:02:13 I loss/online/main/E1: 3.2776975631713867
10-30 23:02:13 I loss/online/total/E1: 3.2776975631713867
10-30 23:02:13 I accuracy1/online/main/E1: 0.473113
10-30 23:11:10 I ------------------
10-30 23:11:10 I Epoch 138/400 (E138_U345276_S176781312)
10-30 23:11:10 I ETA: 10.31 15.32.42 estimated_duration: 1-00:54:46.88 time_since_last_log: 00:08:57.07 time_per_update: 00:00:00.21 
10-30 23:11:10 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 23:11:10 I loss/online/main/E1: 3.2839789390563965
10-30 23:11:10 I loss/online/total/E1: 3.2839789390563965
10-30 23:11:10 I accuracy1/online/main/E1: 0.472485
10-30 23:20:06 I ------------------
10-30 23:20:06 I Epoch 139/400 (E139_U347778_S178062336)
10-30 23:20:06 I ETA: 10.31 15.47.40 estimated_duration: 1-01:09:44.63 time_since_last_log: 00:08:55.27 time_per_update: 00:00:00.21 
10-30 23:20:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 23:20:06 I loss/online/main/E1: 3.2797484397888184
10-30 23:20:06 I loss/online/total/E1: 3.2797484397888184
10-30 23:20:06 I accuracy1/online/main/E1: 0.473403
10-30 23:29:01 I ------------------
10-30 23:29:01 I Epoch 140/400 (E140_U350280_S179343360)
10-30 23:29:01 I ETA: 10.31 16.02.26 estimated_duration: 1-01:24:30.81 time_since_last_log: 00:08:55.74 time_per_update: 00:00:00.21 
10-30 23:29:01 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 23:29:01 I loss/online/main/E1: 3.2749593257904053
10-30 23:29:01 I loss/online/total/E1: 3.2749593257904053
10-30 23:29:01 I accuracy1/online/main/E1: 0.474182
10-30 23:29:02 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest model.th
10-30 23:29:04 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest optim.th
10-30 23:29:04 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/trainer cp=latest.th
10-30 23:29:21 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-30 23:29:21 I accuracy1/val/main: 0.707140
10-30 23:29:21 I loss/val/main: 1.1953125
10-30 23:38:16 I ------------------
10-30 23:38:16 I Epoch 141/400 (E141_U352782_S180624384)
10-30 23:38:16 I ETA: 10.31 16.17.53 estimated_duration: 1-01:39:57.65 time_since_last_log: 00:09:14.45 time_per_update: 00:00:00.22 
10-30 23:38:16 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 23:38:16 I loss/online/main/E1: 3.2712104320526123
10-30 23:38:16 I loss/online/total/E1: 3.2712104320526123
10-30 23:38:16 I accuracy1/online/main/E1: 0.474644
10-30 23:47:13 I ------------------
10-30 23:47:13 I Epoch 142/400 (E142_U355284_S181905408)
10-30 23:47:13 I ETA: 10.31 16.32.18 estimated_duration: 1-01:54:22.52 time_since_last_log: 00:08:57.20 time_per_update: 00:00:00.21 
10-30 23:47:13 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 23:47:13 I loss/online/main/E1: 3.270061492919922
10-30 23:47:13 I loss/online/total/E1: 3.270061492919922
10-30 23:47:13 I accuracy1/online/main/E1: 0.475691
10-30 23:56:08 I ------------------
10-30 23:56:08 I Epoch 143/400 (E143_U357786_S183186432)
10-30 23:56:08 I ETA: 10.31 16.46.26 estimated_duration: 1-02:08:30.00 time_since_last_log: 00:08:55.35 time_per_update: 00:00:00.21 
10-30 23:56:08 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-30 23:56:08 I loss/online/main/E1: 3.2528700828552246
10-30 23:56:08 I loss/online/total/E1: 3.2528700828552246
10-30 23:56:08 I accuracy1/online/main/E1: 0.477166
10-31 00:05:05 I ------------------
10-31 00:05:05 I Epoch 144/400 (E144_U360288_S184467456)
10-31 00:05:05 I ETA: 10.31 17.00.24 estimated_duration: 1-02:22:28.42 time_since_last_log: 00:08:56.35 time_per_update: 00:00:00.21 
10-31 00:05:05 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 00:05:05 I loss/online/main/E1: 3.255333423614502
10-31 00:05:05 I loss/online/total/E1: 3.255333423614502
10-31 00:05:05 I accuracy1/online/main/E1: 0.478339
10-31 00:14:01 I ------------------
10-31 00:14:01 I Epoch 145/400 (E145_U362790_S185748480)
10-31 00:14:01 I ETA: 10.31 17.14.10 estimated_duration: 1-02:36:14.03 time_since_last_log: 00:08:55.92 time_per_update: 00:00:00.21 
10-31 00:14:01 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 00:14:01 I loss/online/main/E1: 3.245234966278076
10-31 00:14:01 I loss/online/total/E1: 3.245234966278076
10-31 00:14:01 I accuracy1/online/main/E1: 0.479351
10-31 00:14:18 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 00:14:18 I accuracy1/val/main: 0.713100
10-31 00:14:18 I loss/val/main: 1.1875
10-31 00:23:15 I ------------------
10-31 00:23:15 I Epoch 146/400 (E146_U365292_S187029504)
10-31 00:23:15 I ETA: 10.31 17.28.34 estimated_duration: 1-02:50:38.24 time_since_last_log: 00:09:14.09 time_per_update: 00:00:00.22 
10-31 00:23:15 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 00:23:15 I loss/online/main/E1: 3.2375380992889404
10-31 00:23:15 I loss/online/total/E1: 3.2375380992889404
10-31 00:23:15 I accuracy1/online/main/E1: 0.480192
10-31 00:32:11 I ------------------
10-31 00:32:11 I Epoch 147/400 (E147_U367794_S188310528)
10-31 00:32:11 I ETA: 10.31 17.41.59 estimated_duration: 1-03:04:02.94 time_since_last_log: 00:08:56.65 time_per_update: 00:00:00.21 
10-31 00:32:11 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 00:32:11 I loss/online/main/E1: 3.2401390075683594
10-31 00:32:11 I loss/online/total/E1: 3.2401390075683594
10-31 00:32:11 I accuracy1/online/main/E1: 0.480694
10-31 00:41:07 I ------------------
10-31 00:41:07 I Epoch 148/400 (E148_U370296_S189591552)
10-31 00:41:07 I ETA: 10.31 17.55.10 estimated_duration: 1-03:17:14.32 time_since_last_log: 00:08:55.77 time_per_update: 00:00:00.21 
10-31 00:41:07 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 00:41:07 I loss/online/main/E1: 3.226226329803467
10-31 00:41:07 I loss/online/total/E1: 3.226226329803467
10-31 00:41:07 I accuracy1/online/main/E1: 0.481864
10-31 00:50:03 I ------------------
10-31 00:50:03 I Epoch 149/400 (E149_U372798_S190872576)
10-31 00:50:03 I ETA: 10.31 18.08.11 estimated_duration: 1-03:30:15.68 time_since_last_log: 00:08:56.02 time_per_update: 00:00:00.21 
10-31 00:50:03 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 00:50:03 I loss/online/main/E1: 3.2334744930267334
10-31 00:50:03 I loss/online/total/E1: 3.2334744930267334
10-31 00:50:03 I accuracy1/online/main/E1: 0.479831
10-31 00:59:01 I ------------------
10-31 00:59:01 I Epoch 150/400 (E150_U375300_S192153600)
10-31 00:59:01 I ETA: 10.31 18.21.06 estimated_duration: 1-03:43:10.84 time_since_last_log: 00:08:57.62 time_per_update: 00:00:00.21 
10-31 00:59:01 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 00:59:01 I loss/online/main/E1: 3.233314037322998
10-31 00:59:01 I loss/online/total/E1: 3.233314037322998
10-31 00:59:01 I accuracy1/online/main/E1: 0.481027
10-31 00:59:02 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest model.th
10-31 00:59:03 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest optim.th
10-31 00:59:03 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/trainer cp=latest.th
10-31 00:59:20 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 00:59:20 I accuracy1/val/main: 0.712480
10-31 00:59:20 I loss/val/main: 1.1640625
10-31 01:08:17 I ------------------
10-31 01:08:17 I Epoch 151/400 (E151_U377802_S193434624)
10-31 01:08:17 I ETA: 10.31 18.34.41 estimated_duration: 1-03:56:45.40 time_since_last_log: 00:09:16.32 time_per_update: 00:00:00.22 
10-31 01:08:17 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 01:08:17 I loss/online/main/E1: 3.2243151664733887
10-31 01:08:17 I loss/online/total/E1: 3.2243151664733887
10-31 01:08:17 I accuracy1/online/main/E1: 0.481614
10-31 01:17:13 I ------------------
10-31 01:17:13 I Epoch 152/400 (E152_U380304_S194715648)
10-31 01:17:13 I ETA: 10.31 18.47.10 estimated_duration: 1-04:09:14.64 time_since_last_log: 00:08:55.69 time_per_update: 00:00:00.21 
10-31 01:17:13 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 01:17:13 I loss/online/main/E1: 3.2259397506713867
10-31 01:17:13 I loss/online/total/E1: 3.2259397506713867
10-31 01:17:13 I accuracy1/online/main/E1: 0.482070
10-31 01:26:10 I ------------------
10-31 01:26:10 I Epoch 153/400 (E153_U382806_S195996672)
10-31 01:26:10 I ETA: 10.31 18.59.33 estimated_duration: 1-04:21:37.19 time_since_last_log: 00:08:56.89 time_per_update: 00:00:00.21 
10-31 01:26:10 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 01:26:10 I loss/online/main/E1: 3.222475290298462
10-31 01:26:10 I loss/online/total/E1: 3.222475290298462
10-31 01:26:10 I accuracy1/online/main/E1: 0.481670
10-31 01:35:06 I ------------------
10-31 01:35:06 I Epoch 154/400 (E154_U385308_S197277696)
10-31 01:35:06 I ETA: 10.31 19.11.43 estimated_duration: 1-04:33:47.80 time_since_last_log: 00:08:56.04 time_per_update: 00:00:00.21 
10-31 01:35:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 01:35:06 I loss/online/main/E1: 3.2127912044525146
10-31 01:35:06 I loss/online/total/E1: 3.2127912044525146
10-31 01:35:06 I accuracy1/online/main/E1: 0.484746
10-31 01:44:04 I ------------------
10-31 01:44:04 I Epoch 155/400 (E155_U387810_S198558720)
10-31 01:44:04 I ETA: 10.31 19.23.50 estimated_duration: 1-04:45:54.10 time_since_last_log: 00:08:58.04 time_per_update: 00:00:00.21 
10-31 01:44:04 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 01:44:04 I loss/online/main/E1: 3.2064290046691895
10-31 01:44:04 I loss/online/total/E1: 3.2064290046691895
10-31 01:44:04 I accuracy1/online/main/E1: 0.485697
10-31 01:44:21 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 01:44:21 I accuracy1/val/main: 0.718040
10-31 01:44:21 I loss/val/main: 1.15625
10-31 01:53:17 I ------------------
10-31 01:53:17 I Epoch 156/400 (E156_U390312_S199839744)
10-31 01:53:17 I ETA: 10.31 19.36.26 estimated_duration: 1-04:58:30.21 time_since_last_log: 00:09:13.25 time_per_update: 00:00:00.22 
10-31 01:53:17 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 01:53:17 I loss/online/main/E1: 3.2008564472198486
10-31 01:53:17 I loss/online/total/E1: 3.2008564472198486
10-31 01:53:17 I accuracy1/online/main/E1: 0.487276
10-31 02:02:15 I ------------------
10-31 02:02:15 I Epoch 157/400 (E157_U392814_S201120768)
10-31 02:02:15 I ETA: 10.31 19.48.12 estimated_duration: 1-05:10:16.58 time_since_last_log: 00:08:57.60 time_per_update: 00:00:00.21 
10-31 02:02:15 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 02:02:15 I loss/online/main/E1: 3.208348274230957
10-31 02:02:15 I loss/online/total/E1: 3.208348274230957
10-31 02:02:15 I accuracy1/online/main/E1: 0.484457
10-31 02:11:11 I ------------------
10-31 02:11:11 I Epoch 158/400 (E158_U395316_S202401792)
10-31 02:11:11 I ETA: 10.31 19.59.45 estimated_duration: 1-05:21:49.73 time_since_last_log: 00:08:55.94 time_per_update: 00:00:00.21 
10-31 02:11:11 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 02:11:11 I loss/online/main/E1: 3.19812273979187
10-31 02:11:11 I loss/online/total/E1: 3.19812273979187
10-31 02:11:11 I accuracy1/online/main/E1: 0.486787
10-31 02:20:07 I ------------------
10-31 02:20:07 I Epoch 159/400 (E159_U397818_S203682816)
10-31 02:20:07 I ETA: 10.31 20.11.11 estimated_duration: 1-05:33:14.95 time_since_last_log: 00:08:56.27 time_per_update: 00:00:00.21 
10-31 02:20:07 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 02:20:07 I loss/online/main/E1: 3.2019500732421875
10-31 02:20:07 I loss/online/total/E1: 3.2019500732421875
10-31 02:20:07 I accuracy1/online/main/E1: 0.486024
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd-gpu-06: error: *** JOB 150080 ON gpu-06 CANCELLED AT 2025-10-31T02:27:02 DUE TO TIME LIMIT ***
slurmstepd-gpu-06: error: *** STEP 150080.0 ON gpu-06 CANCELLED AT 2025-10-31T02:27:02 DUE TO TIME LIMIT ***
