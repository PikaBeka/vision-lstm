MASTER_ADDR: gpu-53
CUDA_VISIBLE_DEVICES=0,1,2,3
Tue Nov  4 16:37:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   38C    P0             79W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   27C    P0             51W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   26C    P0             51W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   26C    P0             50W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
torch: 2.5.1+cu121 cuda: 12.1 cuda available: True
11-04 16:37:53 I initializing rank=3 local_rank=3 nodes=1 hostname=gpu-53 master_addr=gpu-53 master_port=55555 (waiting for all 4 processes to connect)
11-04 16:37:53 I initializing rank=0 local_rank=0 nodes=1 hostname=gpu-53 master_addr=gpu-53 master_port=55555 (waiting for all 4 processes to connect)
11-04 16:37:53 I initializing rank=2 local_rank=2 nodes=1 hostname=gpu-53 master_addr=gpu-53 master_port=55555 (waiting for all 4 processes to connect)
11-04 16:37:53 I initializing rank=1 local_rank=1 nodes=1 hostname=gpu-53 master_addr=gpu-53 master_port=55555 (waiting for all 4 processes to connect)
[rank1]:[W1104 16:37:53.890773035 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W1104 16:37:53.255425738 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W1104 16:37:54.617769230 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W1104 16:37:54.617833409 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
11-04 16:37:54 I initialized process rank=1 local_rank=1 pid=1666037
11-04 16:37:54 I initialized process rank=0 local_rank=0 pid=1666036
11-04 16:37:54 I initialized process rank=3 local_rank=3 pid=1666039
11-04 16:37:54 I initialized process rank=2 local_rank=2 pid=1666038
11-04 16:37:54 I initialized 4 processes
11-04 16:37:54 W disabled cudnn benchmark
11-04 16:37:54 W enabled cudnn deterministic
11-04 16:37:54 I log file: /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/log.txt
11-04 16:37:54 I no seed specified -> using seed=0
11-04 16:37:54 I ------------------
11-04 16:37:54 I initializing wandb (mode=online)
11-04 16:37:54 I logging into wandb (host=https://api.wandb.ai/ rank=0)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
wandb: Currently logged in as: beka-kalmahanbet (ml710_project) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
11-04 16:37:56 I logged into wandb (host=https://api.wandb.ai/)
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/wandb/run-20251104_163756-ddgjaz5t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run in1k-lstm-80m16-e400res192-bialter-bilatflat-lr1e3-conv2d3-bias/in1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/beka-kalmahanbet-mbzuai/minLSTM
wandb: üöÄ View run at https://wandb.ai/beka-kalmahanbet-mbzuai/minLSTM/runs/ddgjaz5t
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
11-04 16:37:56 I ------------------
11-04 16:37:56 I stage_id: ddgjaz5t
11-04 16:37:56 I python main_train.py --hp src/vislstm/yamls/pretrain/vil/lstm_80M16_e400_bialter_bilatflat_conv2d3_lr1e3_res192_bias.yaml --resume_stage_id 3cnt46tb --resume_checkpoint latest --num_workers 5
11-04 16:37:56 I ------------------
11-04 16:37:56 I VERSION CHECK
11-04 16:37:56 I executable: /home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/bin/python
11-04 16:37:56 I python version: 3.9.21
11-04 16:37:56 I torch version: 2.5.1+cu121
11-04 16:37:56 I torch.cuda version: 12.1
11-04 16:37:56 I torchvision.version: 0.20.1+cu121
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
11-04 16:37:57 I initialized process rank=1 local_rank=1 pid=1666037 hostname=gpu-53
11-04 16:37:57 I torchmetrics version: 1.6.2
11-04 16:37:57 I kappaschedules version: 0.0.31
11-04 16:37:57 I kappamodules version: 0.1.76
11-04 16:37:57 I ------------------
11-04 16:37:57 I SYSTEM INFO
11-04 16:37:57 I host name: gpu-53
11-04 16:37:57 I OS: Linux-5.15.161-ql-generic-13.0-14-x86_64-with-glibc2.35
11-04 16:37:57 I OS version: #1 SMP Wed Jun 26 16:19:39 UTC 2024
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
11-04 16:37:58 I initialized process rank=3 local_rank=3 pid=1666039 hostname=gpu-53
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
11-04 16:37:58 I initialized process rank=2 local_rank=2 pid=1666038 hostname=gpu-53
11-04 16:37:59 I CUDA version: 12.4
11-04 16:37:59 I current commit hash: 879894a2c4205819466aaff45b583fe3b517c036
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
11-04 16:37:59 I latest git tag: 
11-04 16:37:59 I initialized process rank=0 local_rank=0 pid=1666036 hostname=gpu-53
11-04 16:37:59 I total_cpu_count: 64
11-04 16:37:59 I ------------------
11-04 16:37:59 I STATIC CONFIG
11-04 16:37:59 I account_name: beknur.kalmakhanbet
11-04 16:37:59 I output_path: /home/beknur.kalmakhanbet/save
11-04 16:37:59 I ------------------
11-04 16:37:59 I CLI ARGS
11-04 16:37:59 I hp: src/vislstm/yamls/pretrain/vil/lstm_80M16_e400_bialter_bilatflat_conv2d3_lr1e3_res192_bias.yaml
11-04 16:37:59 I accelerator: gpu
11-04 16:37:59 I num_workers: 5
11-04 16:37:59 I testrun: False
11-04 16:37:59 I minmodelrun: False
11-04 16:37:59 I mindatarun: False
11-04 16:37:59 I mindurationrun: False
11-04 16:37:59 I static_config_uri: static_config.yaml
11-04 16:37:59 I resume_stage_id: 3cnt46tb
11-04 16:37:59 I resume_checkpoint: latest
11-04 16:37:59 I ------------------
11-04 16:37:59 I DIST CONFIG
11-04 16:37:59 I rank: 0
11-04 16:37:59 I local_rank: 0
11-04 16:37:59 I world_size: 4
11-04 16:37:59 I nodes: 1
11-04 16:37:59 I backend: nccl
11-04 16:37:59 I slurm job id: 152939
11-04 16:37:59 I hostnames: gpu-53
11-04 16:37:59 I ------------------
master_factory_base_path: vislstm
stage_name: in1k
datasets:
  train:
    kind: imagenet1k
    split: train
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: random_resized_crop
        size: 192
        scale:
        - 0.08
        - 1.0
        interpolation: bicubic
      - kind: random_horizontal_flip
      - kind: transforms.three_augment
        blur_sigma:
        - 0.1
        - 2.0
      - kind: color_jitter
        brightness: 0.3
        contrast: 0.3
        saturation: 0.3
        hue: 0.0
      - kind: imagenet1k_norm
    - kind: one_hot_wrapper
    collators:
    - kind: mix_collator
      mixup_alpha: 0.8
      cutmix_alpha: 1.0
      mixup_p: 0.5
      cutmix_p: 0.5
      apply_mode: batch
      lamb_mode: batch
      shuffle_mode: flip
  val:
    kind: imagenet1k
    split: val
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: resize
        size: 192
        interpolation: bicubic
      - kind: center_crop
        size: 192
      - kind: imagenet1k_norm
model:
  kind: models.single.vislstm
  patch_size: 16
  dim: 768
  depth: 24
  bidirectional: false
  alternation: bidirectional
  conv1d_kernel_size: 3
  use_conv2d: true
  bias: true
  pos_embed_mode: learnable
  drop_path_rate: 0.2
  drop_path_decay: false
  mode: classifier
  pooling:
    kind: bilateral
    aggregate: flatten
  optim:
    kind: adamw
    lr: 0.001
    betas:
    - 0.9
    - 0.999
    weight_decay: 0.05
    clip_grad_norm: 1.0
    schedule:
      kind: linear_warmup_cosine_decay_schedule
      warmup_epochs: 5
      end_value: 1.0e-06
    lr_scaler:
      kind: linear_lr_scaler
      divisor: 1024
trainer:
  kind: classification_trainer
  precision: bfloat16
  backup_precision: float16
  max_epochs: 400
  effective_batch_size: 512
  log_every_n_epochs: 1
  use_torch_compile: true
  callbacks:
  - kind: checkpoint_callback
  - kind: checkpoint_callback
    every_n_epochs: 10
    save_weights: false
    save_latest_weights: true
    save_latest_optim: true
  - kind: offline_accuracy_callback
    every_n_epochs: 5
    dataset_key: val
  initializer:
    kind: resume_initializer
    stage_id: 3cnt46tb
    checkpoint: latest
11-04 16:37:59 I copied unresolved hp to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/hp_unresolved.yaml
11-04 16:37:59 I dumped resolved hp to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/hp_resolved.yaml
11-04 16:37:59 I ------------------
11-04 16:37:59 I training stage 'in1k'
11-04 16:37:59 I using different seeds per process (seed+rank)
11-04 16:37:59 I set seed to 0
11-04 16:37:59 I ------------------
11-04 16:37:59 I initializing datasets
11-04 16:37:59 I initializing train
11-04 16:38:03 I instantiating sample_wrapper x_transform_wrapper
11-04 16:38:03 I instantiating sample_wrapper one_hot_wrapper
11-04 16:38:03 I initializing val
11-04 16:38:04 I instantiating sample_wrapper x_transform_wrapper
11-04 16:38:04 I ------------------
11-04 16:38:04 I initializing trainer
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
11-04 16:38:05 I using precision: torch.bfloat16 (desired=bfloat16 backup=float16)
11-04 16:38:05 I main_sampler: DistributedSampler(num_repeats=1, shuffle=True)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
11-04 16:38:05 I loaded checkpoint from trainer_state_dict: {'epoch': 290, 'update': 725580, 'sample': 371496960, 'callback_state_dicts': [None, None, None]}
11-04 16:38:05 I ------------------
11-04 16:38:05 I creating model
11-04 16:38:05 I input_shape: (3, 192, 192)
11-04 16:38:05 I pos_embed.is_learnable=True
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
11-04 16:38:06 I drop_path_rate: 0.2
11-04 16:38:06 I model:
VisLSTM(
  (pooling): Bilateral(aggregate=flatten)
  (patch_embed): VitPatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_embed): VitPosEmbed2d()
  (xlstm): xLSTMBlockStack(
    (blocks): ModuleList(
      (0-23): 24 x mLSTMBlock(
        (drop_path1): DropPath(drop_prob=0.200)
        (xlstm_norm): LayerNorm()
        (xlstm): mLSTMLayer(
          (proj_up): Linear(in_features=768, out_features=3072, bias=True)
          (conv1d): SequenceConv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
          (conv_act_fn): SiLU()
          (mlstm_cell): mLSTMCell(
            (linear_h): FeedForward(
              (linear1): Linear(in_features=1536, out_features=8, bias=True)
              (activation): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=8, out_features=1536, bias=True)
            )
            (linear_i): FeedForward(
              (linear1): Linear(in_features=1536, out_features=8, bias=True)
              (activation): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=8, out_features=1536, bias=True)
            )
            (linear_f): FeedForward(
              (linear1): Linear(in_features=1536, out_features=8, bias=True)
              (activation): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=8, out_features=1536, bias=True)
            )
          )
          (ogate_act_fn): SiLU()
          (proj_down): Linear(in_features=1536, out_features=768, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layerscale): Identity()
        )
      )
    )
    (post_blocks_norm): LayerNorm()
  )
  (head): Sequential(
    (0): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
    (1): Linear(in_features=1536, out_features=1000, bias=True)
  )
)
11-04 16:38:06 I vislstm initialize optimizer
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
11-04 16:38:06 I base lr: 1e-3
11-04 16:38:06 I scaled lr: 5e-4
11-04 16:38:06 I lr_scaler=LinearLrScaler(divisor=1024)
11-04 16:38:06 I lr_scale_factor=512
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
11-04 16:38:06 I exclude_bias_from_wd=True exclude_norm_from_wd=True param_group_modifiers=[WeightDecayByNameModifier(name=pos_embed.embed)]
11-04 16:38:06 I using 2 param groups:
11-04 16:38:06 I len(params)=218
11-04 16:38:06 I weight_decay=0.0 len(params)=295
11-04 16:38:06 I ------------------
11-04 16:38:06 I loading trainer/model state for resuming
11-04 16:38:06 I loading state from checkpoint 3cnt46tb/in1k/latest
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
11-04 16:38:06 I loaded trainer checkpoint /home/beknur.kalmakhanbet/save/in1k/3cnt46tb/checkpoints/trainer cp=latest.th
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
11-04 16:38:08 I loaded weights of vislstm from /home/beknur.kalmakhanbet/save/in1k/3cnt46tb/checkpoints/vislstm cp=latest model.th
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
11-04 16:38:10 I loaded optimizer of vislstm from /home/beknur.kalmakhanbet/save/in1k/3cnt46tb/checkpoints/vislstm cp=latest optim.th
11-04 16:38:10 I added default DatasetStatsCallback
11-04 16:38:10 I added default ParamCountCallback
11-04 16:38:10 I added default CopyPreviousConfigCallback
11-04 16:38:10 I added default CopyPreviousSummaryCallback
11-04 16:38:10 I added default ProgressCallback(every_n_epochs=1)
11-04 16:38:10 I added default TrainTimeCallback(every_n_epochs=1)
11-04 16:38:10 I added default OnlineLossCallback(every_n_epochs=1)
11-04 16:38:10 I added default LrCallback(every_n_updates=50)
11-04 16:38:10 I added default FreezerCallback(every_n_updates=50)
11-04 16:38:10 I added default OnlineLossCallback(every_n_updates=50)
11-04 16:38:10 I replacing BatchNorm layers with SyncBatchNorm
11-04 16:38:10 I wrapping model with torch.compile
11-04 16:38:12 I ------------------
11-04 16:38:12 I PREPARE TRAINER
11-04 16:38:12 I calculating batch_size and accumulation_steps (effective_batch_size=512)
11-04 16:38:12 I torch.compile is used -> automatic batchsize not supported
11-04 16:38:12 I train_batches per epoch: 2502 (world_size=4 batch_size=128)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
11-04 16:38:12 I initializing dataloader
11-04 16:38:12 I OfflineAccuracyCallback(every_n_epochs=5) registered InterleavedSamplerConfig(every_n_epochs=5) dataset_mode='x class'
11-04 16:38:12 I created dataloader (batch_size=128 num_workers=5 pin_memory=True total_cpu_count=64 prefetch_factor=2)
11-04 16:38:12 I concatenated dataset properties:
11-04 16:38:12 I - mode='index x class' len=1281167 root_dataset=<vislstm.datasets.imagenet1k.Imagenet1k object at 0x14866746fe80>
11-04 16:38:12 I - mode='x class' len=50000 root_dataset=<vislstm.datasets.imagenet1k.Imagenet1k object at 0x14866746ffa0>
11-04 16:38:12 I ------------------
11-04 16:38:12 I BEFORE TRAINING
11-04 16:38:12 I train: 1281167 samples
11-04 16:38:12 I val: 50000 samples
11-04 16:38:12 I parameter counts (trainable | frozen)
11-04 16:38:12 I 89,592,616 | 0 | vislstm
11-04 16:38:12 I estimated checkpoint size: 1.0GB
11-04 16:38:12 I estimated weight checkpoint size: 358.3MB
11-04 16:38:12 I estimated optim checkpoint size: 716.7MB
11-04 16:38:12 I estimated size for 1 checkpoints: 358.3MB
11-04 16:38:12 I estimated checkpoint size: 1.0GB
11-04 16:38:12 I estimated weight checkpoint size: 358.3MB
11-04 16:38:12 I estimated optim checkpoint size: 716.7MB
11-04 16:38:12 I estimated size for 41 checkpoints: 0.0B
11-04 16:38:12 I ------------------
11-04 16:38:12 I DatasetStatsCallback
11-04 16:38:12 I ParamCountCallback
11-04 16:38:12 I CopyPreviousConfigCallback
11-04 16:38:12 I CopyPreviousSummaryCallback
11-04 16:38:12 I ProgressCallback(every_n_epochs=1)
11-04 16:38:12 I TrainTimeCallback(every_n_epochs=1)
11-04 16:38:12 I OnlineLossCallback(every_n_epochs=1)
11-04 16:38:12 I LrCallback(every_n_updates=50)
11-04 16:38:12 I FreezerCallback(every_n_updates=50)
11-04 16:38:12 I OnlineLossCallback(every_n_updates=50)
11-04 16:38:12 I OnlineAccuracyCallback(every_n_updates=50)
11-04 16:38:12 I OnlineAccuracyCallback(every_n_epochs=1)
11-04 16:38:12 I CheckpointCallback()
11-04 16:38:12 I CheckpointCallback(every_n_epochs=10)
11-04 16:38:12 I OfflineAccuracyCallback(every_n_epochs=5)
11-04 16:38:12 I ------------------
11-04 16:38:12 I START TRAINING
11-04 16:38:12 I initializing dataloader workers
11-04 16:38:12 I initialized dataloader workers
[rank1]:W1104 16:38:13.494248 1666037 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:W1104 16:38:13.498094 1666038 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W1104 16:38:13.529224 1666036 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank3]:W1104 16:38:13.581792 1666039 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
11-04 16:39:34 I 0 unused parameters
11-04 16:48:29 I ------------------
11-04 16:48:29 I Epoch 291/400 (E291_U728082_S372777984)
11-04 16:48:29 I ETA: 11.04 16.52.20 estimated_duration: 00:14:08.60 time_since_last_log: 00:10:17.36 time_per_update: 00:00:00.00 
11-04 16:48:29 I data=[0.00, 0.00, 0.00, 0.00] update=[0.24, 0.24, 0.24, 0.24]
11-04 16:48:29 I loss/online/main/E1: 2.544552803039551
11-04 16:48:29 I loss/online/total/E1: 2.544552803039551
11-04 16:48:29 I accuracy1/online/main/E1: 0.594576
11-04 16:57:25 I ------------------
11-04 16:57:25 I Epoch 292/400 (E292_U730584_S374059008)
11-04 16:57:25 I ETA: 11.04 17.00.44 estimated_duration: 00:12:14.49 time_since_last_log: 00:08:55.68 time_per_update: 00:00:00.21 
11-04 16:57:25 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 16:57:25 I loss/online/main/E1: 2.5351853370666504
11-04 16:57:25 I loss/online/total/E1: 2.5351853370666504
11-04 16:57:25 I accuracy1/online/main/E1: 0.596515
11-04 17:06:20 I ------------------
11-04 17:06:20 I Epoch 293/400 (E293_U733086_S375340032)
11-04 17:06:20 I ETA: 11.04 17.12.53 estimated_duration: 00:24:23.74 time_since_last_log: 00:08:55.52 time_per_update: 00:00:00.21 
11-04 17:06:20 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 17:06:20 I loss/online/main/E1: 2.5356831550598145
11-04 17:06:20 I loss/online/total/E1: 2.5356831550598145
11-04 17:06:20 I accuracy1/online/main/E1: 0.597837
11-04 17:15:17 I ------------------
11-04 17:15:17 I Epoch 294/400 (E294_U735588_S376621056)
11-04 17:15:17 I ETA: 11.04 17.24.59 estimated_duration: 00:36:30.24 time_since_last_log: 00:08:57.16 time_per_update: 00:00:00.21 
11-04 17:15:17 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 17:15:17 I loss/online/main/E1: 2.527294158935547
11-04 17:15:17 I loss/online/total/E1: 2.527294158935547
11-04 17:15:17 I accuracy1/online/main/E1: 0.597296
11-04 17:24:14 I ------------------
11-04 17:24:14 I Epoch 295/400 (E295_U738090_S377902080)
11-04 17:24:14 I ETA: 11.04 17.37.00 estimated_duration: 00:48:30.61 time_since_last_log: 00:08:56.29 time_per_update: 00:00:00.21 
11-04 17:24:14 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 17:24:14 I loss/online/main/E1: 2.533432722091675
11-04 17:24:14 I loss/online/total/E1: 2.533432722091675
11-04 17:24:14 I accuracy1/online/main/E1: 0.598094
11-04 17:24:31 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.18
11-04 17:24:31 I accuracy1/val/main: 0.782440
11-04 17:24:31 I loss/val/main: 0.890625
11-04 17:33:27 I ------------------
11-04 17:33:27 I Epoch 296/400 (E296_U740592_S379183104)
11-04 17:33:27 I ETA: 11.04 17.49.18 estimated_duration: 01:00:49.36 time_since_last_log: 00:09:13.48 time_per_update: 00:00:00.22 
11-04 17:33:27 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 17:33:27 I loss/online/main/E1: 2.5198209285736084
11-04 17:33:27 I loss/online/total/E1: 2.5198209285736084
11-04 17:33:27 I accuracy1/online/main/E1: 0.599969
11-04 17:42:23 I ------------------
11-04 17:42:23 I Epoch 297/400 (E297_U743094_S380464128)
11-04 17:42:23 I ETA: 11.04 18.01.08 estimated_duration: 01:12:39.23 time_since_last_log: 00:08:55.77 time_per_update: 00:00:00.21 
11-04 17:42:23 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 17:42:23 I loss/online/main/E1: 2.516401767730713
11-04 17:42:23 I loss/online/total/E1: 2.516401767730713
11-04 17:42:23 I accuracy1/online/main/E1: 0.599494
11-04 17:51:21 I ------------------
11-04 17:51:21 I Epoch 298/400 (E298_U745596_S381745152)
11-04 17:51:21 I ETA: 11.04 18.12.56 estimated_duration: 01:24:26.83 time_since_last_log: 00:08:57.63 time_per_update: 00:00:00.21 
11-04 17:51:21 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 17:51:21 I loss/online/main/E1: 2.506343364715576
11-04 17:51:21 I loss/online/total/E1: 2.506343364715576
11-04 17:51:21 I accuracy1/online/main/E1: 0.602871
11-04 18:00:16 I ------------------
11-04 18:00:16 I Epoch 299/400 (E299_U748098_S383026176)
11-04 18:00:16 I ETA: 11.04 18.24.36 estimated_duration: 01:36:07.02 time_since_last_log: 00:08:55.64 time_per_update: 00:00:00.21 
11-04 18:00:16 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 18:00:16 I loss/online/main/E1: 2.5006940364837646
11-04 18:00:16 I loss/online/total/E1: 2.5006940364837646
11-04 18:00:16 I accuracy1/online/main/E1: 0.603378
11-04 18:09:11 I ------------------
11-04 18:09:11 I Epoch 300/400 (E300_U750600_S384307200)
11-04 18:09:11 I ETA: 11.04 18.36.11 estimated_duration: 01:47:41.70 time_since_last_log: 00:08:55.02 time_per_update: 00:00:00.21 
11-04 18:09:11 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 18:09:11 I loss/online/main/E1: 2.489021062850952
11-04 18:09:11 I loss/online/total/E1: 2.489021062850952
11-04 18:09:11 I accuracy1/online/main/E1: 0.604079
11-04 18:09:12 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest model.th
11-04 18:09:14 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest optim.th
11-04 18:09:14 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/trainer cp=latest.th
11-04 18:09:31 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-04 18:09:31 I accuracy1/val/main: 0.785020
11-04 18:09:31 I loss/val/main: 0.875
11-04 18:18:27 I ------------------
11-04 18:18:27 I Epoch 301/400 (E301_U753102_S385588224)
11-04 18:18:27 I ETA: 11.04 18.48.09 estimated_duration: 01:59:39.70 time_since_last_log: 00:09:16.03 time_per_update: 00:00:00.22 
11-04 18:18:27 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 18:18:27 I loss/online/main/E1: 2.494337558746338
11-04 18:18:27 I loss/online/total/E1: 2.494337558746338
11-04 18:18:27 I accuracy1/online/main/E1: 0.602971
11-04 18:27:26 I ------------------
11-04 18:27:26 I Epoch 302/400 (E302_U755604_S386869248)
11-04 18:27:26 I ETA: 11.04 18.59.39 estimated_duration: 02:11:09.63 time_since_last_log: 00:08:58.47 time_per_update: 00:00:00.21 
11-04 18:27:26 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 18:27:26 I loss/online/main/E1: 2.4954123497009277
11-04 18:27:26 I loss/online/total/E1: 2.4954123497009277
11-04 18:27:26 I accuracy1/online/main/E1: 0.604114
11-04 18:36:21 I ------------------
11-04 18:36:21 I Epoch 303/400 (E303_U758106_S388150272)
11-04 18:36:21 I ETA: 11.04 19.11.00 estimated_duration: 02:22:31.18 time_since_last_log: 00:08:55.58 time_per_update: 00:00:00.21 
11-04 18:36:21 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 18:36:21 I loss/online/main/E1: 2.476652145385742
11-04 18:36:21 I loss/online/total/E1: 2.476652145385742
11-04 18:36:21 I accuracy1/online/main/E1: 0.606352
11-04 18:45:17 I ------------------
11-04 18:45:17 I Epoch 304/400 (E304_U760608_S389431296)
11-04 18:45:17 I ETA: 11.04 19.22.18 estimated_duration: 02:33:48.52 time_since_last_log: 00:08:55.80 time_per_update: 00:00:00.21 
11-04 18:45:17 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 18:45:17 I loss/online/main/E1: 2.4700450897216797
11-04 18:45:17 I loss/online/total/E1: 2.4700450897216797
11-04 18:45:17 I accuracy1/online/main/E1: 0.607660
11-04 18:54:13 I ------------------
11-04 18:54:13 I Epoch 305/400 (E305_U763110_S390712320)
11-04 18:54:13 I ETA: 11.04 19.33.30 estimated_duration: 02:45:01.31 time_since_last_log: 00:08:55.73 time_per_update: 00:00:00.21 
11-04 18:54:13 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 18:54:13 I loss/online/main/E1: 2.4717397689819336
11-04 18:54:13 I loss/online/total/E1: 2.4717397689819336
11-04 18:54:13 I accuracy1/online/main/E1: 0.606952
11-04 18:54:30 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-04 18:54:30 I accuracy1/val/main: 0.786520
11-04 18:54:30 I loss/val/main: 0.87109375
11-04 19:03:27 I ------------------
11-04 19:03:27 I Epoch 306/400 (E306_U765612_S391993344)
11-04 19:03:27 I ETA: 11.04 19.45.02 estimated_duration: 02:56:33.38 time_since_last_log: 00:09:13.83 time_per_update: 00:00:00.22 
11-04 19:03:27 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 19:03:27 I loss/online/main/E1: 2.4593582153320312
11-04 19:03:27 I loss/online/total/E1: 2.4593582153320312
11-04 19:03:27 I accuracy1/online/main/E1: 0.609334
11-04 19:12:23 I ------------------
11-04 19:12:23 I Epoch 307/400 (E307_U768114_S393274368)
11-04 19:12:23 I ETA: 11.04 19.56.08 estimated_duration: 03:07:38.58 time_since_last_log: 00:08:56.70 time_per_update: 00:00:00.21 
11-04 19:12:23 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 19:12:23 I loss/online/main/E1: 2.4504833221435547
11-04 19:12:23 I loss/online/total/E1: 2.4504833221435547
11-04 19:12:23 I accuracy1/online/main/E1: 0.612291
11-04 19:21:19 I ------------------
11-04 19:21:19 I Epoch 308/400 (E308_U770616_S394555392)
11-04 19:21:19 I ETA: 11.04 20.07.08 estimated_duration: 03:18:38.52 time_since_last_log: 00:08:55.99 time_per_update: 00:00:00.21 
11-04 19:21:19 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 19:21:19 I loss/online/main/E1: 2.4405300617218018
11-04 19:21:19 I loss/online/total/E1: 2.4405300617218018
11-04 19:21:19 I accuracy1/online/main/E1: 0.613452
11-04 19:30:17 I ------------------
11-04 19:30:17 I Epoch 309/400 (E309_U773118_S395836416)
11-04 19:30:17 I ETA: 11.04 20.18.05 estimated_duration: 03:29:35.60 time_since_last_log: 00:08:57.09 time_per_update: 00:00:00.21 
11-04 19:30:17 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 19:30:17 I loss/online/main/E1: 2.4281773567199707
11-04 19:30:17 I loss/online/total/E1: 2.4281773567199707
11-04 19:30:17 I accuracy1/online/main/E1: 0.612804
11-04 19:39:16 I ------------------
11-04 19:39:16 I Epoch 310/400 (E310_U775620_S397117440)
11-04 19:39:16 I ETA: 11.04 20.29.01 estimated_duration: 03:40:32.02 time_since_last_log: 00:08:59.87 time_per_update: 00:00:00.21 
11-04 19:39:16 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 19:39:16 I loss/online/main/E1: 2.4305195808410645
11-04 19:39:16 I loss/online/total/E1: 2.4305195808410645
11-04 19:39:16 I accuracy1/online/main/E1: 0.613110
11-04 19:39:17 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest model.th
11-04 19:39:19 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest optim.th
11-04 19:39:19 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/trainer cp=latest.th
11-04 19:39:36 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-04 19:39:36 I accuracy1/val/main: 0.790600
11-04 19:39:36 I loss/val/main: 0.8515625
11-04 19:48:32 I ------------------
11-04 19:48:32 I Epoch 311/400 (E311_U778122_S398398464)
11-04 19:48:32 I ETA: 11.04 20.40.14 estimated_duration: 03:51:44.89 time_since_last_log: 00:09:15.94 time_per_update: 00:00:00.22 
11-04 19:48:32 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 19:48:32 I loss/online/main/E1: 2.425290107727051
11-04 19:48:32 I loss/online/total/E1: 2.425290107727051
11-04 19:48:32 I accuracy1/online/main/E1: 0.614658
11-04 19:57:28 I ------------------
11-04 19:57:28 I Epoch 312/400 (E312_U780624_S399679488)
11-04 19:57:28 I ETA: 11.04 20.50.57 estimated_duration: 04:02:27.76 time_since_last_log: 00:08:55.93 time_per_update: 00:00:00.21 
11-04 19:57:28 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 19:57:28 I loss/online/main/E1: 2.419071674346924
11-04 19:57:28 I loss/online/total/E1: 2.419071674346924
11-04 19:57:28 I accuracy1/online/main/E1: 0.617324
11-04 20:06:25 I ------------------
11-04 20:06:25 I Epoch 313/400 (E313_U783126_S400960512)
11-04 20:06:25 I ETA: 11.04 21.01.37 estimated_duration: 04:13:08.10 time_since_last_log: 00:08:57.18 time_per_update: 00:00:00.21 
11-04 20:06:25 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 20:06:25 I loss/online/main/E1: 2.428363800048828
11-04 20:06:25 I loss/online/total/E1: 2.428363800048828
11-04 20:06:25 I accuracy1/online/main/E1: 0.614072
11-04 20:15:24 I ------------------
11-04 20:15:24 I Epoch 314/400 (E314_U785628_S402241536)
11-04 20:15:24 I ETA: 11.04 21.12.15 estimated_duration: 04:23:46.03 time_since_last_log: 00:08:58.49 time_per_update: 00:00:00.21 
11-04 20:15:24 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 20:15:24 I loss/online/main/E1: 2.4100351333618164
11-04 20:15:24 I loss/online/total/E1: 2.4100351333618164
11-04 20:15:24 I accuracy1/online/main/E1: 0.617699
11-04 20:24:20 I ------------------
11-04 20:24:20 I Epoch 315/400 (E315_U788130_S403522560)
11-04 20:24:20 I ETA: 11.04 21.22.46 estimated_duration: 04:34:16.59 time_since_last_log: 00:08:55.89 time_per_update: 00:00:00.21 
11-04 20:24:20 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 20:24:20 I loss/online/main/E1: 2.4175992012023926
11-04 20:24:20 I loss/online/total/E1: 2.4175992012023926
11-04 20:24:20 I accuracy1/online/main/E1: 0.615715
11-04 20:24:37 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-04 20:24:37 I accuracy1/val/main: 0.790180
11-04 20:24:37 I loss/val/main: 0.8515625
11-04 20:33:33 I ------------------
11-04 20:33:33 I Epoch 316/400 (E316_U790632_S404803584)
11-04 20:33:33 I ETA: 11.04 21.33.34 estimated_duration: 04:45:05.12 time_since_last_log: 00:09:13.24 time_per_update: 00:00:00.22 
11-04 20:33:33 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 20:33:33 I loss/online/main/E1: 2.402508020401001
11-04 20:33:33 I loss/online/total/E1: 2.402508020401001
11-04 20:33:33 I accuracy1/online/main/E1: 0.618678
11-04 20:42:31 I ------------------
11-04 20:42:31 I Epoch 317/400 (E317_U793134_S406084608)
11-04 20:42:31 I ETA: 11.04 21.43.59 estimated_duration: 04:55:30.09 time_since_last_log: 00:08:57.83 time_per_update: 00:00:00.21 
11-04 20:42:31 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 20:42:31 I loss/online/main/E1: 2.3976924419403076
11-04 20:42:31 I loss/online/total/E1: 2.3976924419403076
11-04 20:42:31 I accuracy1/online/main/E1: 0.620413
11-04 20:51:29 I ------------------
11-04 20:51:29 I Epoch 318/400 (E318_U795636_S407365632)
11-04 20:51:29 I ETA: 11.04 21.54.21 estimated_duration: 05:05:51.47 time_since_last_log: 00:08:58.10 time_per_update: 00:00:00.21 
11-04 20:51:29 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 20:51:29 I loss/online/main/E1: 2.399963617324829
11-04 20:51:29 I loss/online/total/E1: 2.399963617324829
11-04 20:51:29 I accuracy1/online/main/E1: 0.620193
11-04 21:00:26 I ------------------
11-04 21:00:26 I Epoch 319/400 (E319_U798138_S408646656)
11-04 21:00:26 I ETA: 11.04 22.04.36 estimated_duration: 05:16:07.03 time_since_last_log: 00:08:56.59 time_per_update: 00:00:00.21 
11-04 21:00:26 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 21:00:26 I loss/online/main/E1: 2.381647825241089
11-04 21:00:26 I loss/online/total/E1: 2.381647825241089
11-04 21:00:26 I accuracy1/online/main/E1: 0.623734
11-04 21:09:23 I ------------------
11-04 21:09:23 I Epoch 320/400 (E320_U800640_S409927680)
11-04 21:09:23 I ETA: 11.04 22.14.49 estimated_duration: 05:26:19.98 time_since_last_log: 00:08:57.58 time_per_update: 00:00:00.21 
11-04 21:09:23 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 21:09:23 I loss/online/main/E1: 2.385251522064209
11-04 21:09:23 I loss/online/total/E1: 2.385251522064209
11-04 21:09:23 I accuracy1/online/main/E1: 0.620771
11-04 21:09:24 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest model.th
11-04 21:09:26 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest optim.th
11-04 21:09:26 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/trainer cp=latest.th
11-04 21:09:43 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-04 21:09:43 I accuracy1/val/main: 0.792200
11-04 21:09:43 I loss/val/main: 0.84375
11-04 21:18:40 I ------------------
11-04 21:18:40 I Epoch 321/400 (E321_U803142_S411208704)
11-04 21:18:40 I ETA: 11.04 22.25.22 estimated_duration: 05:36:53.09 time_since_last_log: 00:09:16.83 time_per_update: 00:00:00.22 
11-04 21:18:40 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 21:18:40 I loss/online/main/E1: 2.3696117401123047
11-04 21:18:40 I loss/online/total/E1: 2.3696117401123047
11-04 21:18:40 I accuracy1/online/main/E1: 0.625326
11-04 21:27:37 I ------------------
11-04 21:27:37 I Epoch 322/400 (E322_U805644_S412489728)
11-04 21:27:37 I ETA: 11.04 22.35.26 estimated_duration: 05:46:57.13 time_since_last_log: 00:08:56.62 time_per_update: 00:00:00.21 
11-04 21:27:37 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 21:27:37 I loss/online/main/E1: 2.3726930618286133
11-04 21:27:37 I loss/online/total/E1: 2.3726930618286133
11-04 21:27:37 I accuracy1/online/main/E1: 0.625090
11-04 21:36:33 I ------------------
11-04 21:36:33 I Epoch 323/400 (E323_U808146_S413770752)
11-04 21:36:33 I ETA: 11.04 22.45.26 estimated_duration: 05:56:56.92 time_since_last_log: 00:08:56.21 time_per_update: 00:00:00.21 
11-04 21:36:33 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 21:36:33 I loss/online/main/E1: 2.3609819412231445
11-04 21:36:33 I loss/online/total/E1: 2.3609819412231445
11-04 21:36:33 I accuracy1/online/main/E1: 0.627148
11-04 21:45:29 I ------------------
11-04 21:45:29 I Epoch 324/400 (E324_U810648_S415051776)
11-04 21:45:29 I ETA: 11.04 22.55.22 estimated_duration: 06:06:53.17 time_since_last_log: 00:08:56.35 time_per_update: 00:00:00.21 
11-04 21:45:29 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 21:45:29 I loss/online/main/E1: 2.351855993270874
11-04 21:45:29 I loss/online/total/E1: 2.351855993270874
11-04 21:45:29 I accuracy1/online/main/E1: 0.628051
11-04 21:54:27 I ------------------
11-04 21:54:27 I Epoch 325/400 (E325_U813150_S416332800)
11-04 21:54:27 I ETA: 11.04 23.05.16 estimated_duration: 06:16:47.34 time_since_last_log: 00:08:57.65 time_per_update: 00:00:00.21 
11-04 21:54:27 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 21:54:27 I loss/online/main/E1: 2.3495688438415527
11-04 21:54:27 I loss/online/total/E1: 2.3495688438415527
11-04 21:54:27 I accuracy1/online/main/E1: 0.628830
11-04 21:54:44 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-04 21:54:44 I accuracy1/val/main: 0.794160
11-04 21:54:44 I loss/val/main: 0.84375
11-04 22:03:39 I ------------------
11-04 22:03:39 I Epoch 326/400 (E326_U815652_S417613824)
11-04 22:03:39 I ETA: 11.04 23.15.25 estimated_duration: 06:26:56.12 time_since_last_log: 00:09:12.53 time_per_update: 00:00:00.22 
11-04 22:03:39 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 22:03:39 I loss/online/main/E1: 2.3429737091064453
11-04 22:03:39 I loss/online/total/E1: 2.3429737091064453
11-04 22:03:39 I accuracy1/online/main/E1: 0.628558
11-04 22:12:36 I ------------------
11-04 22:12:36 I Epoch 327/400 (E327_U818154_S418894848)
11-04 22:12:36 I ETA: 11.04 23.25.10 estimated_duration: 06:36:41.25 time_since_last_log: 00:08:56.26 time_per_update: 00:00:00.21 
11-04 22:12:36 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 22:12:36 I loss/online/main/E1: 2.341447591781616
11-04 22:12:36 I loss/online/total/E1: 2.341447591781616
11-04 22:12:36 I accuracy1/online/main/E1: 0.627938
11-04 22:21:32 I ------------------
11-04 22:21:32 I Epoch 328/400 (E328_U820656_S420175872)
11-04 22:21:32 I ETA: 11.04 23.34.52 estimated_duration: 06:46:23.36 time_since_last_log: 00:08:56.71 time_per_update: 00:00:00.21 
11-04 22:21:32 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 22:21:32 I loss/online/main/E1: 2.3372199535369873
11-04 22:21:32 I loss/online/total/E1: 2.3372199535369873
11-04 22:21:32 I accuracy1/online/main/E1: 0.631557
11-04 22:30:31 I ------------------
11-04 22:30:31 I Epoch 329/400 (E329_U823158_S421456896)
11-04 22:30:31 I ETA: 11.04 23.44.33 estimated_duration: 06:56:04.14 time_since_last_log: 00:08:58.54 time_per_update: 00:00:00.21 
11-04 22:30:31 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 22:30:31 I loss/online/main/E1: 2.336357593536377
11-04 22:30:31 I loss/online/total/E1: 2.336357593536377
11-04 22:30:31 I accuracy1/online/main/E1: 0.631496
11-04 22:39:27 I ------------------
11-04 22:39:27 I Epoch 330/400 (E330_U825660_S422737920)
11-04 22:39:27 I ETA: 11.04 23.54.08 estimated_duration: 07:05:38.69 time_since_last_log: 00:08:56.31 time_per_update: 00:00:00.21 
11-04 22:39:27 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 22:39:27 I loss/online/main/E1: 2.3202712535858154
11-04 22:39:27 I loss/online/total/E1: 2.3202712535858154
11-04 22:39:27 I accuracy1/online/main/E1: 0.634440
11-04 22:39:28 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest model.th
11-04 22:39:30 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest optim.th
11-04 22:39:30 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/trainer cp=latest.th
11-04 22:39:47 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-04 22:39:47 I accuracy1/val/main: 0.794780
11-04 22:39:47 I loss/val/main: 0.83984375
11-04 22:48:44 I ------------------
11-04 22:48:44 I Epoch 331/400 (E331_U828162_S424018944)
11-04 22:48:44 I ETA: 11.05 00.04.03 estimated_duration: 07:15:34.14 time_since_last_log: 00:09:16.48 time_per_update: 00:00:00.22 
11-04 22:48:44 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 22:48:44 I loss/online/main/E1: 2.318537473678589
11-04 22:48:44 I loss/online/total/E1: 2.318537473678589
11-04 22:48:44 I accuracy1/online/main/E1: 0.632590
11-04 22:57:40 I ------------------
11-04 22:57:40 I Epoch 332/400 (E332_U830664_S425299968)
11-04 22:57:40 I ETA: 11.05 00.13.30 estimated_duration: 07:25:01.23 time_since_last_log: 00:08:55.94 time_per_update: 00:00:00.21 
11-04 22:57:40 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 22:57:40 I loss/online/main/E1: 2.304476022720337
11-04 22:57:40 I loss/online/total/E1: 2.304476022720337
11-04 22:57:40 I accuracy1/online/main/E1: 0.635820
11-04 23:06:38 I ------------------
11-04 23:06:38 I Epoch 333/400 (E333_U833166_S426580992)
11-04 23:06:38 I ETA: 11.05 00.22.57 estimated_duration: 07:34:27.67 time_since_last_log: 00:08:58.24 time_per_update: 00:00:00.21 
11-04 23:06:38 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 23:06:38 I loss/online/main/E1: 2.3177757263183594
11-04 23:06:38 I loss/online/total/E1: 2.3177757263183594
11-04 23:06:38 I accuracy1/online/main/E1: 0.634200
11-04 23:15:34 I ------------------
11-04 23:15:34 I Epoch 334/400 (E334_U835668_S427862016)
11-04 23:15:34 I ETA: 11.05 00.32.18 estimated_duration: 07:43:48.52 time_since_last_log: 00:08:56.41 time_per_update: 00:00:00.21 
11-04 23:15:34 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 23:15:34 I loss/online/main/E1: 2.2974557876586914
11-04 23:15:34 I loss/online/total/E1: 2.2974557876586914
11-04 23:15:34 I accuracy1/online/main/E1: 0.637069
11-04 23:24:31 I ------------------
11-04 23:24:31 I Epoch 335/400 (E335_U838170_S429143040)
11-04 23:24:31 I ETA: 11.05 00.41.35 estimated_duration: 07:53:05.80 time_since_last_log: 00:08:56.24 time_per_update: 00:00:00.21 
11-04 23:24:31 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 23:24:31 I loss/online/main/E1: 2.294508695602417
11-04 23:24:31 I loss/online/total/E1: 2.294508695602417
11-04 23:24:31 I accuracy1/online/main/E1: 0.638171
11-04 23:24:48 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-04 23:24:48 I accuracy1/val/main: 0.796140
11-04 23:24:48 I loss/val/main: 0.82421875
11-04 23:33:45 I ------------------
11-04 23:33:45 I Epoch 336/400 (E336_U840672_S430424064)
11-04 23:33:45 I ETA: 11.05 00.51.10 estimated_duration: 08:02:41.36 time_since_last_log: 00:09:14.38 time_per_update: 00:00:00.22 
11-04 23:33:45 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 23:33:45 I loss/online/main/E1: 2.293789863586426
11-04 23:33:45 I loss/online/total/E1: 2.293789863586426
11-04 23:33:45 I accuracy1/online/main/E1: 0.637556
11-04 23:42:42 I ------------------
11-04 23:42:42 I Epoch 337/400 (E337_U843174_S431705088)
11-04 23:42:42 I ETA: 11.05 01.00.22 estimated_duration: 08:11:52.64 time_since_last_log: 00:08:56.81 time_per_update: 00:00:00.21 
11-04 23:42:42 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 23:42:42 I loss/online/main/E1: 2.292850971221924
11-04 23:42:42 I loss/online/total/E1: 2.292850971221924
11-04 23:42:42 I accuracy1/online/main/E1: 0.637994
11-04 23:51:38 I ------------------
11-04 23:51:38 I Epoch 338/400 (E338_U845676_S432986112)
11-04 23:51:38 I ETA: 11.05 01.09.29 estimated_duration: 08:20:59.62 time_since_last_log: 00:08:55.95 time_per_update: 00:00:00.21 
11-04 23:51:38 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-04 23:51:38 I loss/online/main/E1: 2.2879607677459717
11-04 23:51:38 I loss/online/total/E1: 2.2879607677459717
11-04 23:51:38 I accuracy1/online/main/E1: 0.638177
11-05 00:00:35 I ------------------
11-05 00:00:35 I Epoch 339/400 (E339_U848178_S434267136)
11-05 00:00:35 I ETA: 11.05 01.18.34 estimated_duration: 08:30:05.07 time_since_last_log: 00:08:57.39 time_per_update: 00:00:00.21 
11-05 00:00:35 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 00:00:35 I loss/online/main/E1: 2.2791707515716553
11-05 00:00:35 I loss/online/total/E1: 2.2791707515716553
11-05 00:00:35 I accuracy1/online/main/E1: 0.641015
11-05 00:09:32 I ------------------
11-05 00:09:32 I Epoch 340/400 (E340_U850680_S435548160)
11-05 00:09:32 I ETA: 11.05 01.27.36 estimated_duration: 08:39:07.12 time_since_last_log: 00:08:57.24 time_per_update: 00:00:00.21 
11-05 00:09:32 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 00:09:32 I loss/online/main/E1: 2.2648448944091797
11-05 00:09:32 I loss/online/total/E1: 2.2648448944091797
11-05 00:09:32 I accuracy1/online/main/E1: 0.644392
11-05 00:09:33 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest model.th
11-05 00:09:35 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest optim.th
11-05 00:09:35 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/trainer cp=latest.th
11-05 00:09:52 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-05 00:09:52 I accuracy1/val/main: 0.798360
11-05 00:09:52 I loss/val/main: 0.8203125
11-05 00:18:49 I ------------------
11-05 00:18:49 I Epoch 341/400 (E341_U853182_S436829184)
11-05 00:18:49 I ETA: 11.05 01.36.58 estimated_duration: 08:48:28.48 time_since_last_log: 00:09:16.41 time_per_update: 00:00:00.22 
11-05 00:18:49 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 00:18:49 I loss/online/main/E1: 2.272190570831299
11-05 00:18:49 I loss/online/total/E1: 2.272190570831299
11-05 00:18:49 I accuracy1/online/main/E1: 0.642294
11-05 00:27:45 I ------------------
11-05 00:27:45 I Epoch 342/400 (E342_U855684_S438110208)
11-05 00:27:45 I ETA: 11.05 01.45.52 estimated_duration: 08:57:23.27 time_since_last_log: 00:08:56.52 time_per_update: 00:00:00.21 
11-05 00:27:45 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 00:27:45 I loss/online/main/E1: 2.2710189819335938
11-05 00:27:45 I loss/online/total/E1: 2.2710189819335938
11-05 00:27:45 I accuracy1/online/main/E1: 0.640360
11-05 00:36:42 I ------------------
11-05 00:36:42 I Epoch 343/400 (E343_U858186_S439391232)
11-05 00:36:42 I ETA: 11.05 01.54.45 estimated_duration: 09:06:15.57 time_since_last_log: 00:08:57.06 time_per_update: 00:00:00.21 
11-05 00:36:42 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 00:36:42 I loss/online/main/E1: 2.262967348098755
11-05 00:36:42 I loss/online/total/E1: 2.262967348098755
11-05 00:36:42 I accuracy1/online/main/E1: 0.642739
11-05 00:45:40 I ------------------
11-05 00:45:40 I Epoch 344/400 (E344_U860688_S440672256)
11-05 00:45:40 I ETA: 11.05 02.03.34 estimated_duration: 09:15:05.24 time_since_last_log: 00:08:57.47 time_per_update: 00:00:00.21 
11-05 00:45:40 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 00:45:40 I loss/online/main/E1: 2.2611756324768066
11-05 00:45:40 I loss/online/total/E1: 2.2611756324768066
11-05 00:45:40 I accuracy1/online/main/E1: 0.643253
11-05 00:54:38 I ------------------
11-05 00:54:38 I Epoch 345/400 (E345_U863190_S441953280)
11-05 00:54:38 I ETA: 11.05 02.12.21 estimated_duration: 09:23:52.39 time_since_last_log: 00:08:57.95 time_per_update: 00:00:00.21 
11-05 00:54:38 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 00:54:38 I loss/online/main/E1: 2.252211093902588
11-05 00:54:38 I loss/online/total/E1: 2.252211093902588
11-05 00:54:38 I accuracy1/online/main/E1: 0.644342
11-05 00:54:55 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-05 00:54:55 I accuracy1/val/main: 0.798060
11-05 00:54:55 I loss/val/main: 0.82421875
11-05 01:03:52 I ------------------
11-05 01:03:52 I Epoch 346/400 (E346_U865692_S443234304)
11-05 01:03:52 I ETA: 11.05 02.21.24 estimated_duration: 09:32:55.30 time_since_last_log: 00:09:14.22 time_per_update: 00:00:00.22 
11-05 01:03:52 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 01:03:52 I loss/online/main/E1: 2.2496414184570312
11-05 01:03:52 I loss/online/total/E1: 2.2496414184570312
11-05 01:03:52 I accuracy1/online/main/E1: 0.644619
11-05 01:12:48 I ------------------
11-05 01:12:48 I Epoch 347/400 (E347_U868194_S444515328)
11-05 01:12:48 I ETA: 11.05 02.30.03 estimated_duration: 09:41:33.97 time_since_last_log: 00:08:55.93 time_per_update: 00:00:00.21 
11-05 01:12:48 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 01:12:48 I loss/online/main/E1: 2.2418975830078125
11-05 01:12:48 I loss/online/total/E1: 2.2418975830078125
11-05 01:12:48 I accuracy1/online/main/E1: 0.646395
11-05 01:21:46 I ------------------
11-05 01:21:46 I Epoch 348/400 (E348_U870696_S445796352)
11-05 01:21:46 I ETA: 11.05 02.38.41 estimated_duration: 09:50:12.08 time_since_last_log: 00:08:58.03 time_per_update: 00:00:00.21 
11-05 01:21:46 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 01:21:46 I loss/online/main/E1: 2.2450709342956543
11-05 01:21:46 I loss/online/total/E1: 2.2450709342956543
11-05 01:21:46 I accuracy1/online/main/E1: 0.645931
11-05 01:30:42 I ------------------
11-05 01:30:42 I Epoch 349/400 (E349_U873198_S447077376)
11-05 01:30:42 I ETA: 11.05 02.47.14 estimated_duration: 09:58:45.07 time_since_last_log: 00:08:56.16 time_per_update: 00:00:00.21 
11-05 01:30:42 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 01:30:42 I loss/online/main/E1: 2.2361390590667725
11-05 01:30:42 I loss/online/total/E1: 2.2361390590667725
11-05 01:30:42 I accuracy1/online/main/E1: 0.647694
11-05 01:39:39 I ------------------
11-05 01:39:39 I Epoch 350/400 (E350_U875700_S448358400)
11-05 01:39:39 I ETA: 11.05 02.55.44 estimated_duration: 10:07:15.45 time_since_last_log: 00:08:56.46 time_per_update: 00:00:00.21 
11-05 01:39:39 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 01:39:39 I loss/online/main/E1: 2.232722759246826
11-05 01:39:39 I loss/online/total/E1: 2.232722759246826
11-05 01:39:39 I accuracy1/online/main/E1: 0.648599
11-05 01:39:39 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest model.th
11-05 01:39:41 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest optim.th
11-05 01:39:41 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/trainer cp=latest.th
11-05 01:39:58 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-05 01:39:58 I accuracy1/val/main: 0.799180
11-05 01:39:58 I loss/val/main: 0.8125
11-05 01:48:53 I ------------------
11-05 01:48:53 I Epoch 351/400 (E351_U878202_S449639424)
11-05 01:48:53 I ETA: 11.05 03.04.33 estimated_duration: 10:16:03.84 time_since_last_log: 00:09:14.81 time_per_update: 00:00:00.22 
11-05 01:48:53 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 01:48:53 I loss/online/main/E1: 2.2326459884643555
11-05 01:48:53 I loss/online/total/E1: 2.2326459884643555
11-05 01:48:53 I accuracy1/online/main/E1: 0.647786
11-05 01:57:51 I ------------------
11-05 01:57:51 I Epoch 352/400 (E352_U880704_S450920448)
11-05 01:57:51 I ETA: 11.05 03.12.59 estimated_duration: 10:24:30.11 time_since_last_log: 00:08:58.00 time_per_update: 00:00:00.21 
11-05 01:57:51 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 01:57:51 I loss/online/main/E1: 2.2290799617767334
11-05 01:57:51 I loss/online/total/E1: 2.2290799617767334
11-05 01:57:51 I accuracy1/online/main/E1: 0.649253
11-05 02:06:47 I ------------------
11-05 02:06:47 I Epoch 353/400 (E353_U883206_S452201472)
11-05 02:06:47 I ETA: 11.05 03.21.20 estimated_duration: 10:32:51.10 time_since_last_log: 00:08:55.89 time_per_update: 00:00:00.21 
11-05 02:06:47 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 02:06:47 I loss/online/main/E1: 2.2138864994049072
11-05 02:06:47 I loss/online/total/E1: 2.2138864994049072
11-05 02:06:47 I accuracy1/online/main/E1: 0.650348
11-05 02:15:44 I ------------------
11-05 02:15:44 I Epoch 354/400 (E354_U885708_S453482496)
11-05 02:15:44 I ETA: 11.05 03.29.39 estimated_duration: 10:41:10.24 time_since_last_log: 00:08:56.75 time_per_update: 00:00:00.21 
11-05 02:15:44 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 02:15:44 I loss/online/main/E1: 2.2144410610198975
11-05 02:15:44 I loss/online/total/E1: 2.2144410610198975
11-05 02:15:44 I accuracy1/online/main/E1: 0.651613
11-05 02:24:41 I ------------------
11-05 02:24:41 I Epoch 355/400 (E355_U888210_S454763520)
11-05 02:24:41 I ETA: 11.05 03.37.56 estimated_duration: 10:49:26.74 time_since_last_log: 00:08:56.91 time_per_update: 00:00:00.21 
11-05 02:24:41 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 02:24:41 I loss/online/main/E1: 2.206742525100708
11-05 02:24:41 I loss/online/total/E1: 2.206742525100708
11-05 02:24:41 I accuracy1/online/main/E1: 0.653745
11-05 02:24:58 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-05 02:24:58 I accuracy1/val/main: 0.801160
11-05 02:24:58 I loss/val/main: 0.8046875
11-05 02:33:55 I ------------------
11-05 02:33:55 I Epoch 356/400 (E356_U890712_S456044544)
11-05 02:33:55 I ETA: 11.05 03.46.29 estimated_duration: 10:58:00.02 time_since_last_log: 00:09:14.34 time_per_update: 00:00:00.22 
11-05 02:33:55 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 02:33:55 I loss/online/main/E1: 2.199016809463501
11-05 02:33:55 I loss/online/total/E1: 2.199016809463501
11-05 02:33:55 I accuracy1/online/main/E1: 0.654490
11-05 02:42:52 I ------------------
11-05 02:42:52 I Epoch 357/400 (E357_U893214_S457325568)
11-05 02:42:52 I ETA: 11.05 03.54.39 estimated_duration: 11:06:10.14 time_since_last_log: 00:08:56.25 time_per_update: 00:00:00.21 
11-05 02:42:52 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 02:42:52 I loss/online/main/E1: 2.199368715286255
11-05 02:42:52 I loss/online/total/E1: 2.199368715286255
11-05 02:42:52 I accuracy1/online/main/E1: 0.654111
11-05 02:51:48 I ------------------
11-05 02:51:48 I Epoch 358/400 (E358_U895716_S458606592)
11-05 02:51:48 I ETA: 11.05 04.02.47 estimated_duration: 11:14:17.79 time_since_last_log: 00:08:56.49 time_per_update: 00:00:00.21 
11-05 02:51:48 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 02:51:48 I loss/online/main/E1: 2.1896300315856934
11-05 02:51:48 I loss/online/total/E1: 2.1896300315856934
11-05 02:51:48 I accuracy1/online/main/E1: 0.655863
11-05 03:00:45 I ------------------
11-05 03:00:45 I Epoch 359/400 (E359_U898218_S459887616)
11-05 03:00:45 I ETA: 11.05 04.10.53 estimated_duration: 11:22:23.58 time_since_last_log: 00:08:57.26 time_per_update: 00:00:00.21 
11-05 03:00:45 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 03:00:45 I loss/online/main/E1: 2.193667411804199
11-05 03:00:45 I loss/online/total/E1: 2.193667411804199
11-05 03:00:45 I accuracy1/online/main/E1: 0.653361
11-05 03:09:43 I ------------------
11-05 03:09:43 I Epoch 360/400 (E360_U900720_S461168640)
11-05 03:09:43 I ETA: 11.05 04.18.56 estimated_duration: 11:30:27.11 time_since_last_log: 00:08:57.67 time_per_update: 00:00:00.21 
11-05 03:09:43 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 03:09:43 I loss/online/main/E1: 2.194713830947876
11-05 03:09:43 I loss/online/total/E1: 2.194713830947876
11-05 03:09:43 I accuracy1/online/main/E1: 0.654631
11-05 03:09:44 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest model.th
11-05 03:09:46 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest optim.th
11-05 03:09:46 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/trainer cp=latest.th
11-05 03:10:02 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-05 03:10:03 I accuracy1/val/main: 0.802460
11-05 03:10:03 I loss/val/main: 0.8046875
11-05 03:18:59 I ------------------
11-05 03:18:59 I Epoch 361/400 (E361_U903222_S462449664)
11-05 03:18:59 I ETA: 11.05 04.27.17 estimated_duration: 11:38:48.46 time_since_last_log: 00:09:16.17 time_per_update: 00:00:00.22 
11-05 03:18:59 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 03:18:59 I loss/online/main/E1: 2.184903144836426
11-05 03:18:59 I loss/online/total/E1: 2.184903144836426
11-05 03:18:59 I accuracy1/online/main/E1: 0.655860
11-05 03:27:56 I ------------------
11-05 03:27:56 I Epoch 362/400 (E362_U905724_S463730688)
11-05 03:27:56 I ETA: 11.05 04.35.14 estimated_duration: 11:46:45.33 time_since_last_log: 00:08:56.53 time_per_update: 00:00:00.21 
11-05 03:27:56 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 03:27:56 I loss/online/main/E1: 2.1875205039978027
11-05 03:27:56 I loss/online/total/E1: 2.1875205039978027
11-05 03:27:56 I accuracy1/online/main/E1: 0.656440
11-05 03:36:54 I ------------------
11-05 03:36:54 I Epoch 363/400 (E363_U908226_S465011712)
11-05 03:36:54 I ETA: 11.05 04.43.10 estimated_duration: 11:54:41.23 time_since_last_log: 00:08:58.05 time_per_update: 00:00:00.21 
11-05 03:36:54 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 03:36:54 I loss/online/main/E1: 2.1880059242248535
11-05 03:36:54 I loss/online/total/E1: 2.1880059242248535
11-05 03:36:54 I accuracy1/online/main/E1: 0.654277
11-05 03:45:50 I ------------------
11-05 03:45:50 I Epoch 364/400 (E364_U910728_S466292736)
11-05 03:45:50 I ETA: 11.05 04.51.01 estimated_duration: 12:02:32.11 time_since_last_log: 00:08:55.86 time_per_update: 00:00:00.21 
11-05 03:45:50 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 03:45:50 I loss/online/main/E1: 2.1771130561828613
11-05 03:45:50 I loss/online/total/E1: 2.1771130561828613
11-05 03:45:50 I accuracy1/online/main/E1: 0.658537
11-05 03:54:47 I ------------------
11-05 03:54:47 I Epoch 365/400 (E365_U913230_S467573760)
11-05 03:54:47 I ETA: 11.05 04.58.51 estimated_duration: 12:10:21.55 time_since_last_log: 00:08:56.90 time_per_update: 00:00:00.21 
11-05 03:54:47 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 03:54:47 I loss/online/main/E1: 2.172823905944824
11-05 03:54:47 I loss/online/total/E1: 2.172823905944824
11-05 03:54:47 I accuracy1/online/main/E1: 0.658707
11-05 03:55:04 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-05 03:55:04 I accuracy1/val/main: 0.801940
11-05 03:55:04 I loss/val/main: 0.80078125
11-05 04:03:59 I ------------------
11-05 04:03:59 I Epoch 366/400 (E366_U915732_S468854784)
11-05 04:03:59 I ETA: 11.05 05.06.55 estimated_duration: 12:18:25.80 time_since_last_log: 00:09:12.82 time_per_update: 00:00:00.22 
11-05 04:03:59 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 04:03:59 I loss/online/main/E1: 2.1706314086914062
11-05 04:03:59 I loss/online/total/E1: 2.1706314086914062
11-05 04:03:59 I accuracy1/online/main/E1: 0.659377
11-05 04:12:57 I ------------------
11-05 04:12:57 I Epoch 367/400 (E367_U918234_S470135808)
11-05 04:12:57 I ETA: 11.05 05.14.40 estimated_duration: 12:26:10.77 time_since_last_log: 00:08:57.55 time_per_update: 00:00:00.21 
11-05 04:12:57 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 04:12:57 I loss/online/main/E1: 2.1738157272338867
11-05 04:12:57 I loss/online/total/E1: 2.1738157272338867
11-05 04:12:57 I accuracy1/online/main/E1: 0.656554
11-05 04:21:53 I ------------------
11-05 04:21:53 I Epoch 368/400 (E368_U920736_S471416832)
11-05 04:21:53 I ETA: 11.05 05.22.20 estimated_duration: 12:33:51.05 time_since_last_log: 00:08:55.57 time_per_update: 00:00:00.21 
11-05 04:21:53 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 04:21:53 I loss/online/main/E1: 2.1648080348968506
11-05 04:21:53 I loss/online/total/E1: 2.1648080348968506
11-05 04:21:53 I accuracy1/online/main/E1: 0.658775
11-05 04:30:49 I ------------------
11-05 04:30:49 I Epoch 369/400 (E369_U923238_S472697856)
11-05 04:30:49 I ETA: 11.05 05.29.58 estimated_duration: 12:41:29.38 time_since_last_log: 00:08:56.07 time_per_update: 00:00:00.21 
11-05 04:30:49 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 04:30:49 I loss/online/main/E1: 2.1696228981018066
11-05 04:30:49 I loss/online/total/E1: 2.1696228981018066
11-05 04:30:49 I accuracy1/online/main/E1: 0.657109
11-05 04:39:46 I ------------------
11-05 04:39:46 I Epoch 370/400 (E370_U925740_S473978880)
11-05 04:39:46 I ETA: 11.05 05.37.36 estimated_duration: 12:49:06.60 time_since_last_log: 00:08:57.35 time_per_update: 00:00:00.21 
11-05 04:39:46 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 04:39:46 I loss/online/main/E1: 2.1643810272216797
11-05 04:39:46 I loss/online/total/E1: 2.1643810272216797
11-05 04:39:46 I accuracy1/online/main/E1: 0.658747
11-05 04:39:47 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest model.th
11-05 04:39:48 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest optim.th
11-05 04:39:48 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/trainer cp=latest.th
11-05 04:40:05 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-05 04:40:05 I accuracy1/val/main: 0.803540
11-05 04:40:05 I loss/val/main: 0.80078125
11-05 04:49:03 I ------------------
11-05 04:49:03 I Epoch 371/400 (E371_U928242_S475259904)
11-05 04:49:03 I ETA: 11.05 05.45.31 estimated_duration: 12:57:02.45 time_since_last_log: 00:09:16.91 time_per_update: 00:00:00.22 
11-05 04:49:03 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 04:49:03 I loss/online/main/E1: 2.159963846206665
11-05 04:49:03 I loss/online/total/E1: 2.159963846206665
11-05 04:49:03 I accuracy1/online/main/E1: 0.660202
11-05 04:57:59 I ------------------
11-05 04:57:59 I Epoch 372/400 (E372_U930744_S476540928)
11-05 04:57:59 I ETA: 11.05 05.53.03 estimated_duration: 13:04:33.85 time_since_last_log: 00:08:56.57 time_per_update: 00:00:00.21 
11-05 04:57:59 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 04:57:59 I loss/online/main/E1: 2.149235248565674
11-05 04:57:59 I loss/online/total/E1: 2.149235248565674
11-05 04:57:59 I accuracy1/online/main/E1: 0.662449
11-05 05:06:57 I ------------------
11-05 05:06:57 I Epoch 373/400 (E373_U933246_S477821952)
11-05 05:06:57 I ETA: 11.05 06.00.33 estimated_duration: 13:12:04.00 time_since_last_log: 00:08:57.66 time_per_update: 00:00:00.21 
11-05 05:06:57 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 05:06:57 I loss/online/main/E1: 2.1573524475097656
11-05 05:06:57 I loss/online/total/E1: 2.1573524475097656
11-05 05:06:57 I accuracy1/online/main/E1: 0.660657
11-05 05:15:53 I ------------------
11-05 05:15:53 I Epoch 374/400 (E374_U935748_S479102976)
11-05 05:15:53 I ETA: 11.05 06.07.59 estimated_duration: 13:19:29.80 time_since_last_log: 00:08:55.86 time_per_update: 00:00:00.21 
11-05 05:15:53 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 05:15:53 I loss/online/main/E1: 2.159477949142456
11-05 05:15:53 I loss/online/total/E1: 2.159477949142456
11-05 05:15:53 I accuracy1/online/main/E1: 0.659145
11-05 05:24:50 I ------------------
11-05 05:24:50 I Epoch 375/400 (E375_U938250_S480384000)
11-05 05:24:50 I ETA: 11.05 06.15.23 estimated_duration: 13:26:54.34 time_since_last_log: 00:08:56.91 time_per_update: 00:00:00.21 
11-05 05:24:50 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 05:24:50 I loss/online/main/E1: 2.1429171562194824
11-05 05:24:50 I loss/online/total/E1: 2.1429171562194824
11-05 05:24:50 I accuracy1/online/main/E1: 0.662683
11-05 05:25:07 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-05 05:25:07 I accuracy1/val/main: 0.803880
11-05 05:25:07 I loss/val/main: 0.80078125
11-05 05:34:03 I ------------------
11-05 05:34:03 I Epoch 376/400 (E376_U940752_S481665024)
11-05 05:34:03 I ETA: 11.05 06.23.03 estimated_duration: 13:34:34.02 time_since_last_log: 00:09:13.36 time_per_update: 00:00:00.22 
11-05 05:34:03 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 05:34:03 I loss/online/main/E1: 2.1539485454559326
11-05 05:34:03 I loss/online/total/E1: 2.1539485454559326
11-05 05:34:03 I accuracy1/online/main/E1: 0.661927
11-05 05:42:59 I ------------------
11-05 05:42:59 I Epoch 377/400 (E377_U943254_S482946048)
11-05 05:42:59 I ETA: 11.05 06.30.22 estimated_duration: 13:41:52.65 time_since_last_log: 00:08:55.84 time_per_update: 00:00:00.21 
11-05 05:42:59 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 05:42:59 I loss/online/main/E1: 2.1483097076416016
11-05 05:42:59 I loss/online/total/E1: 2.1483097076416016
11-05 05:42:59 I accuracy1/online/main/E1: 0.662638
11-05 05:51:57 I ------------------
11-05 05:51:57 I Epoch 378/400 (E378_U945756_S484227072)
11-05 05:51:57 I ETA: 11.05 06.37.40 estimated_duration: 13:49:10.91 time_since_last_log: 00:08:57.68 time_per_update: 00:00:00.21 
11-05 05:51:57 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 05:51:57 I loss/online/main/E1: 2.1428444385528564
11-05 05:51:57 I loss/online/total/E1: 2.1428444385528564
11-05 05:51:57 I accuracy1/online/main/E1: 0.664319
11-05 06:00:54 I ------------------
11-05 06:00:54 I Epoch 379/400 (E379_U948258_S485508096)
11-05 06:00:54 I ETA: 11.05 06.44.55 estimated_duration: 13:56:26.00 time_since_last_log: 00:08:56.87 time_per_update: 00:00:00.21 
11-05 06:00:54 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 06:00:54 I loss/online/main/E1: 2.1384143829345703
11-05 06:00:54 I loss/online/total/E1: 2.1384143829345703
11-05 06:00:54 I accuracy1/online/main/E1: 0.662672
11-05 06:09:51 I ------------------
11-05 06:09:51 I Epoch 380/400 (E380_U950760_S486789120)
11-05 06:09:51 I ETA: 11.05 06.52.09 estimated_duration: 14:03:39.55 time_since_last_log: 00:08:57.59 time_per_update: 00:00:00.21 
11-05 06:09:51 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 06:09:51 I loss/online/main/E1: 2.131941795349121
11-05 06:09:51 I loss/online/total/E1: 2.131941795349121
11-05 06:09:51 I accuracy1/online/main/E1: 0.664011
11-05 06:09:52 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest model.th
11-05 06:09:54 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest optim.th
11-05 06:09:54 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/trainer cp=latest.th
11-05 06:10:11 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-05 06:10:11 I accuracy1/val/main: 0.804200
11-05 06:10:11 I loss/val/main: 0.796875
11-05 06:19:07 I ------------------
11-05 06:19:07 I Epoch 381/400 (E381_U953262_S488070144)
11-05 06:19:07 I ETA: 11.05 06.59.39 estimated_duration: 14:11:10.39 time_since_last_log: 00:09:16.24 time_per_update: 00:00:00.22 
11-05 06:19:08 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 06:19:08 I loss/online/main/E1: 2.1401848793029785
11-05 06:19:08 I loss/online/total/E1: 2.1401848793029785
11-05 06:19:08 I accuracy1/online/main/E1: 0.664087
11-05 06:28:06 I ------------------
11-05 06:28:06 I Epoch 382/400 (E382_U955764_S489351168)
11-05 06:28:06 I ETA: 11.05 07.06.49 estimated_duration: 14:18:19.78 time_since_last_log: 00:08:58.01 time_per_update: 00:00:00.21 
11-05 06:28:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 06:28:06 I loss/online/main/E1: 2.1270711421966553
11-05 06:28:06 I loss/online/total/E1: 2.1270711421966553
11-05 06:28:06 I accuracy1/online/main/E1: 0.665930
11-05 06:37:02 I ------------------
11-05 06:37:02 I Epoch 383/400 (E383_U958266_S490632192)
11-05 06:37:02 I ETA: 11.05 07.13.54 estimated_duration: 14:25:25.10 time_since_last_log: 00:08:56.26 time_per_update: 00:00:00.21 
11-05 06:37:02 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 06:37:02 I loss/online/main/E1: 2.1381025314331055
11-05 06:37:02 I loss/online/total/E1: 2.1381025314331055
11-05 06:37:02 I accuracy1/online/main/E1: 0.663923
11-05 06:45:58 I ------------------
11-05 06:45:58 I Epoch 384/400 (E384_U960768_S491913216)
11-05 06:45:58 I ETA: 11.05 07.20.57 estimated_duration: 14:32:28.33 time_since_last_log: 00:08:56.40 time_per_update: 00:00:00.21 
11-05 06:45:58 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 06:45:58 I loss/online/main/E1: 2.1259336471557617
11-05 06:45:58 I loss/online/total/E1: 2.1259336471557617
11-05 06:45:58 I accuracy1/online/main/E1: 0.665934
11-05 06:54:54 I ------------------
11-05 06:54:54 I Epoch 385/400 (E385_U963270_S493194240)
11-05 06:54:54 I ETA: 11.05 07.27.58 estimated_duration: 14:39:29.03 time_since_last_log: 00:08:56.08 time_per_update: 00:00:00.21 
11-05 06:54:54 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 06:54:54 I loss/online/main/E1: 2.1343212127685547
11-05 06:54:54 I loss/online/total/E1: 2.1343212127685547
11-05 06:54:54 I accuracy1/online/main/E1: 0.663056
11-05 06:55:11 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-05 06:55:11 I accuracy1/val/main: 0.804320
11-05 06:55:11 I loss/val/main: 0.796875
11-05 07:04:09 I ------------------
11-05 07:04:09 I Epoch 386/400 (E386_U965772_S494475264)
11-05 07:04:09 I ETA: 11.05 07.35.16 estimated_duration: 14:46:46.71 time_since_last_log: 00:09:14.57 time_per_update: 00:00:00.22 
11-05 07:04:09 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 07:04:09 I loss/online/main/E1: 2.1319775581359863
11-05 07:04:09 I loss/online/total/E1: 2.1319775581359863
11-05 07:04:09 I accuracy1/online/main/E1: 0.663617
11-05 07:13:05 I ------------------
11-05 07:13:05 I Epoch 387/400 (E387_U968274_S495756288)
11-05 07:13:05 I ETA: 11.05 07.42.12 estimated_duration: 14:53:43.15 time_since_last_log: 00:08:56.22 time_per_update: 00:00:00.21 
11-05 07:13:05 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 07:13:05 I loss/online/main/E1: 2.128599166870117
11-05 07:13:05 I loss/online/total/E1: 2.128599166870117
11-05 07:13:05 I accuracy1/online/main/E1: 0.667121
11-05 07:22:02 I ------------------
11-05 07:22:02 I Epoch 388/400 (E388_U970776_S497037312)
11-05 07:22:02 I ETA: 11.05 07.49.07 estimated_duration: 15:00:38.05 time_since_last_log: 00:08:56.82 time_per_update: 00:00:00.21 
11-05 07:22:02 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 07:22:02 I loss/online/main/E1: 2.1245734691619873
11-05 07:22:02 I loss/online/total/E1: 2.1245734691619873
11-05 07:22:02 I accuracy1/online/main/E1: 0.667863
11-05 07:30:58 I ------------------
11-05 07:30:58 I Epoch 389/400 (E389_U973278_S498318336)
11-05 07:30:58 I ETA: 11.05 07.55.59 estimated_duration: 15:07:29.94 time_since_last_log: 00:08:55.96 time_per_update: 00:00:00.21 
11-05 07:30:58 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 07:30:58 I loss/online/main/E1: 2.1195712089538574
11-05 07:30:58 I loss/online/total/E1: 2.1195712089538574
11-05 07:30:58 I accuracy1/online/main/E1: 0.667791
11-05 07:39:56 I ------------------
11-05 07:39:56 I Epoch 390/400 (E390_U975780_S499599360)
11-05 07:39:56 I ETA: 11.05 08.02.51 estimated_duration: 15:14:21.91 time_since_last_log: 00:08:58.11 time_per_update: 00:00:00.21 
11-05 07:39:56 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 07:39:56 I loss/online/main/E1: 2.1354763507843018
11-05 07:39:56 I loss/online/total/E1: 2.1354763507843018
11-05 07:39:56 I accuracy1/online/main/E1: 0.663194
11-05 07:39:57 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest model.th
11-05 07:39:58 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest optim.th
11-05 07:39:58 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/trainer cp=latest.th
11-05 07:40:15 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-05 07:40:15 I accuracy1/val/main: 0.804480
11-05 07:40:15 I loss/val/main: 0.79296875
11-05 07:49:12 I ------------------
11-05 07:49:12 I Epoch 391/400 (E391_U978282_S500880384)
11-05 07:49:12 I ETA: 11.05 08.10.00 estimated_duration: 15:21:30.50 time_since_last_log: 00:09:16.41 time_per_update: 00:00:00.22 
11-05 07:49:12 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 07:49:12 I loss/online/main/E1: 2.118483543395996
11-05 07:49:12 I loss/online/total/E1: 2.118483543395996
11-05 07:49:12 I accuracy1/online/main/E1: 0.667493
11-05 07:58:11 I ------------------
11-05 07:58:11 I Epoch 392/400 (E392_U980784_S502161408)
11-05 07:58:11 I ETA: 11.05 08.16.47 estimated_duration: 15:28:18.43 time_since_last_log: 00:08:58.32 time_per_update: 00:00:00.21 
11-05 07:58:11 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 07:58:11 I loss/online/main/E1: 2.1217710971832275
11-05 07:58:11 I loss/online/total/E1: 2.1217710971832275
11-05 07:58:11 I accuracy1/online/main/E1: 0.665315
11-05 08:07:07 I ------------------
11-05 08:07:07 I Epoch 393/400 (E393_U983286_S503442432)
11-05 08:07:07 I ETA: 11.05 08.23.31 estimated_duration: 15:35:02.39 time_since_last_log: 00:08:56.47 time_per_update: 00:00:00.21 
11-05 08:07:07 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 08:07:07 I loss/online/main/E1: 2.126868486404419
11-05 08:07:07 I loss/online/total/E1: 2.126868486404419
11-05 08:07:07 I accuracy1/online/main/E1: 0.666210
11-05 08:16:05 I ------------------
11-05 08:16:05 I Epoch 394/400 (E394_U985788_S504723456)
11-05 08:16:05 I ETA: 11.05 08.30.15 estimated_duration: 15:41:45.65 time_since_last_log: 00:08:57.80 time_per_update: 00:00:00.21 
11-05 08:16:05 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 08:16:05 I loss/online/main/E1: 2.1265218257904053
11-05 08:16:05 I loss/online/total/E1: 2.1265218257904053
11-05 08:16:05 I accuracy1/online/main/E1: 0.666090
11-05 08:25:01 I ------------------
11-05 08:25:01 I Epoch 395/400 (E395_U988290_S506004480)
11-05 08:25:01 I ETA: 11.05 08.36.54 estimated_duration: 15:48:25.35 time_since_last_log: 00:08:56.31 time_per_update: 00:00:00.21 
11-05 08:25:01 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 08:25:01 I loss/online/main/E1: 2.123476028442383
11-05 08:25:01 I loss/online/total/E1: 2.123476028442383
11-05 08:25:01 I accuracy1/online/main/E1: 0.667952
11-05 08:25:18 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-05 08:25:18 I accuracy1/val/main: 0.804420
11-05 08:25:18 I loss/val/main: 0.796875
11-05 08:34:15 I ------------------
11-05 08:34:15 I Epoch 396/400 (E396_U990792_S507285504)
11-05 08:34:15 I ETA: 11.05 08.43.50 estimated_duration: 15:55:20.92 time_since_last_log: 00:09:14.02 time_per_update: 00:00:00.22 
11-05 08:34:15 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 08:34:15 I loss/online/main/E1: 2.124210834503174
11-05 08:34:15 I loss/online/total/E1: 2.124210834503174
11-05 08:34:15 I accuracy1/online/main/E1: 0.666700
11-05 08:43:12 I ------------------
11-05 08:43:12 I Epoch 397/400 (E397_U993294_S508566528)
11-05 08:43:12 I ETA: 11.05 08.50.26 estimated_duration: 16:01:56.60 time_since_last_log: 00:08:56.36 time_per_update: 00:00:00.21 
11-05 08:43:12 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 08:43:12 I loss/online/main/E1: 2.1137022972106934
11-05 08:43:12 I loss/online/total/E1: 2.1137022972106934
11-05 08:43:12 I accuracy1/online/main/E1: 0.666795
11-05 08:52:09 I ------------------
11-05 08:52:09 I Epoch 398/400 (E398_U995796_S509847552)
11-05 08:52:09 I ETA: 11.05 08.57.01 estimated_duration: 16:08:31.71 time_since_last_log: 00:08:57.77 time_per_update: 00:00:00.21 
11-05 08:52:09 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 08:52:09 I loss/online/main/E1: 2.1114180088043213
11-05 08:52:09 I loss/online/total/E1: 2.1114180088043213
11-05 08:52:09 I accuracy1/online/main/E1: 0.668191
11-05 09:01:06 I ------------------
11-05 09:01:06 I Epoch 399/400 (E399_U998298_S511128576)
11-05 09:01:06 I ETA: 11.05 09.03.33 estimated_duration: 16:15:04.08 time_since_last_log: 00:08:57.02 time_per_update: 00:00:00.21 
11-05 09:01:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 09:01:06 I loss/online/main/E1: 2.1190474033355713
11-05 09:01:06 I loss/online/total/E1: 2.1190474033355713
11-05 09:01:07 I accuracy1/online/main/E1: 0.667527
11-05 09:10:04 I ------------------
11-05 09:10:04 I Epoch 400/400 (E400_U1000800_S512409600)
11-05 09:10:04 I ETA: 11.05 09.10.04 estimated_duration: 16:21:34.53 time_since_last_log: 00:08:57.08 time_per_update: 00:00:00.21 
11-05 09:10:04 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-05 09:10:04 I loss/online/main/E1: 2.1252694129943848
11-05 09:10:04 I loss/online/total/E1: 2.1252694129943848
11-05 09:10:04 I accuracy1/online/main/E1: 0.665774
11-05 09:10:04 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest model.th
11-05 09:10:06 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest optim.th
11-05 09:10:06 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/trainer cp=latest.th
11-05 09:10:23 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-05 09:10:23 I accuracy1/val/main: 0.804700
11-05 09:10:23 I loss/val/main: 0.796875
11-05 09:10:25 I ------------------
11-05 09:10:25 I AFTER TRAINING
11-05 09:10:25 I ------------------
11-05 09:10:25 I total_train_data_time:   [83.59, 78.14, 79.06, 75.03]
11-05 09:10:25 I total_update_time: [57541.03, 57561.70, 57532.58, 57547.08]
11-05 09:10:26 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=last model.th
11-05 09:10:26 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/trainer cp=last.th
11-05 09:10:26 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest model.th
11-05 09:10:28 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/vislstm cp=latest optim.th
11-05 09:10:28 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/checkpoints/trainer cp=latest.th
11-05 09:10:28 I ------------------
11-05 09:10:28 I offline_accuracy_callback dataset_key=val.x.class
11-05 09:10:28 I total_data_time:    [0.03, 0.03, 0.07, 0.03]
11-05 09:10:28 I total_forward_time: [3.78, 3.78, 3.75, 3.78]
11-05 09:10:28 I writing 5610 log entries to /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/primitive/entries.th
11-05 09:10:28 I ------------------
11-05 09:10:28 I summarize logvalues
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/providers/summary_providers/primitive_summary_provider.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  entries = torch.load(entries_uri)
11-05 09:10:28 I loss/online/main/U50/min: 1.9881757497787476
11-05 09:10:28 I loss/online/total/U50/min: 1.9881757497787476
11-05 09:10:28 I accuracy1/online/main/U50/max: 0.6972265243530273
11-05 09:10:28 I loss/online/main/E1/min: 2.1114180088043213
11-05 09:10:28 I loss/online/total/E1/min: 2.1114180088043213
11-05 09:10:28 I accuracy1/online/main/E1/max: 0.6681911945343018
11-05 09:10:28 I accuracy1/val/main/max: 0.8047000169754028
11-05 09:10:28 I loss/val/main/min: 0.79296875
11-05 09:10:28 I pushing summarized logvalues to wandb
11-05 09:10:28 W cuda profiling is not activated -> all cuda calls are executed asynchronously -> this will result in inaccurate profiling times where the time for all asynchronous cuda operation will be attributed to the first synchronous cuda operation https://github.com/BenediktAlkin/KappaProfiler?tab=readme-ov-file#time-async-operations
11-05 09:10:28 I full profiling times:
 59541.54 train
     0.00 train.DatasetStatsCallback.before_training
     0.00 train.ParamCountCallback.before_training
     0.00 train.CopyPreviousConfigCallback.before_training
     0.00 train.CopyPreviousSummaryCallback.before_training
     0.00 train.ProgressCallback(every_n_epochs=1).before_training
     0.00 train.OnlineAccuracyCallback(every_n_updates=50).before_training
     0.00 train.OnlineAccuracyCallback(every_n_epochs=1).before_training
     0.00 train.CheckpointCallback().before_training
     0.00 train.CheckpointCallback(every_n_epochs=10).before_training
     0.00 train.OfflineAccuracyCallback(every_n_epochs=5).before_training
     0.21 train.iterator
    83.59 train.data_loading
 57541.03 train.update
     2.71 train.OnlineLossCallback(every_n_epochs=1).track_after_accumulation_step
     1.74 train.OnlineLossCallback(every_n_updates=50).track_after_accumulation_step
   144.80 train.OnlineAccuracyCallback(every_n_updates=50).track_after_accumulation_step
    87.84 train.OnlineAccuracyCallback(every_n_epochs=1).track_after_accumulation_step
     1.07 train.TrainTimeCallback(every_n_epochs=1).track_after_update_step
     0.18 train.LrCallback(every_n_updates=50).after_update
     0.03 train.FreezerCallback(every_n_updates=50).after_update
     7.17 train.OnlineLossCallback(every_n_updates=50).after_update
     1.70 train.OnlineAccuracyCallback(every_n_updates=50).after_update
     0.15 train.ProgressCallback(every_n_epochs=1).after_epoch
     0.28 train.TrainTimeCallback(every_n_epochs=1).after_epoch
     1.45 train.OnlineLossCallback(every_n_epochs=1).after_epoch
     0.82 train.OnlineAccuracyCallback(every_n_epochs=1).after_epoch
   375.68 train.OfflineAccuracyCallback(every_n_epochs=5).after_epoch
    26.54 train.CheckpointCallback(every_n_epochs=10).after_epoch
     0.00 train.TrainTimeCallback(every_n_epochs=1).after_training
     0.66 train.CheckpointCallback().after_training
     2.11 train.CheckpointCallback(every_n_epochs=10).after_training
11-05 09:10:28 I ------------------
11-05 09:10:28 I CLEANUP
11-05 09:10:28 I ------------------
11-05 09:10:28 I encountered 1 warnings
11-05 09:10:28 I encountered 0 errors
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:          accuracy1/online/main/E1 ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:         accuracy1/online/main/U50 ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:                accuracy1/val/main ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                             epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:               loss/online/main/E1 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:              loss/online/main/U50 ‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:              loss/online/total/E1 ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:             loss/online/total/U50 ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ
wandb:                     loss/val/main ‚ñà‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:          optim/lr/vislstm/default ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: optim/lr/vislstm/weight_decay=0e0 ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                            sample ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                            update ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:          accuracy1/online/main/E1 0.66577
wandb:     accuracy1/online/main/E1/last 0.66577
wandb:      accuracy1/online/main/E1/max 0.66819
wandb:         accuracy1/online/main/U50 0.66617
wandb:    accuracy1/online/main/U50/last 0.66617
wandb:     accuracy1/online/main/U50/max 0.69723
wandb:                accuracy1/val/main 0.8047
wandb:           accuracy1/val/main/last 0.8047
wandb:            accuracy1/val/main/max 0.8047
wandb:                ds_stats/train/len 1281167
wandb:                  ds_stats/val/len 50000
wandb:                             epoch 400
wandb:               loss/online/main/E1 2.12527
wandb:          loss/online/main/E1/last 2.12527
wandb:           loss/online/main/E1/min 2.11142
wandb:              loss/online/main/U50 2.12294
wandb:         loss/online/main/U50/last 2.12294
wandb:          loss/online/main/U50/min 1.98818
wandb:              loss/online/total/E1 2.12527
wandb:         loss/online/total/E1/last 2.12527
wandb:          loss/online/total/E1/min 2.11142
wandb:             loss/online/total/U50 2.12294
wandb:        loss/online/total/U50/last 2.12294
wandb:         loss/online/total/U50/min 1.98818
wandb:                     loss/val/main 0.79688
wandb:                loss/val/main/last 0.79688
wandb:                 loss/val/main/min 0.79297
wandb:          optim/lr/vislstm/default 0.0
wandb: optim/lr/vislstm/weight_decay=0e0 0.0
wandb:        param_count/vislstm/frozen 0
wandb:     param_count/vislstm/trainable 89592616
wandb:                            sample 512409600
wandb:                            update 1000800
wandb: 
wandb: üöÄ View run in1k-lstm-80m16-e400res192-bialter-bilatflat-lr1e3-conv2d3-bias/in1k at: https://wandb.ai/beka-kalmahanbet-mbzuai/minLSTM/runs/ddgjaz5t
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/beka-kalmahanbet-mbzuai/minLSTM
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /home/beknur.kalmakhanbet/save/in1k/ddgjaz5t/wandb/run-20251104_163756-ddgjaz5t/logs
