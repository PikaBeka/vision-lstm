MASTER_ADDR: gpu-53
CUDA_VISIBLE_DEVICES=0,1,2,3
Thu Sep 25 16:13:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   26C    P0             53W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   26C    P0             50W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   25C    P0             51W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   25C    P0             49W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
torch: 2.5.1+cu121 cuda: 12.1 cuda available: True
09-25 16:13:37 I initializing rank=1 local_rank=1 nodes=1 hostname=gpu-53 master_addr=gpu-53 master_port=55555 (waiting for all 4 processes to connect)
09-25 16:13:37 I initializing rank=0 local_rank=0 nodes=1 hostname=gpu-53 master_addr=gpu-53 master_port=55555 (waiting for all 4 processes to connect)
09-25 16:13:37 I initializing rank=2 local_rank=2 nodes=1 hostname=gpu-53 master_addr=gpu-53 master_port=55555 (waiting for all 4 processes to connect)
09-25 16:13:37 I initializing rank=3 local_rank=3 nodes=1 hostname=gpu-53 master_addr=gpu-53 master_port=55555 (waiting for all 4 processes to connect)
[rank3]:[W925 16:13:38.666197782 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W925 16:13:38.074827235 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W925 16:13:38.322772063 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W925 16:13:38.325950300 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
09-25 16:13:39 I initialized process rank=1 local_rank=1 pid=3625721
09-25 16:13:39 I initialized process rank=2 local_rank=2 pid=3625722
09-25 16:13:39 I initialized process rank=3 local_rank=3 pid=3625723
09-25 16:13:39 I initialized process rank=0 local_rank=0 pid=3625720
09-25 16:13:39 I initialized 4 processes
09-25 16:13:39 W disabled cudnn benchmark
09-25 16:13:39 W enabled cudnn deterministic
09-25 16:13:39 I log file: /home/beknur.kalmakhanbet/save/in1k/a0gelq34/log.txt
09-25 16:13:39 I no seed specified -> using seed=0
09-25 16:13:39 I ------------------
09-25 16:13:39 I initializing wandb (mode=disabled)
fatal: No annotated tags can describe 'ec5acd7aa3ce77ad5724a951e54be1fb0f907e64'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe 'ec5acd7aa3ce77ad5724a951e54be1fb0f907e64'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe 'ec5acd7aa3ce77ad5724a951e54be1fb0f907e64'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe 'ec5acd7aa3ce77ad5724a951e54be1fb0f907e64'.
However, there were unannotated tags: try --tags.
09-25 16:13:39 I ------------------
09-25 16:13:39 I stage_id: a0gelq34
09-25 16:13:39 I python main_train.py --hp src/vislstm/yamls/pretrain/vil/lstm_80M16_e400_bialter_bilatflat_conv2d3_lr1e3_res192_bias.yaml --resume_stage_id ogqgazbl --resume_checkpoint latest --num_workers 5
09-25 16:13:39 I ------------------
09-25 16:13:39 I VERSION CHECK
09-25 16:13:39 I executable: /home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/bin/python
09-25 16:13:39 I python version: 3.9.21
09-25 16:13:39 I torch version: 2.5.1+cu121
09-25 16:13:39 I torch.cuda version: 12.1
09-25 16:13:39 I torchvision.version: 0.20.1+cu121
09-25 16:13:41 I torchmetrics version: 1.6.2
09-25 16:13:41 I kappaschedules version: 0.0.31
09-25 16:13:41 I kappamodules version: 0.1.76
09-25 16:13:41 I ------------------
09-25 16:13:41 I SYSTEM INFO
09-25 16:13:41 I host name: gpu-53
09-25 16:13:41 I OS: Linux-5.15.161-ql-generic-13.0-14-x86_64-with-glibc2.35
09-25 16:13:41 I OS version: #1 SMP Wed Jun 26 16:19:39 UTC 2024
fatal: No annotated tags can describe 'ec5acd7aa3ce77ad5724a951e54be1fb0f907e64'.
However, there were unannotated tags: try --tags.
09-25 16:13:42 I initialized process rank=1 local_rank=1 pid=3625721 hostname=gpu-53
09-25 16:13:43 I CUDA version: 12.4
09-25 16:13:43 I current commit hash: 38066a64724b9a34b6f051f3df3c5f68def24e6b
fatal: No annotated tags can describe 'ec5acd7aa3ce77ad5724a951e54be1fb0f907e64'.
However, there were unannotated tags: try --tags.
09-25 16:13:43 I initialized process rank=3 local_rank=3 pid=3625723 hostname=gpu-53
fatal: No annotated tags can describe 'ec5acd7aa3ce77ad5724a951e54be1fb0f907e64'.
However, there were unannotated tags: try --tags.
09-25 16:13:43 I latest git tag: 
09-25 16:13:43 I initialized process rank=0 local_rank=0 pid=3625720 hostname=gpu-53
09-25 16:13:43 I total_cpu_count: 16
09-25 16:13:43 I ------------------
09-25 16:13:43 I STATIC CONFIG
09-25 16:13:43 I account_name: beknur.kalmakhanbet
09-25 16:13:43 I output_path: /home/beknur.kalmakhanbet/save
09-25 16:13:43 I ------------------
09-25 16:13:43 I CLI ARGS
09-25 16:13:43 I hp: src/vislstm/yamls/pretrain/vil/lstm_80M16_e400_bialter_bilatflat_conv2d3_lr1e3_res192_bias.yaml
09-25 16:13:43 I accelerator: gpu
09-25 16:13:43 I num_workers: 5
09-25 16:13:43 I testrun: False
09-25 16:13:43 I minmodelrun: False
09-25 16:13:43 I mindatarun: False
09-25 16:13:43 I mindurationrun: False
09-25 16:13:43 I static_config_uri: static_config.yaml
09-25 16:13:43 I resume_stage_id: ogqgazbl
09-25 16:13:43 I resume_checkpoint: latest
09-25 16:13:43 I ------------------
09-25 16:13:43 I DIST CONFIG
09-25 16:13:43 I rank: 0
09-25 16:13:43 I local_rank: 0
09-25 16:13:43 I world_size: 4
09-25 16:13:43 I nodes: 1
09-25 16:13:43 I backend: nccl
09-25 16:13:43 I slurm job id: 141931
09-25 16:13:43 I hostnames: gpu-53
09-25 16:13:43 I ------------------
master_factory_base_path: vislstm
stage_name: in1k
datasets:
  train:
    kind: imagenet1k
    split: train
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: random_resized_crop
        size: 192
        scale:
        - 0.08
        - 1.0
        interpolation: bicubic
      - kind: random_horizontal_flip
      - kind: transforms.three_augment
        blur_sigma:
        - 0.1
        - 2.0
      - kind: color_jitter
        brightness: 0.3
        contrast: 0.3
        saturation: 0.3
        hue: 0.0
      - kind: imagenet1k_norm
    - kind: one_hot_wrapper
    collators:
    - kind: mix_collator
      mixup_alpha: 0.8
      cutmix_alpha: 1.0
      mixup_p: 0.5
      cutmix_p: 0.5
      apply_mode: batch
      lamb_mode: batch
      shuffle_mode: flip
  val:
    kind: imagenet1k
    split: val
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: resize
        size: 192
        interpolation: bicubic
      - kind: center_crop
        size: 192
      - kind: imagenet1k_norm
model:
  kind: models.single.vislstm
  patch_size: 16
  dim: 768
  depth: 24
  bidirectional: false
  alternation: bidirectional
  conv1d_kernel_size: 3
  use_conv2d: true
  bias: true
  pos_embed_mode: learnable
  drop_path_rate: 0.2
  drop_path_decay: false
  mode: classifier
  pooling:
    kind: bilateral
    aggregate: flatten
  optim:
    kind: adamw
    lr: 0.001
    betas:
    - 0.9
    - 0.999
    weight_decay: 0.05
    clip_grad_norm: 1.0
    schedule:
      kind: linear_warmup_cosine_decay_schedule
      warmup_epochs: 5
      end_value: 1.0e-06
    lr_scaler:
      kind: linear_lr_scaler
      divisor: 1024
trainer:
  kind: classification_trainer
  precision: bfloat16
  backup_precision: float16
  max_epochs: 400
  effective_batch_size: 512
  log_every_n_epochs: 1
  use_torch_compile: true
  callbacks:
  - kind: checkpoint_callback
  - kind: checkpoint_callback
    every_n_epochs: 10
    save_weights: false
    save_latest_weights: true
    save_latest_optim: true
  - kind: offline_accuracy_callback
    every_n_epochs: 1
    dataset_key: val
  initializer:
    kind: resume_initializer
    stage_id: ogqgazbl
    checkpoint: latest
09-25 16:13:43 I copied unresolved hp to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/hp_unresolved.yaml
09-25 16:13:43 I dumped resolved hp to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/hp_resolved.yaml
09-25 16:13:43 I ------------------
09-25 16:13:43 I training stage 'in1k'
09-25 16:13:43 I using different seeds per process (seed+rank)
09-25 16:13:43 I set seed to 0
09-25 16:13:43 I ------------------
09-25 16:13:43 I initializing datasets
09-25 16:13:43 I initializing train
fatal: No annotated tags can describe 'ec5acd7aa3ce77ad5724a951e54be1fb0f907e64'.
However, there were unannotated tags: try --tags.
09-25 16:13:43 I initialized process rank=2 local_rank=2 pid=3625722 hostname=gpu-53
09-25 16:13:48 I instantiating sample_wrapper x_transform_wrapper
09-25 16:13:48 I instantiating sample_wrapper one_hot_wrapper
09-25 16:13:48 I initializing val
09-25 16:13:49 I instantiating sample_wrapper x_transform_wrapper
09-25 16:13:49 I ------------------
09-25 16:13:49 I initializing trainer
09-25 16:13:49 I using precision: torch.bfloat16 (desired=bfloat16 backup=float16)
09-25 16:13:49 I main_sampler: DistributedSampler(num_repeats=1, shuffle=True)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
09-25 16:13:49 I loaded checkpoint from trainer_state_dict: {'epoch': 290, 'update': 725580, 'sample': 371496960, 'callback_state_dicts': [None, None, None]}
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
09-25 16:13:49 I ------------------
09-25 16:13:49 I creating model
09-25 16:13:49 I input_shape: (3, 192, 192)
09-25 16:13:49 I pos_embed.is_learnable=True
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
09-25 16:13:51 I drop_path_rate: 0.2
09-25 16:13:51 I model:
VisLSTM(
  (pooling): Bilateral(aggregate=flatten)
  (patch_embed): VitPatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_embed): VitPosEmbed2d()
  (xlstm): xLSTMBlockStack(
    (blocks): ModuleList(
      (0-23): 24 x mLSTMBlock(
        (drop_path1): DropPath(drop_prob=0.200)
        (xlstm_norm): LayerNorm()
        (xlstm): mLSTMLayer(
          (proj_up): Linear(in_features=768, out_features=3072, bias=True)
          (conv1d): SequenceConv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
          (conv_act_fn): SiLU()
          (mlstm_cell): mLSTMCell(
            (linear_i): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,), groups=1536, bias=False)
            (linear_f): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,), groups=1536, bias=False)
            (linear_h): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,), groups=1536, bias=False)
          )
          (ogate_act_fn): SiLU()
          (proj_down): Linear(in_features=1536, out_features=768, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layerscale): Identity()
        )
      )
    )
    (post_blocks_norm): LayerNorm()
  )
  (head): Sequential(
    (0): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
    (1): Linear(in_features=1536, out_features=1000, bias=True)
  )
)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
09-25 16:13:51 I vislstm initialize optimizer
09-25 16:13:51 I base lr: 1e-3
09-25 16:13:51 I scaled lr: 5e-4
09-25 16:13:51 I lr_scaler=LinearLrScaler(divisor=1024)
09-25 16:13:51 I lr_scale_factor=512
09-25 16:13:51 I exclude_bias_from_wd=True exclude_norm_from_wd=True param_group_modifiers=[WeightDecayByNameModifier(name=pos_embed.embed)]
09-25 16:13:51 I using 2 param groups:
09-25 16:13:51 I len(params)=146
09-25 16:13:51 I weight_decay=0.0 len(params)=151
09-25 16:13:51 I ------------------
09-25 16:13:51 I loading trainer/model state for resuming
09-25 16:13:51 I loading state from checkpoint ogqgazbl/in1k/latest
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
09-25 16:13:51 I loaded trainer checkpoint /home/beknur.kalmakhanbet/save/in1k/ogqgazbl/checkpoints/trainer cp=latest.th
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
09-25 16:13:52 I loaded weights of vislstm from /home/beknur.kalmakhanbet/save/in1k/ogqgazbl/checkpoints/vislstm cp=latest model.th
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
09-25 16:13:55 I loaded optimizer of vislstm from /home/beknur.kalmakhanbet/save/in1k/ogqgazbl/checkpoints/vislstm cp=latest optim.th
09-25 16:13:55 I added default DatasetStatsCallback
09-25 16:13:55 I added default ParamCountCallback
09-25 16:13:55 I added default CopyPreviousConfigCallback
09-25 16:13:55 I added default CopyPreviousSummaryCallback
09-25 16:13:55 I added default ProgressCallback(every_n_epochs=1)
09-25 16:13:55 I added default TrainTimeCallback(every_n_epochs=1)
09-25 16:13:55 I added default OnlineLossCallback(every_n_epochs=1)
09-25 16:13:55 I added default LrCallback(every_n_updates=50)
09-25 16:13:55 I added default FreezerCallback(every_n_updates=50)
09-25 16:13:55 I added default OnlineLossCallback(every_n_updates=50)
09-25 16:13:55 I replacing BatchNorm layers with SyncBatchNorm
09-25 16:13:55 I wrapping model with torch.compile
09-25 16:13:56 I ------------------
09-25 16:13:56 I PREPARE TRAINER
09-25 16:13:56 I calculating batch_size and accumulation_steps (effective_batch_size=512)
09-25 16:13:56 I torch.compile is used -> automatic batchsize not supported
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
09-25 16:13:56 I train_batches per epoch: 2502 (world_size=4 batch_size=128)
09-25 16:13:56 I initializing dataloader
09-25 16:13:56 I OfflineAccuracyCallback(every_n_epochs=1) registered InterleavedSamplerConfig(every_n_epochs=1) dataset_mode='x class'
09-25 16:13:56 I created dataloader (batch_size=128 num_workers=5 pin_memory=True total_cpu_count=16 prefetch_factor=2)
09-25 16:13:56 I concatenated dataset properties:
09-25 16:13:56 I - mode='index x class' len=1281167 root_dataset=<vislstm.datasets.imagenet1k.Imagenet1k object at 0x15275ba207f0>
09-25 16:13:56 I - mode='x class' len=50000 root_dataset=<vislstm.datasets.imagenet1k.Imagenet1k object at 0x1525b8f95dc0>
09-25 16:13:56 I ------------------
09-25 16:13:56 I BEFORE TRAINING
09-25 16:13:56 I train: 1281167 samples
09-25 16:13:56 I val: 50000 samples
09-25 16:13:56 I parameter counts (trainable | frozen)
09-25 16:13:56 I 87,822,568 | 0 | vislstm
09-25 16:13:56 I estimated checkpoint size: 1.0GB
09-25 16:13:56 I estimated weight checkpoint size: 351.2MB
09-25 16:13:56 I estimated optim checkpoint size: 702.5MB
09-25 16:13:56 I estimated size for 1 checkpoints: 351.2MB
09-25 16:13:56 I estimated checkpoint size: 1.0GB
09-25 16:13:56 I estimated weight checkpoint size: 351.2MB
09-25 16:13:56 I estimated optim checkpoint size: 702.5MB
09-25 16:13:56 I estimated size for 41 checkpoints: 0.0B
09-25 16:13:56 I ------------------
09-25 16:13:56 I DatasetStatsCallback
09-25 16:13:56 I ParamCountCallback
09-25 16:13:56 I CopyPreviousConfigCallback
09-25 16:13:56 I CopyPreviousSummaryCallback
09-25 16:13:56 I ProgressCallback(every_n_epochs=1)
09-25 16:13:56 I TrainTimeCallback(every_n_epochs=1)
09-25 16:13:56 I OnlineLossCallback(every_n_epochs=1)
09-25 16:13:56 I LrCallback(every_n_updates=50)
09-25 16:13:56 I FreezerCallback(every_n_updates=50)
09-25 16:13:56 I OnlineLossCallback(every_n_updates=50)
09-25 16:13:56 I OnlineAccuracyCallback(every_n_updates=50)
09-25 16:13:56 I OnlineAccuracyCallback(every_n_epochs=1)
09-25 16:13:56 I CheckpointCallback()
09-25 16:13:56 I CheckpointCallback(every_n_epochs=10)
09-25 16:13:56 I OfflineAccuracyCallback(every_n_epochs=1)
09-25 16:13:56 I ------------------
09-25 16:13:56 I START TRAINING
09-25 16:13:56 I initializing dataloader workers
09-25 16:13:57 I initialized dataloader workers
[rank1]:W0925 16:13:58.145220 3625721 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:W0925 16:13:58.178246 3625722 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank3]:W0925 16:13:58.268431 3625723 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W0925 16:13:58.306515 3625720 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
09-25 16:15:11 I 0 unused parameters
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
09-25 16:25:39 I ------------------
09-25 16:25:39 I Epoch 291/400 (E291_U728082_S372777984)
09-25 16:25:39 I ETA: 09.25 16.30.03 estimated_duration: 00:16:06.59 time_since_last_log: 00:11:43.19 time_per_update: 00:00:00.00 
09-25 16:25:39 I data=[0.00, 0.00, 0.00, 0.00] update=[0.28, 0.28, 0.28, 0.28]
09-25 16:25:39 I loss/online/main/E1: 2.8233251571655273
09-25 16:25:39 I loss/online/total/E1: 2.8233251571655273
09-25 16:25:39 I accuracy1/online/main/E1: 0.545167
09-25 16:26:04 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.25
09-25 16:26:04 I accuracy1/val/main: 0.762420
09-25 16:26:04 I loss/val/main: 0.97265625
09-25 16:36:32 I ------------------
09-25 16:36:32 I Epoch 292/400 (E292_U730584_S374059008)
09-25 16:36:32 I ETA: 09.25 16.40.34 estimated_duration: 00:14:54.54 time_since_last_log: 00:10:52.41 time_per_update: 00:00:00.26 
09-25 16:36:32 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 16:36:32 I loss/online/main/E1: 2.8025448322296143
09-25 16:36:32 I loss/online/total/E1: 2.8025448322296143
09-25 16:36:32 I accuracy1/online/main/E1: 0.550238
09-25 16:36:56 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 16:36:56 I accuracy1/val/main: 0.762160
09-25 16:36:56 I loss/val/main: 0.97265625
09-25 16:47:24 I ------------------
09-25 16:47:24 I Epoch 293/400 (E293_U733086_S375340032)
09-25 16:47:24 I ETA: 09.25 16.55.22 estimated_duration: 00:29:42.37 time_since_last_log: 00:10:51.98 time_per_update: 00:00:00.26 
09-25 16:47:24 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 16:47:24 I loss/online/main/E1: 2.796963691711426
09-25 16:47:24 I loss/online/total/E1: 2.796963691711426
09-25 16:47:24 I accuracy1/online/main/E1: 0.550449
09-25 16:47:48 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 16:47:48 I accuracy1/val/main: 0.763280
09-25 16:47:48 I loss/val/main: 0.9609375
09-25 16:58:16 I ------------------
09-25 16:58:16 I Epoch 294/400 (E294_U735588_S376621056)
09-25 16:58:16 I ETA: 09.25 17.10.04 estimated_duration: 00:44:24.41 time_since_last_log: 00:10:52.17 time_per_update: 00:00:00.26 
09-25 16:58:16 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 16:58:16 I loss/online/main/E1: 2.7797884941101074
09-25 16:58:16 I loss/online/total/E1: 2.7797884941101074
09-25 16:58:16 I accuracy1/online/main/E1: 0.553795
09-25 16:58:40 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 16:58:40 I accuracy1/val/main: 0.764900
09-25 16:58:40 I loss/val/main: 0.95703125
09-25 17:09:09 I ------------------
09-25 17:09:09 I Epoch 295/400 (E295_U738090_S377902080)
09-25 17:09:09 I ETA: 09.25 17.24.40 estimated_duration: 00:59:01.10 time_since_last_log: 00:10:52.65 time_per_update: 00:00:00.26 
09-25 17:09:09 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 17:09:09 I loss/online/main/E1: 2.779268503189087
09-25 17:09:09 I loss/online/total/E1: 2.779268503189087
09-25 17:09:09 I accuracy1/online/main/E1: 0.554359
09-25 17:09:32 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 17:09:33 I accuracy1/val/main: 0.766160
09-25 17:09:33 I loss/val/main: 0.953125
09-25 17:20:01 I ------------------
09-25 17:20:01 I Epoch 296/400 (E296_U740592_S379183104)
09-25 17:20:01 I ETA: 09.25 17.39.11 estimated_duration: 01:13:31.21 time_since_last_log: 00:10:52.18 time_per_update: 00:00:00.26 
09-25 17:20:01 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 17:20:01 I loss/online/main/E1: 2.7807493209838867
09-25 17:20:01 I loss/online/total/E1: 2.7807493209838867
09-25 17:20:01 I accuracy1/online/main/E1: 0.552945
09-25 17:20:25 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 17:20:25 I accuracy1/val/main: 0.766020
09-25 17:20:25 I loss/val/main: 0.95703125
09-25 17:30:53 I ------------------
09-25 17:30:53 I Epoch 297/400 (E297_U743094_S380464128)
09-25 17:30:53 I ETA: 09.25 17.53.35 estimated_duration: 01:27:55.53 time_since_last_log: 00:10:52.25 time_per_update: 00:00:00.26 
09-25 17:30:53 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 17:30:53 I loss/online/main/E1: 2.7716352939605713
09-25 17:30:53 I loss/online/total/E1: 2.7716352939605713
09-25 17:30:53 I accuracy1/online/main/E1: 0.554567
09-25 17:31:17 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 17:31:17 I accuracy1/val/main: 0.767600
09-25 17:31:17 I loss/val/main: 0.95703125
                                                                                                                                  09-25 17:41:46 I ------------------
09-25 17:41:46 I Epoch 298/400 (E298_U745596_S381745152)
09-25 17:41:46 I ETA: 09.25 18.07.54 estimated_duration: 01:42:14.71 time_since_last_log: 00:10:52.75 time_per_update: 00:00:00.26 
09-25 17:41:46 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 17:41:46 I loss/online/main/E1: 2.7690112590789795
09-25 17:41:46 I loss/online/total/E1: 2.7690112590789795
09-25 17:41:46 I accuracy1/online/main/E1: 0.554971
09-25 17:42:10 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 17:42:10 I accuracy1/val/main: 0.765380
09-25 17:42:10 I loss/val/main: 0.95703125
09-25 17:52:40 I ------------------
09-25 17:52:40 I Epoch 299/400 (E299_U748098_S383026176)
09-25 17:52:40 I ETA: 09.25 18.22.09 estimated_duration: 01:56:29.89 time_since_last_log: 00:10:54.08 time_per_update: 00:00:00.26 
09-25 17:52:40 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 17:52:40 I loss/online/main/E1: 2.7636799812316895
09-25 17:52:40 I loss/online/total/E1: 2.7636799812316895
09-25 17:52:40 I accuracy1/online/main/E1: 0.555360
09-25 17:53:04 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 17:53:04 I accuracy1/val/main: 0.767120
09-25 17:53:04 I loss/val/main: 0.953125
09-25 18:03:32 I ------------------
09-25 18:03:32 I Epoch 300/400 (E300_U750600_S384307200)
09-25 18:03:32 I ETA: 09.25 18.36.16 estimated_duration: 02:10:36.40 time_since_last_log: 00:10:51.86 time_per_update: 00:00:00.26 
09-25 18:03:32 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 18:03:32 I loss/online/main/E1: 2.7671022415161133
09-25 18:03:32 I loss/online/total/E1: 2.7671022415161133
09-25 18:03:32 I accuracy1/online/main/E1: 0.555932
09-25 18:03:33 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest model.th
09-25 18:03:34 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest optim.th
09-25 18:03:34 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/trainer cp=latest.th
09-25 18:03:58 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 18:03:58 I accuracy1/val/main: 0.767200
09-25 18:03:58 I loss/val/main: 0.94140625
09-25 18:14:27 I ------------------
09-25 18:14:27 I Epoch 301/400 (E301_U753102_S385588224)
09-25 18:14:27 I ETA: 09.25 18.50.21 estimated_duration: 02:24:41.59 time_since_last_log: 00:10:55.12 time_per_update: 00:00:00.26 
09-25 18:14:27 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 18:14:27 I loss/online/main/E1: 2.7460315227508545
09-25 18:14:27 I loss/online/total/E1: 2.7460315227508545
09-25 18:14:27 I accuracy1/online/main/E1: 0.560256
09-25 18:14:51 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 18:14:51 I accuracy1/val/main: 0.767160
09-25 18:14:51 I loss/val/main: 0.9609375
09-25 18:25:20 I ------------------
09-25 18:25:20 I Epoch 302/400 (E302_U755604_S386869248)
09-25 18:25:20 I ETA: 09.25 19.04.18 estimated_duration: 02:38:38.57 time_since_last_log: 00:10:53.16 time_per_update: 00:00:00.26 
09-25 18:25:20 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 18:25:20 I loss/online/main/E1: 2.74735164642334
09-25 18:25:20 I loss/online/total/E1: 2.74735164642334
09-25 18:25:20 I accuracy1/online/main/E1: 0.559513
09-25 18:25:44 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 18:25:44 I accuracy1/val/main: 0.767320
09-25 18:25:44 I loss/val/main: 0.953125
09-25 18:36:12 I ------------------
09-25 18:36:12 I Epoch 303/400 (E303_U758106_S388150272)
09-25 18:36:12 I ETA: 09.25 19.18.08 estimated_duration: 02:52:28.30 time_since_last_log: 00:10:51.87 time_per_update: 00:00:00.26 
09-25 18:36:12 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 18:36:12 I loss/online/main/E1: 2.7413735389709473
09-25 18:36:12 I loss/online/total/E1: 2.7413735389709473
09-25 18:36:12 I accuracy1/online/main/E1: 0.559078
09-25 18:36:36 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 18:36:36 I accuracy1/val/main: 0.767720
09-25 18:36:36 I loss/val/main: 0.95703125
09-25 18:47:04 I ------------------
09-25 18:47:04 I Epoch 304/400 (E304_U760608_S389431296)
09-25 18:47:04 I ETA: 09.25 19.31.52 estimated_duration: 03:06:12.37 time_since_last_log: 00:10:51.73 time_per_update: 00:00:00.26 
09-25 18:47:04 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 18:47:04 I loss/online/main/E1: 2.721891164779663
09-25 18:47:04 I loss/online/total/E1: 2.721891164779663
09-25 18:47:04 I accuracy1/online/main/E1: 0.561861
09-25 18:47:28 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 18:47:28 I accuracy1/val/main: 0.768560
09-25 18:47:28 I loss/val/main: 0.94140625
                                                                                                     09-25 18:57:57 I ------------------
09-25 18:57:57 I Epoch 305/400 (E305_U763110_S390712320)
09-25 18:57:57 I ETA: 09.25 19.45.33 estimated_duration: 03:19:53.49 time_since_last_log: 00:10:53.61 time_per_update: 00:00:00.26 
09-25 18:57:57 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 18:57:57 I loss/online/main/E1: 2.7286667823791504
09-25 18:57:57 I loss/online/total/E1: 2.7286667823791504
09-25 18:57:57 I accuracy1/online/main/E1: 0.561623
09-25 18:58:21 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 18:58:21 I accuracy1/val/main: 0.770660
09-25 18:58:21 I loss/val/main: 0.94921875
09-25 19:08:49 I ------------------
09-25 19:08:49 I Epoch 306/400 (E306_U765612_S391993344)
09-25 19:08:49 I ETA: 09.25 19.59.07 estimated_duration: 03:33:27.36 time_since_last_log: 00:10:52.18 time_per_update: 00:00:00.26 
09-25 19:08:49 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 19:08:49 I loss/online/main/E1: 2.7218780517578125
09-25 19:08:49 I loss/online/total/E1: 2.7218780517578125
09-25 19:08:49 I accuracy1/online/main/E1: 0.564192
09-25 19:09:13 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 19:09:13 I accuracy1/val/main: 0.769320
09-25 19:09:13 I loss/val/main: 0.94140625
09-25 19:19:41 I ------------------
09-25 19:19:41 I Epoch 307/400 (E307_U768114_S393274368)
09-25 19:19:41 I ETA: 09.25 20.12.35 estimated_duration: 03:46:55.28 time_since_last_log: 00:10:51.70 time_per_update: 00:00:00.26 
09-25 19:19:41 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 19:19:41 I loss/online/main/E1: 2.725302219390869
09-25 19:19:41 I loss/online/total/E1: 2.725302219390869
09-25 19:19:41 I accuracy1/online/main/E1: 0.562465
09-25 19:20:05 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 19:20:05 I accuracy1/val/main: 0.770140
09-25 19:20:05 I loss/val/main: 0.9296875
09-25 19:30:33 I ------------------
09-25 19:30:33 I Epoch 308/400 (E308_U770616_S394555392)
09-25 19:30:33 I ETA: 09.25 20.25.58 estimated_duration: 04:00:18.26 time_since_last_log: 00:10:51.95 time_per_update: 00:00:00.26 
09-25 19:30:33 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 19:30:33 I loss/online/main/E1: 2.7047359943389893
09-25 19:30:33 I loss/online/total/E1: 2.7047359943389893
09-25 19:30:33 I accuracy1/online/main/E1: 0.566520
09-25 19:30:57 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 19:30:57 I accuracy1/val/main: 0.771060
09-25 19:30:57 I loss/val/main: 0.94140625
09-25 19:41:26 I ------------------
09-25 19:41:26 I Epoch 309/400 (E309_U773118_S395836416)
09-25 19:41:26 I ETA: 09.25 20.39.17 estimated_duration: 04:13:37.23 time_since_last_log: 00:10:52.88 time_per_update: 00:00:00.26 
09-25 19:41:26 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 19:41:26 I loss/online/main/E1: 2.7136030197143555
09-25 19:41:26 I loss/online/total/E1: 2.7136030197143555
09-25 19:41:26 I accuracy1/online/main/E1: 0.563797
09-25 19:41:50 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 19:41:50 I accuracy1/val/main: 0.770680
09-25 19:41:50 I loss/val/main: 0.9296875
09-25 19:52:19 I ------------------
09-25 19:52:19 I Epoch 310/400 (E310_U775620_S397117440)
09-25 19:52:19 I ETA: 09.25 20.52.30 estimated_duration: 04:26:51.13 time_since_last_log: 00:10:52.96 time_per_update: 00:00:00.26 
09-25 19:52:19 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 19:52:19 I loss/online/main/E1: 2.6971123218536377
09-25 19:52:19 I loss/online/total/E1: 2.6971123218536377
09-25 19:52:19 I accuracy1/online/main/E1: 0.567940
09-25 19:52:20 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest model.th
09-25 19:52:21 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest optim.th
09-25 19:52:21 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/trainer cp=latest.th
09-25 19:52:45 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 19:52:45 I accuracy1/val/main: 0.773040
09-25 19:52:45 I loss/val/main: 0.921875
09-25 20:03:14 I ------------------
09-25 20:03:14 I Epoch 311/400 (E311_U778122_S398398464)
09-25 20:03:14 I ETA: 09.25 21.05.42 estimated_duration: 04:40:02.83 time_since_last_log: 00:10:55.23 time_per_update: 00:00:00.26 
09-25 20:03:14 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 20:03:14 I loss/online/main/E1: 2.689639091491699
09-25 20:03:14 I loss/online/total/E1: 2.689639091491699
09-25 20:03:14 I accuracy1/online/main/E1: 0.567227
09-25 20:03:38 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 20:03:38 I accuracy1/val/main: 0.773900
09-25 20:03:38 I loss/val/main: 0.9296875
09-25 20:14:08 I ------------------
09-25 20:14:08 I Epoch 312/400 (E312_U780624_S399679488)
09-25 20:14:08 I ETA: 09.25 21.18.47 estimated_duration: 04:53:07.19 time_since_last_log: 00:10:53.48 time_per_update: 00:00:00.26 
09-25 20:14:08 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 20:14:08 I loss/online/main/E1: 2.6778481006622314
09-25 20:14:08 I loss/online/total/E1: 2.6778481006622314
09-25 20:14:08 I accuracy1/online/main/E1: 0.570418
09-25 20:14:32 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 20:14:32 I accuracy1/val/main: 0.772840
09-25 20:14:32 I loss/val/main: 0.9296875
09-25 20:25:00 I ------------------
09-25 20:25:00 I Epoch 313/400 (E313_U783126_S400960512)
09-25 20:25:00 I ETA: 09.25 21.31.45 estimated_duration: 05:06:05.57 time_since_last_log: 00:10:52.73 time_per_update: 00:00:00.26 
09-25 20:25:00 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 20:25:00 I loss/online/main/E1: 2.6725778579711914
09-25 20:25:00 I loss/online/total/E1: 2.6725778579711914
09-25 20:25:00 I accuracy1/online/main/E1: 0.571297
09-25 20:25:24 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 20:25:24 I accuracy1/val/main: 0.773160
09-25 20:25:24 I loss/val/main: 0.92578125
09-25 20:35:53 I ------------------
09-25 20:35:53 I Epoch 314/400 (E314_U785628_S402241536)
09-25 20:35:53 I ETA: 09.25 21.44.38 estimated_duration: 05:18:59.09 time_since_last_log: 00:10:52.82 time_per_update: 00:00:00.26 
09-25 20:35:53 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 20:35:53 I loss/online/main/E1: 2.683446168899536
09-25 20:35:53 I loss/online/total/E1: 2.683446168899536
09-25 20:35:53 I accuracy1/online/main/E1: 0.570722
09-25 20:36:17 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 20:36:17 I accuracy1/val/main: 0.772120
09-25 20:36:17 I loss/val/main: 0.93359375
09-25 20:46:49 I ------------------
09-25 20:46:49 I Epoch 315/400 (E315_U788130_S403522560)
09-25 20:46:49 I ETA: 09.25 21.57.30 estimated_duration: 05:31:50.92 time_since_last_log: 00:10:55.37 time_per_update: 00:00:00.26 
09-25 20:46:49 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 20:46:49 I loss/online/main/E1: 2.6774179935455322
09-25 20:46:49 I loss/online/total/E1: 2.6774179935455322
09-25 20:46:49 I accuracy1/online/main/E1: 0.571066
09-25 20:47:13 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 20:47:13 I accuracy1/val/main: 0.775580
09-25 20:47:13 I loss/val/main: 0.90625
09-25 20:57:42 I ------------------
09-25 20:57:42 I Epoch 316/400 (E316_U790632_S404803584)
09-25 20:57:42 I ETA: 09.25 22.10.15 estimated_duration: 05:44:35.65 time_since_last_log: 00:10:53.63 time_per_update: 00:00:00.26 
09-25 20:57:42 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 20:57:42 I loss/online/main/E1: 2.6665689945220947
09-25 20:57:42 I loss/online/total/E1: 2.6665689945220947
09-25 20:57:42 I accuracy1/online/main/E1: 0.570640
09-25 20:58:06 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 20:58:06 I accuracy1/val/main: 0.774340
09-25 20:58:06 I loss/val/main: 0.91796875
09-25 21:08:36 I ------------------
09-25 21:08:36 I Epoch 317/400 (E317_U793134_S406084608)
09-25 21:08:36 I ETA: 09.25 22.22.55 estimated_duration: 05:57:15.29 time_since_last_log: 00:10:53.43 time_per_update: 00:00:00.26 
09-25 21:08:36 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 21:08:36 I loss/online/main/E1: 2.6487317085266113
09-25 21:08:36 I loss/online/total/E1: 2.6487317085266113
09-25 21:08:36 I accuracy1/online/main/E1: 0.574438
09-25 21:09:00 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 21:09:00 I accuracy1/val/main: 0.774020
09-25 21:09:00 I loss/val/main: 0.9140625
09-25 21:19:27 I ------------------
09-25 21:19:27 I Epoch 318/400 (E318_U795636_S407365632)
09-25 21:19:27 I ETA: 09.25 22.35.27 estimated_duration: 06:09:47.54 time_since_last_log: 00:10:51.37 time_per_update: 00:00:00.26 
09-25 21:19:27 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 21:19:27 I loss/online/main/E1: 2.642880916595459
09-25 21:19:27 I loss/online/total/E1: 2.642880916595459
09-25 21:19:27 I accuracy1/online/main/E1: 0.576705
09-25 21:19:51 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 21:19:51 I accuracy1/val/main: 0.776340
09-25 21:19:51 I loss/val/main: 0.91796875
09-25 21:30:21 I ------------------
09-25 21:30:21 I Epoch 319/400 (E319_U798138_S408646656)
09-25 21:30:21 I ETA: 09.25 22.47.57 estimated_duration: 06:22:18.05 time_since_last_log: 00:10:53.75 time_per_update: 00:00:00.26 
09-25 21:30:21 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 21:30:21 I loss/online/main/E1: 2.6499321460723877
09-25 21:30:21 I loss/online/total/E1: 2.6499321460723877
09-25 21:30:21 I accuracy1/online/main/E1: 0.574924
09-25 21:30:45 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 21:30:45 I accuracy1/val/main: 0.775140
09-25 21:30:45 I loss/val/main: 0.921875
09-25 21:41:14 I ------------------
09-25 21:41:14 I Epoch 320/400 (E320_U800640_S409927680)
09-25 21:41:14 I ETA: 09.25 23.00.23 estimated_duration: 06:34:43.75 time_since_last_log: 00:10:53.67 time_per_update: 00:00:00.26 
09-25 21:41:14 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 21:41:14 I loss/online/main/E1: 2.635362148284912
09-25 21:41:14 I loss/online/total/E1: 2.635362148284912
09-25 21:41:14 I accuracy1/online/main/E1: 0.576986
09-25 21:41:15 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest model.th
09-25 21:41:17 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest optim.th
09-25 21:41:17 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/trainer cp=latest.th
09-25 21:41:41 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 21:41:41 I accuracy1/val/main: 0.776780
09-25 21:41:41 I loss/val/main: 0.91796875
09-25 21:52:08 I ------------------
09-25 21:52:08 I Epoch 321/400 (E321_U803142_S411208704)
09-25 21:52:08 I ETA: 09.25 23.12.44 estimated_duration: 06:47:04.49 time_since_last_log: 00:10:53.42 time_per_update: 00:00:00.26 
09-25 21:52:08 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 21:52:08 I loss/online/main/E1: 2.6413724422454834
09-25 21:52:08 I loss/online/total/E1: 2.6413724422454834
09-25 21:52:08 I accuracy1/online/main/E1: 0.576785
09-25 21:52:32 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 21:52:32 I accuracy1/val/main: 0.775780
09-25 21:52:32 I loss/val/main: 0.91796875
09-25 22:03:01 I ------------------
09-25 22:03:01 I Epoch 322/400 (E322_U805644_S412489728)
09-25 22:03:01 I ETA: 09.25 23.24.59 estimated_duration: 06:59:19.72 time_since_last_log: 00:10:52.71 time_per_update: 00:00:00.26 
09-25 22:03:01 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 22:03:01 I loss/online/main/E1: 2.6348462104797363
09-25 22:03:01 I loss/online/total/E1: 2.6348462104797363
09-25 22:03:01 I accuracy1/online/main/E1: 0.576956
09-25 22:03:25 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 22:03:25 I accuracy1/val/main: 0.778060
09-25 22:03:25 I loss/val/main: 0.90625
09-25 22:13:53 I ------------------
09-25 22:13:53 I Epoch 323/400 (E323_U808146_S413770752)
09-25 22:13:53 I ETA: 09.25 23.37.10 estimated_duration: 07:11:30.52 time_since_last_log: 00:10:52.82 time_per_update: 00:00:00.26 
09-25 22:13:53 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 22:13:53 I loss/online/main/E1: 2.6350533962249756
09-25 22:13:53 I loss/online/total/E1: 2.6350533962249756
09-25 22:13:53 I accuracy1/online/main/E1: 0.576517
09-25 22:14:17 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 22:14:17 I accuracy1/val/main: 0.776840
09-25 22:14:17 I loss/val/main: 0.91015625
09-25 22:24:47 I ------------------
09-25 22:24:47 I Epoch 324/400 (E324_U810648_S415051776)
09-25 22:24:47 I ETA: 09.25 23.49.17 estimated_duration: 07:23:37.47 time_since_last_log: 00:10:53.37 time_per_update: 00:00:00.26 
09-25 22:24:47 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 22:24:47 I loss/online/main/E1: 2.6085574626922607
09-25 22:24:47 I loss/online/total/E1: 2.6085574626922607
09-25 22:24:47 I accuracy1/online/main/E1: 0.580925
09-25 22:25:11 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 22:25:11 I accuracy1/val/main: 0.776220
09-25 22:25:11 I loss/val/main: 0.90625
09-25 22:35:40 I ------------------
09-25 22:35:40 I Epoch 325/400 (E325_U813150_S416332800)
09-25 22:35:40 I ETA: 09.26 00.01.19 estimated_duration: 07:35:39.59 time_since_last_log: 00:10:53.08 time_per_update: 00:00:00.26 
09-25 22:35:40 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 22:35:40 I loss/online/main/E1: 2.6073174476623535
09-25 22:35:40 I loss/online/total/E1: 2.6073174476623535
09-25 22:35:40 I accuracy1/online/main/E1: 0.582575
09-25 22:36:04 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 22:36:04 I accuracy1/val/main: 0.779420
09-25 22:36:04 I loss/val/main: 0.8984375
09-25 22:46:34 I ------------------
09-25 22:46:34 I Epoch 326/400 (E326_U815652_S417613824)
09-25 22:46:34 I ETA: 09.26 00.13.18 estimated_duration: 07:47:38.61 time_since_last_log: 00:10:54.19 time_per_update: 00:00:00.26 
09-25 22:46:34 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 22:46:34 I loss/online/main/E1: 2.6161551475524902
09-25 22:46:34 I loss/online/total/E1: 2.6161551475524902
09-25 22:46:34 I accuracy1/online/main/E1: 0.578671
09-25 22:46:58 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 22:46:58 I accuracy1/val/main: 0.778660
09-25 22:46:58 I loss/val/main: 0.90234375
09-25 22:57:26 I ------------------
09-25 22:57:26 I Epoch 327/400 (E327_U818154_S418894848)
09-25 22:57:26 I ETA: 09.26 00.25.10 estimated_duration: 07:59:30.90 time_since_last_log: 00:10:52.29 time_per_update: 00:00:00.26 
09-25 22:57:26 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 22:57:26 I loss/online/main/E1: 2.5965516567230225
09-25 22:57:26 I loss/online/total/E1: 2.5965516567230225
09-25 22:57:26 I accuracy1/online/main/E1: 0.584383
09-25 22:57:50 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 22:57:50 I accuracy1/val/main: 0.778440
09-25 22:57:50 I loss/val/main: 0.8984375
09-25 23:08:18 I ------------------
09-25 23:08:18 I Epoch 328/400 (E328_U820656_S420175872)
09-25 23:08:18 I ETA: 09.26 00.36.57 estimated_duration: 08:11:18.14 time_since_last_log: 00:10:51.72 time_per_update: 00:00:00.26 
09-25 23:08:18 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 23:08:18 I loss/online/main/E1: 2.599114179611206
09-25 23:08:18 I loss/online/total/E1: 2.599114179611206
09-25 23:08:18 I accuracy1/online/main/E1: 0.583209
09-25 23:08:42 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 23:08:42 I accuracy1/val/main: 0.780900
09-25 23:08:42 I loss/val/main: 0.89453125
09-25 23:19:11 I ------------------
09-25 23:19:11 I Epoch 329/400 (E329_U823158_S421456896)
09-25 23:19:11 I ETA: 09.26 00.48.42 estimated_duration: 08:23:03.11 time_since_last_log: 00:10:53.40 time_per_update: 00:00:00.26 
09-25 23:19:11 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 23:19:11 I loss/online/main/E1: 2.599123239517212
09-25 23:19:12 I loss/online/total/E1: 2.599123239517212
09-25 23:19:12 I accuracy1/online/main/E1: 0.582994
09-25 23:19:35 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 23:19:36 I accuracy1/val/main: 0.778920
09-25 23:19:36 I loss/val/main: 0.90234375
09-25 23:30:05 I ------------------
09-25 23:30:05 I Epoch 330/400 (E330_U825660_S422737920)
09-25 23:30:05 I ETA: 09.26 01.00.23 estimated_duration: 08:34:43.91 time_since_last_log: 00:10:53.50 time_per_update: 00:00:00.26 
09-25 23:30:05 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 23:30:05 I loss/online/main/E1: 2.598006010055542
09-25 23:30:05 I loss/online/total/E1: 2.598006010055542
09-25 23:30:05 I accuracy1/online/main/E1: 0.583796
09-25 23:30:06 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest model.th
09-25 23:30:07 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest optim.th
09-25 23:30:07 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/trainer cp=latest.th
09-25 23:30:31 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 23:30:31 I accuracy1/val/main: 0.781160
09-25 23:30:31 I loss/val/main: 0.89453125
09-25 23:41:00 I ------------------
09-25 23:41:00 I Epoch 331/400 (E331_U828162_S424018944)
09-25 23:41:00 I ETA: 09.26 01.12.02 estimated_duration: 08:46:22.50 time_since_last_log: 00:10:55.18 time_per_update: 00:00:00.26 
09-25 23:41:00 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 23:41:00 I loss/online/main/E1: 2.5821566581726074
09-25 23:41:00 I loss/online/total/E1: 2.5821566581726074
09-25 23:41:00 I accuracy1/online/main/E1: 0.585350
09-25 23:41:24 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 23:41:24 I accuracy1/val/main: 0.780500
09-25 23:41:24 I loss/val/main: 0.890625
09-25 23:51:53 I ------------------
09-25 23:51:53 I Epoch 332/400 (E332_U830664_S425299968)
09-25 23:51:53 I ETA: 09.26 01.23.34 estimated_duration: 08:57:54.49 time_since_last_log: 00:10:53.21 time_per_update: 00:00:00.26 
09-25 23:51:53 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-25 23:51:53 I loss/online/main/E1: 2.576943874359131
09-25 23:51:53 I loss/online/total/E1: 2.576943874359131
09-25 23:51:53 I accuracy1/online/main/E1: 0.587766
09-25 23:52:17 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-25 23:52:17 I accuracy1/val/main: 0.782000
09-25 23:52:17 I loss/val/main: 0.89453125
09-26 00:02:47 I ------------------
09-26 00:02:47 I Epoch 333/400 (E333_U833166_S426580992)
09-26 00:02:47 I ETA: 09.26 01.35.02 estimated_duration: 09:09:22.20 time_since_last_log: 00:10:53.11 time_per_update: 00:00:00.26 
09-26 00:02:47 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 00:02:47 I loss/online/main/E1: 2.5741522312164307
09-26 00:02:47 I loss/online/total/E1: 2.5741522312164307
09-26 00:02:47 I accuracy1/online/main/E1: 0.587993
09-26 00:03:10 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 00:03:11 I accuracy1/val/main: 0.782680
09-26 00:03:11 I loss/val/main: 0.89453125
09-26 00:13:40 I ------------------
09-26 00:13:40 I Epoch 334/400 (E334_U835668_S427862016)
09-26 00:13:40 I ETA: 09.26 01.46.26 estimated_duration: 09:20:46.19 time_since_last_log: 00:10:53.45 time_per_update: 00:00:00.26 
09-26 00:13:40 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 00:13:40 I loss/online/main/E1: 2.563159942626953
09-26 00:13:40 I loss/online/total/E1: 2.563159942626953
09-26 00:13:40 I accuracy1/online/main/E1: 0.589856
09-26 00:14:04 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 00:14:04 I accuracy1/val/main: 0.783720
09-26 00:14:04 I loss/val/main: 0.8828125
09-26 00:24:34 I ------------------
09-26 00:24:34 I Epoch 335/400 (E335_U838170_S429143040)
09-26 00:24:34 I ETA: 09.26 01.57.46 estimated_duration: 09:32:06.21 time_since_last_log: 00:10:53.56 time_per_update: 00:00:00.26 
09-26 00:24:34 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 00:24:34 I loss/online/main/E1: 2.5599629878997803
09-26 00:24:34 I loss/online/total/E1: 2.5599629878997803
09-26 00:24:34 I accuracy1/online/main/E1: 0.589343
09-26 00:24:57 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 00:24:58 I accuracy1/val/main: 0.784120
09-26 00:24:58 I loss/val/main: 0.890625
09-26 00:35:27 I ------------------
09-26 00:35:27 I Epoch 336/400 (E336_U840672_S430424064)
09-26 00:35:27 I ETA: 09.26 02.09.01 estimated_duration: 09:43:21.82 time_since_last_log: 00:10:53.27 time_per_update: 00:00:00.26 
09-26 00:35:27 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 00:35:27 I loss/online/main/E1: 2.562166452407837
09-26 00:35:27 I loss/online/total/E1: 2.562166452407837
09-26 00:35:27 I accuracy1/online/main/E1: 0.588033
09-26 00:35:51 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 00:35:51 I accuracy1/val/main: 0.784660
09-26 00:35:51 I loss/val/main: 0.88671875
09-26 00:46:20 I ------------------
09-26 00:46:20 I Epoch 337/400 (E337_U843174_S431705088)
09-26 00:46:20 I ETA: 09.26 02.20.13 estimated_duration: 09:54:33.26 time_since_last_log: 00:10:53.14 time_per_update: 00:00:00.26 
09-26 00:46:20 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 00:46:20 I loss/online/main/E1: 2.56668758392334
09-26 00:46:20 I loss/online/total/E1: 2.56668758392334
09-26 00:46:20 I accuracy1/online/main/E1: 0.588548
09-26 00:46:44 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 00:46:44 I accuracy1/val/main: 0.783960
09-26 00:46:44 I loss/val/main: 0.88671875
09-26 00:57:14 I ------------------
09-26 00:57:14 I Epoch 338/400 (E338_U845676_S432986112)
09-26 00:57:14 I ETA: 09.26 02.31.21 estimated_duration: 10:05:41.46 time_since_last_log: 00:10:53.77 time_per_update: 00:00:00.26 
09-26 00:57:14 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 00:57:14 I loss/online/main/E1: 2.5486488342285156
09-26 00:57:14 I loss/online/total/E1: 2.5486488342285156
09-26 00:57:14 I accuracy1/online/main/E1: 0.591070
09-26 00:57:38 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 00:57:38 I accuracy1/val/main: 0.785680
09-26 00:57:38 I loss/val/main: 0.8828125
09-26 01:08:06 I ------------------
09-26 01:08:06 I Epoch 339/400 (E339_U848178_S434267136)
09-26 01:08:06 I ETA: 09.26 02.42.23 estimated_duration: 10:16:43.87 time_since_last_log: 00:10:52.21 time_per_update: 00:00:00.26 
09-26 01:08:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 01:08:06 I loss/online/main/E1: 2.549062490463257
09-26 01:08:06 I loss/online/total/E1: 2.549062490463257
09-26 01:08:06 I accuracy1/online/main/E1: 0.591008
09-26 01:08:30 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 01:08:30 I accuracy1/val/main: 0.784140
09-26 01:08:30 I loss/val/main: 0.87890625
09-26 01:18:59 I ------------------
09-26 01:18:59 I Epoch 340/400 (E340_U850680_S435548160)
09-26 01:18:59 I ETA: 09.26 02.53.23 estimated_duration: 10:27:43.50 time_since_last_log: 00:10:53.18 time_per_update: 00:00:00.26 
09-26 01:18:59 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 01:18:59 I loss/online/main/E1: 2.532992362976074
09-26 01:18:59 I loss/online/total/E1: 2.532992362976074
09-26 01:18:59 I accuracy1/online/main/E1: 0.594959
09-26 01:19:00 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest model.th
09-26 01:19:02 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest optim.th
09-26 01:19:02 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/trainer cp=latest.th
09-26 01:19:25 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 01:19:26 I accuracy1/val/main: 0.784260
09-26 01:19:26 I loss/val/main: 0.8828125
09-26 01:29:56 I ------------------
09-26 01:29:56 I Epoch 341/400 (E341_U853182_S436829184)
09-26 01:29:56 I ETA: 09.26 03.04.23 estimated_duration: 10:38:43.36 time_since_last_log: 00:10:56.68 time_per_update: 00:00:00.26 
09-26 01:29:56 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 01:29:56 I loss/online/main/E1: 2.5343844890594482
09-26 01:29:56 I loss/online/total/E1: 2.5343844890594482
09-26 01:29:56 I accuracy1/online/main/E1: 0.595154
09-26 01:30:20 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 01:30:20 I accuracy1/val/main: 0.784920
09-26 01:30:20 I loss/val/main: 0.87890625
09-26 01:40:48 I ------------------
09-26 01:40:48 I Epoch 342/400 (E342_U855684_S438110208)
09-26 01:40:48 I ETA: 09.26 03.15.14 estimated_duration: 10:49:34.39 time_since_last_log: 00:10:52.44 time_per_update: 00:00:00.26 
09-26 01:40:48 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 01:40:48 I loss/online/main/E1: 2.5361270904541016
09-26 01:40:48 I loss/online/total/E1: 2.5361270904541016
09-26 01:40:48 I accuracy1/online/main/E1: 0.593649
09-26 01:41:12 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 01:41:12 I accuracy1/val/main: 0.786660
09-26 01:41:12 I loss/val/main: 0.875
09-26 01:51:41 I ------------------
09-26 01:51:41 I Epoch 343/400 (E343_U858186_S439391232)
09-26 01:51:41 I ETA: 09.26 03.26.02 estimated_duration: 11:00:22.54 time_since_last_log: 00:10:53.23 time_per_update: 00:00:00.26 
09-26 01:51:41 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 01:51:41 I loss/online/main/E1: 2.529397964477539
09-26 01:51:41 I loss/online/total/E1: 2.529397964477539
09-26 01:51:42 I accuracy1/online/main/E1: 0.594783
09-26 01:52:05 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 01:52:06 I accuracy1/val/main: 0.786200
09-26 01:52:06 I loss/val/main: 0.87890625
09-26 02:02:35 I ------------------
09-26 02:02:35 I Epoch 344/400 (E344_U860688_S440672256)
09-26 02:02:35 I ETA: 09.26 03.36.47 estimated_duration: 11:11:07.53 time_since_last_log: 00:10:53.76 time_per_update: 00:00:00.26 
09-26 02:02:35 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 02:02:35 I loss/online/main/E1: 2.5303163528442383
09-26 02:02:35 I loss/online/total/E1: 2.5303163528442383
09-26 02:02:35 I accuracy1/online/main/E1: 0.594471
09-26 02:02:59 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 02:02:59 I accuracy1/val/main: 0.786640
09-26 02:02:59 I loss/val/main: 0.8671875
09-26 02:13:29 I ------------------
09-26 02:13:29 I Epoch 345/400 (E345_U863190_S441953280)
09-26 02:13:29 I ETA: 09.26 03.47.28 estimated_duration: 11:21:48.44 time_since_last_log: 00:10:53.48 time_per_update: 00:00:00.26 
09-26 02:13:29 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 02:13:29 I loss/online/main/E1: 2.5204715728759766
09-26 02:13:29 I loss/online/total/E1: 2.5204715728759766
09-26 02:13:29 I accuracy1/online/main/E1: 0.596740
09-26 02:13:53 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 02:13:53 I accuracy1/val/main: 0.786100
09-26 02:13:53 I loss/val/main: 0.87109375
09-26 02:24:20 I ------------------
09-26 02:24:20 I Epoch 346/400 (E346_U865692_S443234304)
09-26 02:24:20 I ETA: 09.26 03.58.02 estimated_duration: 11:32:22.57 time_since_last_log: 00:10:50.83 time_per_update: 00:00:00.26 
09-26 02:24:20 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 02:24:20 I loss/online/main/E1: 2.5205607414245605
09-26 02:24:20 I loss/online/total/E1: 2.5205607414245605
09-26 02:24:20 I accuracy1/online/main/E1: 0.595223
09-26 02:24:44 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 02:24:44 I accuracy1/val/main: 0.787080
09-26 02:24:44 I loss/val/main: 0.875
09-26 02:35:13 I ------------------
09-26 02:35:13 I Epoch 347/400 (E347_U868194_S444515328)
09-26 02:35:13 I ETA: 09.26 04.08.35 estimated_duration: 11:42:55.51 time_since_last_log: 00:10:52.98 time_per_update: 00:00:00.26 
09-26 02:35:13 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 02:35:13 I loss/online/main/E1: 2.4962754249572754
09-26 02:35:13 I loss/online/total/E1: 2.4962754249572754
09-26 02:35:13 I accuracy1/online/main/E1: 0.600445
09-26 02:35:37 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 02:35:37 I accuracy1/val/main: 0.787200
09-26 02:35:37 I loss/val/main: 0.87109375
09-26 02:46:06 I ------------------
09-26 02:46:06 I Epoch 348/400 (E348_U870696_S445796352)
09-26 02:46:06 I ETA: 09.26 04.19.05 estimated_duration: 11:53:25.48 time_since_last_log: 00:10:53.56 time_per_update: 00:00:00.26 
09-26 02:46:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 02:46:06 I loss/online/main/E1: 2.495725393295288
09-26 02:46:06 I loss/online/total/E1: 2.495725393295288
09-26 02:46:06 I accuracy1/online/main/E1: 0.600652
09-26 02:46:30 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 02:46:30 I accuracy1/val/main: 0.786860
09-26 02:46:30 I loss/val/main: 0.86328125
09-26 02:56:57 I ------------------
09-26 02:56:57 I Epoch 349/400 (E349_U873198_S447077376)
09-26 02:56:57 I ETA: 09.26 04.29.29 estimated_duration: 12:03:49.30 time_since_last_log: 00:10:51.36 time_per_update: 00:00:00.26 
09-26 02:56:57 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 02:56:58 I loss/online/main/E1: 2.4972357749938965
09-26 02:56:58 I loss/online/total/E1: 2.4972357749938965
09-26 02:56:58 I accuracy1/online/main/E1: 0.599777
09-26 02:57:21 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 02:57:22 I accuracy1/val/main: 0.786980
09-26 02:57:22 I loss/val/main: 0.86328125
09-26 03:07:49 I ------------------
09-26 03:07:49 I Epoch 350/400 (E350_U875700_S448358400)
09-26 03:07:49 I ETA: 09.26 04.39.49 estimated_duration: 12:14:09.75 time_since_last_log: 00:10:51.54 time_per_update: 00:00:00.26 
09-26 03:07:49 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 03:07:49 I loss/online/main/E1: 2.503758668899536
09-26 03:07:49 I loss/online/total/E1: 2.503758668899536
09-26 03:07:49 I accuracy1/online/main/E1: 0.598543
09-26 03:07:50 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest model.th
09-26 03:07:51 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest optim.th
09-26 03:07:51 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/trainer cp=latest.th
09-26 03:08:15 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 03:08:15 I accuracy1/val/main: 0.788220
09-26 03:08:15 I loss/val/main: 0.86328125
09-26 03:18:44 I ------------------
09-26 03:18:44 I Epoch 351/400 (E351_U878202_S449639424)
09-26 03:18:44 I ETA: 09.26 04.50.09 estimated_duration: 12:24:30.17 time_since_last_log: 00:10:54.62 time_per_update: 00:00:00.26 
09-26 03:18:44 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 03:18:44 I loss/online/main/E1: 2.489978313446045
09-26 03:18:44 I loss/online/total/E1: 2.489978313446045
09-26 03:18:44 I accuracy1/online/main/E1: 0.602235
09-26 03:19:08 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 03:19:08 I accuracy1/val/main: 0.786740
09-26 03:19:08 I loss/val/main: 0.86328125
09-26 03:29:38 I ------------------
09-26 03:29:38 I Epoch 352/400 (E352_U880704_S450920448)
09-26 03:29:38 I ETA: 09.26 05.00.26 estimated_duration: 12:34:46.38 time_since_last_log: 00:10:54.03 time_per_update: 00:00:00.26 
09-26 03:29:38 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 03:29:38 I loss/online/main/E1: 2.491736650466919
09-26 03:29:38 I loss/online/total/E1: 2.491736650466919
09-26 03:29:38 I accuracy1/online/main/E1: 0.600303
09-26 03:30:02 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 03:30:02 I accuracy1/val/main: 0.788440
09-26 03:30:02 I loss/val/main: 0.859375
09-26 03:40:28 I ------------------
09-26 03:40:28 I Epoch 353/400 (E353_U883206_S452201472)
09-26 03:40:28 I ETA: 09.26 05.10.35 estimated_duration: 12:44:55.28 time_since_last_log: 00:10:50.67 time_per_update: 00:00:00.26 
09-26 03:40:28 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 03:40:28 I loss/online/main/E1: 2.4813289642333984
09-26 03:40:28 I loss/online/total/E1: 2.4813289642333984
09-26 03:40:28 I accuracy1/online/main/E1: 0.602920
09-26 03:40:52 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 03:40:52 I accuracy1/val/main: 0.788420
09-26 03:40:52 I loss/val/main: 0.859375
09-26 03:51:23 I ------------------
09-26 03:51:23 I Epoch 354/400 (E354_U885708_S453482496)
09-26 03:51:23 I ETA: 09.26 05.20.44 estimated_duration: 12:55:04.84 time_since_last_log: 00:10:54.31 time_per_update: 00:00:00.26 
09-26 03:51:23 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 03:51:23 I loss/online/main/E1: 2.482361316680908
09-26 03:51:23 I loss/online/total/E1: 2.482361316680908
09-26 03:51:23 I accuracy1/online/main/E1: 0.602951
09-26 03:51:47 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 03:51:47 I accuracy1/val/main: 0.788260
09-26 03:51:47 I loss/val/main: 0.859375
09-26 04:02:16 I ------------------
09-26 04:02:16 I Epoch 355/400 (E355_U888210_S454763520)
09-26 04:02:16 I ETA: 09.26 05.30.49 estimated_duration: 13:05:09.74 time_since_last_log: 00:10:53.22 time_per_update: 00:00:00.26 
09-26 04:02:16 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 04:02:16 I loss/online/main/E1: 2.4729278087615967
09-26 04:02:16 I loss/online/total/E1: 2.4729278087615967
09-26 04:02:16 I accuracy1/online/main/E1: 0.605025
09-26 04:02:40 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 04:02:40 I accuracy1/val/main: 0.788360
09-26 04:02:40 I loss/val/main: 0.86328125
09-26 04:13:07 I ------------------
09-26 04:13:07 I Epoch 356/400 (E356_U890712_S456044544)
09-26 04:13:07 I ETA: 09.26 05.40.48 estimated_duration: 13:15:08.72 time_since_last_log: 00:10:50.99 time_per_update: 00:00:00.26 
09-26 04:13:07 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 04:13:07 I loss/online/main/E1: 2.476069211959839
09-26 04:13:07 I loss/online/total/E1: 2.476069211959839
09-26 04:13:07 I accuracy1/online/main/E1: 0.602443
09-26 04:13:31 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 04:13:31 I accuracy1/val/main: 0.790180
09-26 04:13:31 I loss/val/main: 0.859375
09-26 04:23:59 I ------------------
09-26 04:23:59 I Epoch 357/400 (E357_U893214_S457325568)
09-26 04:23:59 I ETA: 09.26 05.50.45 estimated_duration: 13:25:05.32 time_since_last_log: 00:10:51.87 time_per_update: 00:00:00.26 
09-26 04:23:59 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 04:23:59 I loss/online/main/E1: 2.4706735610961914
09-26 04:23:59 I loss/online/total/E1: 2.4706735610961914
09-26 04:23:59 I accuracy1/online/main/E1: 0.604456
09-26 04:24:23 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 04:24:23 I accuracy1/val/main: 0.789280
09-26 04:24:23 I loss/val/main: 0.85546875
09-26 04:34:52 I ------------------
09-26 04:34:52 I Epoch 358/400 (E358_U895716_S458606592)
09-26 04:34:52 I ETA: 09.26 06.00.39 estimated_duration: 13:34:59.85 time_since_last_log: 00:10:53.01 time_per_update: 00:00:00.26 
09-26 04:34:52 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 04:34:52 I loss/online/main/E1: 2.4594991207122803
09-26 04:34:52 I loss/online/total/E1: 2.4594991207122803
09-26 04:34:52 I accuracy1/online/main/E1: 0.606319
09-26 04:35:16 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 04:35:16 I accuracy1/val/main: 0.788940
09-26 04:35:16 I loss/val/main: 0.859375
09-26 04:45:45 I ------------------
09-26 04:45:45 I Epoch 359/400 (E359_U898218_S459887616)
09-26 04:45:45 I ETA: 09.26 06.10.31 estimated_duration: 13:44:51.22 time_since_last_log: 00:10:53.16 time_per_update: 00:00:00.26 
09-26 04:45:45 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 04:45:45 I loss/online/main/E1: 2.469846725463867
09-26 04:45:45 I loss/online/total/E1: 2.469846725463867
09-26 04:45:45 I accuracy1/online/main/E1: 0.604452
09-26 04:46:09 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 04:46:09 I accuracy1/val/main: 0.789860
09-26 04:46:09 I loss/val/main: 0.86328125
09-26 04:56:37 I ------------------
09-26 04:56:37 I Epoch 360/400 (E360_U900720_S461168640)
09-26 04:56:37 I ETA: 09.26 06.20.18 estimated_duration: 13:54:38.33 time_since_last_log: 00:10:52.28 time_per_update: 00:00:00.26 
09-26 04:56:37 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 04:56:37 I loss/online/main/E1: 2.465772867202759
09-26 04:56:37 I loss/online/total/E1: 2.465772867202759
09-26 04:56:37 I accuracy1/online/main/E1: 0.605838
09-26 04:56:38 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest model.th
09-26 04:56:40 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest optim.th
09-26 04:56:40 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/trainer cp=latest.th
09-26 04:57:04 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 04:57:04 I accuracy1/val/main: 0.790560
09-26 04:57:04 I loss/val/main: 0.86328125
09-26 05:07:32 I ------------------
09-26 05:07:32 I Epoch 361/400 (E361_U903222_S462449664)
09-26 05:07:32 I ETA: 09.26 06.30.04 estimated_duration: 14:04:24.39 time_since_last_log: 00:10:54.28 time_per_update: 00:00:00.26 
09-26 05:07:32 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 05:07:32 I loss/online/main/E1: 2.4687576293945312
09-26 05:07:32 I loss/online/total/E1: 2.4687576293945312
09-26 05:07:32 I accuracy1/online/main/E1: 0.606235
09-26 05:07:55 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 05:07:56 I accuracy1/val/main: 0.790160
09-26 05:07:56 I loss/val/main: 0.859375
09-26 05:18:25 I ------------------
09-26 05:18:25 I Epoch 362/400 (E362_U905724_S463730688)
09-26 05:18:25 I ETA: 09.26 06.39.46 estimated_duration: 14:14:06.83 time_since_last_log: 00:10:53.94 time_per_update: 00:00:00.26 
09-26 05:18:25 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 05:18:25 I loss/online/main/E1: 2.4471616744995117
09-26 05:18:25 I loss/online/total/E1: 2.4471616744995117
09-26 05:18:26 I accuracy1/online/main/E1: 0.608457
09-26 05:18:49 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 05:18:50 I accuracy1/val/main: 0.790900
09-26 05:18:50 I loss/val/main: 0.85546875
09-26 05:29:18 I ------------------
09-26 05:29:18 I Epoch 363/400 (E363_U908226_S465011712)
09-26 05:29:18 I ETA: 09.26 06.49.23 estimated_duration: 14:23:43.96 time_since_last_log: 00:10:52.05 time_per_update: 00:00:00.26 
09-26 05:29:18 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 05:29:18 I loss/online/main/E1: 2.4466519355773926
09-26 05:29:18 I loss/online/total/E1: 2.4466519355773926
09-26 05:29:18 I accuracy1/online/main/E1: 0.607452
09-26 05:29:41 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 05:29:42 I accuracy1/val/main: 0.790680
09-26 05:29:42 I loss/val/main: 0.859375
09-26 05:40:10 I ------------------
09-26 05:40:10 I Epoch 364/400 (E364_U910728_S466292736)
09-26 05:40:10 I ETA: 09.26 06.58.58 estimated_duration: 14:33:18.78 time_since_last_log: 00:10:52.83 time_per_update: 00:00:00.26 
09-26 05:40:10 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 05:40:10 I loss/online/main/E1: 2.4395065307617188
09-26 05:40:10 I loss/online/total/E1: 2.4395065307617188
09-26 05:40:10 I accuracy1/online/main/E1: 0.609640
09-26 05:40:34 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 05:40:34 I accuracy1/val/main: 0.790380
09-26 05:40:34 I loss/val/main: 0.85546875
09-26 05:51:03 I ------------------
09-26 05:51:03 I Epoch 365/400 (E365_U913230_S467573760)
09-26 05:51:03 I ETA: 09.26 07.08.30 estimated_duration: 14:42:50.23 time_since_last_log: 00:10:52.65 time_per_update: 00:00:00.26 
09-26 05:51:03 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 05:51:03 I loss/online/main/E1: 2.439756393432617
09-26 05:51:03 I loss/online/total/E1: 2.439756393432617
09-26 05:51:03 I accuracy1/online/main/E1: 0.610156
09-26 05:51:27 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 05:51:27 I accuracy1/val/main: 0.790460
09-26 05:51:27 I loss/val/main: 0.8515625
09-26 06:01:56 I ------------------
09-26 06:01:56 I Epoch 366/400 (E366_U915732_S468854784)
09-26 06:01:56 I ETA: 09.26 07.17.59 estimated_duration: 14:52:19.23 time_since_last_log: 00:10:53.26 time_per_update: 00:00:00.26 
09-26 06:01:56 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 06:01:56 I loss/online/main/E1: 2.4440951347351074
09-26 06:01:56 I loss/online/total/E1: 2.4440951347351074
09-26 06:01:56 I accuracy1/online/main/E1: 0.609205
09-26 06:02:20 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 06:02:20 I accuracy1/val/main: 0.792100
09-26 06:02:20 I loss/val/main: 0.8515625
09-26 06:12:48 I ------------------
09-26 06:12:48 I Epoch 367/400 (E367_U918234_S470135808)
09-26 06:12:48 I ETA: 09.26 07.27.23 estimated_duration: 15:01:43.33 time_since_last_log: 00:10:51.63 time_per_update: 00:00:00.26 
09-26 06:12:48 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 06:12:48 I loss/online/main/E1: 2.431344985961914
09-26 06:12:48 I loss/online/total/E1: 2.431344985961914
09-26 06:12:48 I accuracy1/online/main/E1: 0.610643
09-26 06:13:12 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 06:13:12 I accuracy1/val/main: 0.790720
09-26 06:13:12 I loss/val/main: 0.8515625
09-26 06:23:41 I ------------------
09-26 06:23:41 I Epoch 368/400 (E368_U920736_S471416832)
09-26 06:23:41 I ETA: 09.26 07.36.46 estimated_duration: 15:11:06.42 time_since_last_log: 00:10:53.52 time_per_update: 00:00:00.26 
09-26 06:23:41 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 06:23:41 I loss/online/main/E1: 2.4289937019348145
09-26 06:23:41 I loss/online/total/E1: 2.4289937019348145
09-26 06:23:41 I accuracy1/online/main/E1: 0.611184
09-26 06:24:05 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 06:24:06 I accuracy1/val/main: 0.790860
09-26 06:24:06 I loss/val/main: 0.85546875
09-26 06:34:35 I ------------------
09-26 06:34:35 I Epoch 369/400 (E369_U923238_S472697856)
09-26 06:34:35 I ETA: 09.26 07.46.05 estimated_duration: 15:20:25.99 time_since_last_log: 00:10:53.10 time_per_update: 00:00:00.26 
09-26 06:34:35 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 06:34:35 I loss/online/main/E1: 2.4226043224334717
09-26 06:34:35 I loss/online/total/E1: 2.4226043224334717
09-26 06:34:35 I accuracy1/online/main/E1: 0.612207
09-26 06:34:59 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 06:34:59 I accuracy1/val/main: 0.792860
09-26 06:34:59 I loss/val/main: 0.84765625
09-26 06:45:28 I ------------------
09-26 06:45:28 I Epoch 370/400 (E370_U925740_S473978880)
09-26 06:45:28 I ETA: 09.26 07.55.22 estimated_duration: 15:29:42.89 time_since_last_log: 00:10:53.44 time_per_update: 00:00:00.26 
09-26 06:45:28 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 06:45:28 I loss/online/main/E1: 2.4356298446655273
09-26 06:45:28 I loss/online/total/E1: 2.4356298446655273
09-26 06:45:28 I accuracy1/online/main/E1: 0.609534
09-26 06:45:29 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest model.th
09-26 06:45:30 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest optim.th
09-26 06:45:30 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/trainer cp=latest.th
09-26 06:45:54 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 06:45:54 I accuracy1/val/main: 0.792940
09-26 06:45:54 I loss/val/main: 0.84765625
09-26 06:56:22 I ------------------
09-26 06:56:22 I Epoch 371/400 (E371_U928242_S475259904)
09-26 06:56:22 I ETA: 09.26 08.04.36 estimated_duration: 15:38:57.10 time_since_last_log: 00:10:53.73 time_per_update: 00:00:00.26 
09-26 06:56:22 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 06:56:22 I loss/online/main/E1: 2.43597149848938
09-26 06:56:22 I loss/online/total/E1: 2.43597149848938
09-26 06:56:22 I accuracy1/online/main/E1: 0.609124
09-26 06:56:46 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 06:56:46 I accuracy1/val/main: 0.792420
09-26 06:56:46 I loss/val/main: 0.84765625
09-26 07:07:15 I ------------------
09-26 07:07:15 I Epoch 372/400 (E372_U930744_S476540928)
09-26 07:07:15 I ETA: 09.26 08.13.47 estimated_duration: 15:48:08.16 time_since_last_log: 00:10:53.58 time_per_update: 00:00:00.26 
09-26 07:07:15 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 07:07:15 I loss/online/main/E1: 2.418386220932007
09-26 07:07:15 I loss/online/total/E1: 2.418386220932007
09-26 07:07:15 I accuracy1/online/main/E1: 0.613261
09-26 07:07:39 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 07:07:39 I accuracy1/val/main: 0.792440
09-26 07:07:39 I loss/val/main: 0.84765625
09-26 07:18:08 I ------------------
09-26 07:18:08 I Epoch 373/400 (E373_U933246_S477821952)
09-26 07:18:08 I ETA: 09.26 08.22.55 estimated_duration: 15:57:15.21 time_since_last_log: 00:10:52.60 time_per_update: 00:00:00.26 
09-26 07:18:08 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 07:18:08 I loss/online/main/E1: 2.4188690185546875
09-26 07:18:08 I loss/online/total/E1: 2.4188690185546875
09-26 07:18:08 I accuracy1/online/main/E1: 0.610962
09-26 07:18:32 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 07:18:32 I accuracy1/val/main: 0.791980
09-26 07:18:32 I loss/val/main: 0.84765625
09-26 07:29:00 I ------------------
09-26 07:29:00 I Epoch 374/400 (E374_U935748_S479102976)
09-26 07:29:00 I ETA: 09.26 08.31.58 estimated_duration: 16:06:18.70 time_since_last_log: 00:10:52.02 time_per_update: 00:00:00.26 
09-26 07:29:00 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 07:29:00 I loss/online/main/E1: 2.423332691192627
09-26 07:29:00 I loss/online/total/E1: 2.423332691192627
09-26 07:29:00 I accuracy1/online/main/E1: 0.611318
09-26 07:29:24 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 07:29:24 I accuracy1/val/main: 0.791340
09-26 07:29:24 I loss/val/main: 0.84375
09-26 07:39:52 I ------------------
09-26 07:39:52 I Epoch 375/400 (E375_U938250_S480384000)
09-26 07:39:52 I ETA: 09.26 08.40.59 estimated_duration: 16:15:19.58 time_since_last_log: 00:10:52.30 time_per_update: 00:00:00.26 
09-26 07:39:52 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 07:39:52 I loss/online/main/E1: 2.415163993835449
09-26 07:39:52 I loss/online/total/E1: 2.415163993835449
09-26 07:39:52 I accuracy1/online/main/E1: 0.613811
09-26 07:40:16 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 07:40:16 I accuracy1/val/main: 0.791980
09-26 07:40:16 I loss/val/main: 0.84765625
09-26 07:50:46 I ------------------
09-26 07:50:46 I Epoch 376/400 (E376_U940752_S481665024)
09-26 07:50:46 I ETA: 09.26 08.49.58 estimated_duration: 16:24:18.84 time_since_last_log: 00:10:53.48 time_per_update: 00:00:00.26 
09-26 07:50:46 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 07:50:46 I loss/online/main/E1: 2.4132518768310547
09-26 07:50:46 I loss/online/total/E1: 2.4132518768310547
09-26 07:50:46 I accuracy1/online/main/E1: 0.613725
09-26 07:51:10 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 07:51:10 I accuracy1/val/main: 0.791840
09-26 07:51:10 I loss/val/main: 0.8515625
09-26 08:01:38 I ------------------
09-26 08:01:38 I Epoch 377/400 (E377_U943254_S482946048)
09-26 08:01:38 I ETA: 09.26 08.58.53 estimated_duration: 16:33:14.14 time_since_last_log: 00:10:52.45 time_per_update: 00:00:00.26 
09-26 08:01:38 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 08:01:38 I loss/online/main/E1: 2.4024667739868164
09-26 08:01:38 I loss/online/total/E1: 2.4024667739868164
09-26 08:01:38 I accuracy1/online/main/E1: 0.616034
09-26 08:02:02 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 08:02:02 I accuracy1/val/main: 0.792540
09-26 08:02:02 I loss/val/main: 0.8515625
09-26 08:12:31 I ------------------
09-26 08:12:31 I Epoch 378/400 (E378_U945756_S484227072)
09-26 08:12:31 I ETA: 09.26 09.07.46 estimated_duration: 16:42:06.64 time_since_last_log: 00:10:52.50 time_per_update: 00:00:00.26 
09-26 08:12:31 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 08:12:31 I loss/online/main/E1: 2.425945520401001
09-26 08:12:31 I loss/online/total/E1: 2.425945520401001
09-26 08:12:31 I accuracy1/online/main/E1: 0.613014
09-26 08:12:55 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 08:12:55 I accuracy1/val/main: 0.793300
09-26 08:12:55 I loss/val/main: 0.84375
09-26 08:23:23 I ------------------
09-26 08:23:23 I Epoch 379/400 (E379_U948258_S485508096)
09-26 08:23:23 I ETA: 09.26 09.16.36 estimated_duration: 16:50:56.37 time_since_last_log: 00:10:52.53 time_per_update: 00:00:00.26 
09-26 08:23:23 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 08:23:23 I loss/online/main/E1: 2.410749912261963
09-26 08:23:23 I loss/online/total/E1: 2.410749912261963
09-26 08:23:23 I accuracy1/online/main/E1: 0.613819
09-26 08:23:47 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 08:23:47 I accuracy1/val/main: 0.793700
09-26 08:23:47 I loss/val/main: 0.84375
09-26 08:34:16 I ------------------
09-26 08:34:16 I Epoch 380/400 (E380_U950760_S486789120)
09-26 08:34:16 I ETA: 09.26 09.25.23 estimated_duration: 16:59:43.65 time_since_last_log: 00:10:52.86 time_per_update: 00:00:00.26 
09-26 08:34:16 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 08:34:16 I loss/online/main/E1: 2.417241334915161
09-26 08:34:16 I loss/online/total/E1: 2.417241334915161
09-26 08:34:16 I accuracy1/online/main/E1: 0.612405
09-26 08:34:17 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest model.th
09-26 08:34:19 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest optim.th
09-26 08:34:19 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/trainer cp=latest.th
09-26 08:34:42 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 08:34:43 I accuracy1/val/main: 0.793600
09-26 08:34:43 I loss/val/main: 0.84375
09-26 08:45:11 I ------------------
09-26 08:45:11 I Epoch 381/400 (E381_U953262_S488070144)
09-26 08:45:11 I ETA: 09.26 09.34.10 estimated_duration: 17:08:30.23 time_since_last_log: 00:10:54.84 time_per_update: 00:00:00.26 
09-26 08:45:11 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 08:45:11 I loss/online/main/E1: 2.417590856552124
09-26 08:45:11 I loss/online/total/E1: 2.417590856552124
09-26 08:45:11 I accuracy1/online/main/E1: 0.613775
09-26 08:45:35 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 08:45:35 I accuracy1/val/main: 0.794360
09-26 08:45:35 I loss/val/main: 0.84375
09-26 08:56:05 I ------------------
09-26 08:56:05 I Epoch 382/400 (E382_U955764_S489351168)
09-26 08:56:05 I ETA: 09.26 09.42.52 estimated_duration: 17:17:13.09 time_since_last_log: 00:10:53.93 time_per_update: 00:00:00.26 
09-26 08:56:05 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 08:56:05 I loss/online/main/E1: 2.408407211303711
09-26 08:56:05 I loss/online/total/E1: 2.408407211303711
09-26 08:56:05 I accuracy1/online/main/E1: 0.614248
09-26 08:56:29 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 08:56:29 I accuracy1/val/main: 0.793900
09-26 08:56:29 I loss/val/main: 0.84375
09-26 09:06:59 I ------------------
09-26 09:06:59 I Epoch 383/400 (E383_U958266_S490632192)
09-26 09:06:59 I ETA: 09.26 09.51.33 estimated_duration: 17:25:53.20 time_since_last_log: 00:10:53.92 time_per_update: 00:00:00.26 
09-26 09:06:59 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 09:06:59 I loss/online/main/E1: 2.4050590991973877
09-26 09:06:59 I loss/online/total/E1: 2.4050590991973877
09-26 09:06:59 I accuracy1/online/main/E1: 0.613857
09-26 09:07:23 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 09:07:23 I accuracy1/val/main: 0.793920
09-26 09:07:23 I loss/val/main: 0.84375
09-26 09:17:53 I ------------------
09-26 09:17:53 I Epoch 384/400 (E384_U960768_S491913216)
09-26 09:17:53 I ETA: 09.26 10.00.10 estimated_duration: 17:34:30.44 time_since_last_log: 00:10:53.77 time_per_update: 00:00:00.26 
09-26 09:17:53 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 09:17:53 I loss/online/main/E1: 2.4050452709198
09-26 09:17:53 I loss/online/total/E1: 2.4050452709198
09-26 09:17:53 I accuracy1/online/main/E1: 0.615130
09-26 09:18:17 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 09:18:17 I accuracy1/val/main: 0.793560
09-26 09:18:17 I loss/val/main: 0.84375
09-26 09:28:45 I ------------------
09-26 09:28:45 I Epoch 385/400 (E385_U963270_S493194240)
09-26 09:28:45 I ETA: 09.26 10.08.42 estimated_duration: 17:43:03.09 time_since_last_log: 00:10:51.94 time_per_update: 00:00:00.26 
09-26 09:28:45 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 09:28:45 I loss/online/main/E1: 2.395202159881592
09-26 09:28:45 I loss/online/total/E1: 2.395202159881592
09-26 09:28:45 I accuracy1/online/main/E1: 0.616606
09-26 09:29:08 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 09:29:09 I accuracy1/val/main: 0.793500
09-26 09:29:09 I loss/val/main: 0.84375
09-26 09:39:38 I ------------------
09-26 09:39:38 I Epoch 386/400 (E386_U965772_S494475264)
09-26 09:39:38 I ETA: 09.26 10.17.14 estimated_duration: 17:51:34.90 time_since_last_log: 00:10:53.70 time_per_update: 00:00:00.26 
09-26 09:39:38 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 09:39:38 I loss/online/main/E1: 2.410379648208618
09-26 09:39:38 I loss/online/total/E1: 2.410379648208618
09-26 09:39:38 I accuracy1/online/main/E1: 0.614128
09-26 09:40:02 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 09:40:02 I accuracy1/val/main: 0.794020
09-26 09:40:02 I loss/val/main: 0.84375
09-26 09:50:32 I ------------------
09-26 09:50:32 I Epoch 387/400 (E387_U968274_S495756288)
09-26 09:50:32 I ETA: 09.26 10.25.43 estimated_duration: 18:00:03.60 time_since_last_log: 00:10:53.27 time_per_update: 00:00:00.26 
09-26 09:50:32 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 09:50:32 I loss/online/main/E1: 2.4056556224823
09-26 09:50:32 I loss/online/total/E1: 2.4056556224823
09-26 09:50:32 I accuracy1/online/main/E1: 0.614914
09-26 09:50:55 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 09:50:56 I accuracy1/val/main: 0.794280
09-26 09:50:56 I loss/val/main: 0.84375
09-26 10:01:24 I ------------------
09-26 10:01:24 I Epoch 388/400 (E388_U970776_S497037312)
09-26 10:01:24 I ETA: 09.26 10.34.08 estimated_duration: 18:08:28.56 time_since_last_log: 00:10:52.18 time_per_update: 00:00:00.26 
09-26 10:01:24 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 10:01:24 I loss/online/main/E1: 2.401015520095825
09-26 10:01:24 I loss/online/total/E1: 2.401015520095825
09-26 10:01:24 I accuracy1/online/main/E1: 0.615282
09-26 10:01:48 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 10:01:48 I accuracy1/val/main: 0.793980
09-26 10:01:48 I loss/val/main: 0.84375
09-26 10:12:17 I ------------------
09-26 10:12:17 I Epoch 389/400 (E389_U973278_S498318336)
09-26 10:12:17 I ETA: 09.26 10.42.32 estimated_duration: 18:16:52.44 time_since_last_log: 00:10:53.66 time_per_update: 00:00:00.26 
09-26 10:12:17 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 10:12:17 I loss/online/main/E1: 2.387704849243164
09-26 10:12:17 I loss/online/total/E1: 2.387704849243164
09-26 10:12:17 I accuracy1/online/main/E1: 0.617998
09-26 10:12:41 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 10:12:41 I accuracy1/val/main: 0.793740
09-26 10:12:41 I loss/val/main: 0.84375
09-26 10:23:11 I ------------------
09-26 10:23:11 I Epoch 390/400 (E390_U975780_S499599360)
09-26 10:23:11 I ETA: 09.26 10.50.53 estimated_duration: 18:25:13.60 time_since_last_log: 00:10:53.54 time_per_update: 00:00:00.26 
09-26 10:23:11 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 10:23:11 I loss/online/main/E1: 2.4049746990203857
09-26 10:23:11 I loss/online/total/E1: 2.4049746990203857
09-26 10:23:11 I accuracy1/online/main/E1: 0.614785
09-26 10:23:12 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest model.th
09-26 10:23:13 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest optim.th
09-26 10:23:13 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/trainer cp=latest.th
09-26 10:23:37 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 10:23:37 I accuracy1/val/main: 0.793480
09-26 10:23:37 I loss/val/main: 0.84375
09-26 10:34:06 I ------------------
09-26 10:34:06 I Epoch 391/400 (E391_U978282_S500880384)
09-26 10:34:06 I ETA: 09.26 10.59.13 estimated_duration: 18:33:33.30 time_since_last_log: 00:10:54.62 time_per_update: 00:00:00.26 
09-26 10:34:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 10:34:06 I loss/online/main/E1: 2.3942813873291016
09-26 10:34:06 I loss/online/total/E1: 2.3942813873291016
09-26 10:34:06 I accuracy1/online/main/E1: 0.617673
09-26 10:34:29 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 10:34:30 I accuracy1/val/main: 0.793700
09-26 10:34:30 I loss/val/main: 0.83984375
09-26 10:44:57 I ------------------
09-26 10:44:57 I Epoch 392/400 (E392_U980784_S502161408)
09-26 10:44:57 I ETA: 09.26 11.07.26 estimated_duration: 18:41:46.99 time_since_last_log: 00:10:51.24 time_per_update: 00:00:00.26 
09-26 10:44:57 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 10:44:57 I loss/online/main/E1: 2.3854546546936035
09-26 10:44:57 I loss/online/total/E1: 2.3854546546936035
09-26 10:44:57 I accuracy1/online/main/E1: 0.618529
09-26 10:45:21 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 10:45:21 I accuracy1/val/main: 0.794140
09-26 10:45:21 I loss/val/main: 0.84375
09-26 10:55:50 I ------------------
09-26 10:55:50 I Epoch 393/400 (E393_U983286_S503442432)
09-26 10:55:50 I ETA: 09.26 11.15.40 estimated_duration: 18:50:00.28 time_since_last_log: 00:10:53.32 time_per_update: 00:00:00.26 
09-26 10:55:50 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 10:55:50 I loss/online/main/E1: 2.4052348136901855
09-26 10:55:50 I loss/online/total/E1: 2.4052348136901855
09-26 10:55:50 I accuracy1/online/main/E1: 0.614721
09-26 10:56:14 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 10:56:14 I accuracy1/val/main: 0.793860
09-26 10:56:14 I loss/val/main: 0.83984375
09-26 11:06:44 I ------------------
09-26 11:06:44 I Epoch 394/400 (E394_U985788_S504723456)
09-26 11:06:44 I ETA: 09.26 11.23.50 estimated_duration: 18:58:11.13 time_since_last_log: 00:10:53.39 time_per_update: 00:00:00.26 
09-26 11:06:44 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 11:06:44 I loss/online/main/E1: 2.394535779953003
09-26 11:06:44 I loss/online/total/E1: 2.394535779953003
09-26 11:06:44 I accuracy1/online/main/E1: 0.616087
09-26 11:07:07 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 11:07:08 I accuracy1/val/main: 0.794480
09-26 11:07:08 I loss/val/main: 0.83984375
09-26 11:17:36 I ------------------
09-26 11:17:36 I Epoch 395/400 (E395_U988290_S506004480)
09-26 11:17:36 I ETA: 09.26 11.31.58 estimated_duration: 19:06:18.43 time_since_last_log: 00:10:52.34 time_per_update: 00:00:00.26 
09-26 11:17:36 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 11:17:36 I loss/online/main/E1: 2.392808437347412
09-26 11:17:36 I loss/online/total/E1: 2.392808437347412
09-26 11:17:36 I accuracy1/online/main/E1: 0.615440
09-26 11:18:00 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 11:18:00 I accuracy1/val/main: 0.794820
09-26 11:18:00 I loss/val/main: 0.83984375
09-26 11:28:29 I ------------------
09-26 11:28:29 I Epoch 396/400 (E396_U990792_S507285504)
09-26 11:28:29 I ETA: 09.26 11.40.03 estimated_duration: 19:14:23.72 time_since_last_log: 00:10:52.80 time_per_update: 00:00:00.26 
09-26 11:28:29 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 11:28:29 I loss/online/main/E1: 2.3993654251098633
09-26 11:28:29 I loss/online/total/E1: 2.3993654251098633
09-26 11:28:29 I accuracy1/online/main/E1: 0.616520
09-26 11:28:53 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 11:28:53 I accuracy1/val/main: 0.793800
09-26 11:28:53 I loss/val/main: 0.84375
09-26 11:39:22 I ------------------
09-26 11:39:22 I Epoch 397/400 (E397_U993294_S508566528)
09-26 11:39:22 I ETA: 09.26 11.48.06 estimated_duration: 19:22:27.10 time_since_last_log: 00:10:53.33 time_per_update: 00:00:00.26 
09-26 11:39:22 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 11:39:22 I loss/online/main/E1: 2.385773181915283
09-26 11:39:22 I loss/online/total/E1: 2.385773181915283
09-26 11:39:22 I accuracy1/online/main/E1: 0.617726
09-26 11:39:46 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 11:39:46 I accuracy1/val/main: 0.793940
09-26 11:39:46 I loss/val/main: 0.83984375
09-26 11:50:14 I ------------------
09-26 11:50:14 I Epoch 398/400 (E398_U995796_S509847552)
09-26 11:50:14 I ETA: 09.26 11.56.06 estimated_duration: 19:30:26.65 time_since_last_log: 00:10:51.95 time_per_update: 00:00:00.26 
09-26 11:50:14 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 11:50:14 I loss/online/main/E1: 2.384072780609131
09-26 11:50:14 I loss/online/total/E1: 2.384072780609131
09-26 11:50:14 I accuracy1/online/main/E1: 0.618096
09-26 11:50:38 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 11:50:38 I accuracy1/val/main: 0.793840
09-26 11:50:38 I loss/val/main: 0.83984375
09-26 12:01:05 I ------------------
09-26 12:01:05 I Epoch 399/400 (E399_U998298_S511128576)
09-26 12:01:05 I ETA: 09.26 12.04.02 estimated_duration: 19:38:23.05 time_since_last_log: 00:10:51.20 time_per_update: 00:00:00.26 
09-26 12:01:05 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 12:01:05 I loss/online/main/E1: 2.384673595428467
09-26 12:01:05 I loss/online/total/E1: 2.384673595428467
09-26 12:01:05 I accuracy1/online/main/E1: 0.617954
09-26 12:01:29 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 12:01:29 I accuracy1/val/main: 0.794540
09-26 12:01:29 I loss/val/main: 0.83984375
09-26 12:11:58 I ------------------
09-26 12:11:58 I Epoch 400/400 (E400_U1000800_S512409600)
09-26 12:11:58 I ETA: 09.26 12.11.58 estimated_duration: 19:46:18.83 time_since_last_log: 00:10:52.98 time_per_update: 00:00:00.26 
09-26 12:11:58 I data=[0.00, 0.00, 0.00, 0.00] update=[0.25, 0.25, 0.25, 0.25]
09-26 12:11:58 I loss/online/main/E1: 2.4013476371765137
09-26 12:11:58 I loss/online/total/E1: 2.4013476371765137
09-26 12:11:58 I accuracy1/online/main/E1: 0.615135
09-26 12:11:59 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest model.th
09-26 12:12:00 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest optim.th
09-26 12:12:00 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/trainer cp=latest.th
09-26 12:12:24 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.24
09-26 12:12:25 I accuracy1/val/main: 0.794360
09-26 12:12:25 I loss/val/main: 0.84375
09-26 12:12:26 I ------------------
09-26 12:12:26 I AFTER TRAINING
09-26 12:12:26 I ------------------
09-26 12:12:26 I total_train_data_time:   [87.88, 83.70, 87.57, 82.17]
09-26 12:12:26 I total_update_time: [68033.23, 68072.88, 68050.72, 68076.92]
09-26 12:12:27 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=last model.th
09-26 12:12:27 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/trainer cp=last.th
09-26 12:12:27 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest model.th
09-26 12:12:29 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/vislstm cp=latest optim.th
09-26 12:12:29 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/checkpoints/trainer cp=latest.th
09-26 12:12:29 I ------------------
09-26 12:12:29 I offline_accuracy_callback dataset_key=val.x.class
09-26 12:12:29 I total_data_time:    [0.16, 0.14, 0.15, 0.16]
09-26 12:12:29 I total_forward_time: [26.70, 26.53, 26.61, 26.70]
09-26 12:12:29 I writing 5610 log entries to /home/beknur.kalmakhanbet/save/in1k/a0gelq34/primitive/entries.th
09-26 12:12:29 I ------------------
09-26 12:12:29 I summarize logvalues
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/providers/summary_providers/primitive_summary_provider.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  entries = torch.load(entries_uri)
09-26 12:12:29 I loss/online/main/U50/min: 2.215944528579712
09-26 12:12:29 I loss/online/total/U50/min: 2.215944528579712
09-26 12:12:29 I accuracy1/online/main/U50/max: 0.6507031321525574
09-26 12:12:29 I loss/online/main/E1/min: 2.384072780609131
09-26 12:12:29 I loss/online/total/E1/min: 2.384072780609131
09-26 12:12:29 I accuracy1/online/main/E1/max: 0.6185286045074463
09-26 12:12:29 I accuracy1/val/main/max: 0.7948200106620789
09-26 12:12:29 I loss/val/main/min: 0.83984375
09-26 12:12:29 W cuda profiling is not activated -> all cuda calls are executed asynchronously -> this will result in inaccurate profiling times where the time for all asynchronous cuda operation will be attributed to the first synchronous cuda operation https://github.com/BenediktAlkin/KappaProfiler?tab=readme-ov-file#time-async-operations
09-26 12:12:29 I full profiling times:
 71918.04 train
     0.00 train.DatasetStatsCallback.before_training
     0.00 train.ParamCountCallback.before_training
     0.00 train.CopyPreviousConfigCallback.before_training
     0.00 train.CopyPreviousSummaryCallback.before_training
     0.00 train.ProgressCallback(every_n_epochs=1).before_training
     0.00 train.OnlineAccuracyCallback(every_n_updates=50).before_training
     0.00 train.OnlineAccuracyCallback(every_n_epochs=1).before_training
     0.00 train.CheckpointCallback().before_training
     0.00 train.CheckpointCallback(every_n_epochs=10).before_training
     0.00 train.OfflineAccuracyCallback(every_n_epochs=1).before_training
     0.49 train.iterator
    87.88 train.data_loading
 68033.23 train.update
     2.67 train.OnlineLossCallback(every_n_epochs=1).track_after_accumulation_step
     1.83 train.OnlineLossCallback(every_n_updates=50).track_after_accumulation_step
   160.85 train.OnlineAccuracyCallback(every_n_updates=50).track_after_accumulation_step
    90.96 train.OnlineAccuracyCallback(every_n_epochs=1).track_after_accumulation_step
     1.22 train.TrainTimeCallback(every_n_epochs=1).track_after_update_step
     0.21 train.LrCallback(every_n_updates=50).after_update
     0.03 train.FreezerCallback(every_n_updates=50).after_update
     9.38 train.OnlineLossCallback(every_n_updates=50).after_update
     1.89 train.OnlineAccuracyCallback(every_n_updates=50).after_update
     0.14 train.ProgressCallback(every_n_epochs=1).after_epoch
     0.29 train.TrainTimeCallback(every_n_epochs=1).after_epoch
     1.51 train.OnlineLossCallback(every_n_epochs=1).after_epoch
     0.96 train.OnlineAccuracyCallback(every_n_epochs=1).after_epoch
  2644.66 train.OfflineAccuracyCallback(every_n_epochs=1).after_epoch
    25.81 train.CheckpointCallback(every_n_epochs=10).after_epoch
     0.03 train.TrainTimeCallback(every_n_epochs=1).after_training
     0.67 train.CheckpointCallback().after_training
     2.09 train.CheckpointCallback(every_n_epochs=10).after_training
09-26 12:12:29 I ------------------
09-26 12:12:29 I CLEANUP
09-26 12:12:29 I ------------------
09-26 12:12:29 I encountered 1 warnings
09-26 12:12:29 I encountered 0 errors
slurmstepd-gpu-53: error: *** JOB 141931 ON gpu-53 CANCELLED AT 2025-09-28T16:13:41 DUE TO TIME LIMIT ***
