MASTER_ADDR: gpu-51
CUDA_VISIBLE_DEVICES=0,1
Fri May  2 02:42:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   25C    P0             52W /  400W |       4MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   25C    P0             52W /  400W |       4MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   24C    P0             51W /  400W |       4MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   24C    P0             50W /  400W |       4MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
WARNING: infoROM is corrupted at gpu 0000:81:00.0
torch: 2.5.1+cu121 cuda: 12.1 cuda available: True
05-02 02:42:58 I found multiple visible devices (CUDA_VISIBLE_DEVICES=2,3) -> set CUDA_VISIBLE_DEVICES=2 (local_rank=0)
05-02 02:42:58 I running single process slurm training
05-02 02:42:59 I initialized process rank=0 local_rank=0 pid=1673647
05-02 02:42:59 I initialized 1 processes
05-02 02:42:59 W disabled cudnn benchmark
05-02 02:42:59 W enabled cudnn deterministic
05-02 02:42:59 I log file: /home/beknur.kalmakhanbet/save/in1k/0azvm0yh/log.txt
05-02 02:42:59 I no seed specified -> using seed=0
05-02 02:42:59 I ------------------
05-02 02:42:59 I initializing wandb (mode=disabled)
fatal: No annotated tags can describe '5e555fadd771e766a58325a1f499a6485161169a'.
However, there were unannotated tags: try --tags.
05-02 02:42:59 I ------------------
05-02 02:42:59 I stage_id: 0azvm0yh
05-02 02:42:59 I python main_train.py --hp src/vislstm/yamls/pretrain/vil/lstm_6M16_e800_bialter_bilatflat_conv2d3_lr1e3_res192_bias.yaml --name minLSTM --num_workers 4
05-02 02:42:59 I ------------------
05-02 02:42:59 I VERSION CHECK
05-02 02:42:59 I executable: /home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/bin/python
05-02 02:42:59 I python version: 3.9.21
05-02 02:42:59 I torch version: 2.5.1+cu121
05-02 02:42:59 I torch.cuda version: 12.1
05-02 02:42:59 I torchvision.version: 0.20.1+cu121
05-02 02:43:01 I torchmetrics version: 1.6.2
05-02 02:43:01 I kappaschedules version: 0.0.31
05-02 02:43:01 I kappamodules version: 0.1.76
05-02 02:43:01 I ------------------
05-02 02:43:01 I SYSTEM INFO
05-02 02:43:01 I host name: gpu-51
05-02 02:43:01 I OS: Linux-5.15.161-ql-generic-13.0-14-x86_64-with-glibc2.35
05-02 02:43:01 I OS version: #1 SMP Wed Jun 26 16:19:39 UTC 2024
05-02 02:43:01 I CUDA version: 12.4
05-02 02:43:01 I current commit hash: 5e555fadd771e766a58325a1f499a6485161169a
fatal: No annotated tags can describe '5e555fadd771e766a58325a1f499a6485161169a'.
However, there were unannotated tags: try --tags.
05-02 02:43:01 I latest git tag: 
05-02 02:43:01 I initialized process rank=0 local_rank=0 pid=1673647 hostname=gpu-51
05-02 02:43:01 I total_cpu_count: 2
05-02 02:43:01 I ------------------
05-02 02:43:01 I STATIC CONFIG
05-02 02:43:01 I account_name: beknur.kalmakhanbet
05-02 02:43:01 I output_path: /home/beknur.kalmakhanbet/save
05-02 02:43:01 I ------------------
05-02 02:43:01 I CLI ARGS
05-02 02:43:01 I hp: src/vislstm/yamls/pretrain/vil/lstm_6M16_e800_bialter_bilatflat_conv2d3_lr1e3_res192_bias.yaml
05-02 02:43:01 I accelerator: gpu
05-02 02:43:01 I num_workers: 4
05-02 02:43:01 I testrun: False
05-02 02:43:01 I minmodelrun: False
05-02 02:43:01 I mindatarun: False
05-02 02:43:01 I mindurationrun: False
05-02 02:43:01 I name: minLSTM
05-02 02:43:01 I static_config_uri: static_config.yaml
05-02 02:43:01 I ------------------
05-02 02:43:01 I DIST CONFIG
05-02 02:43:01 I rank: 0
05-02 02:43:01 I local_rank: 0
05-02 02:43:01 I world_size: 1
05-02 02:43:01 I nodes: 1
05-02 02:43:01 I backend: None
05-02 02:43:01 I slurm job id: 115175
05-02 02:43:01 I hostnames: gpu-51
05-02 02:43:01 I ------------------
master_factory_base_path: vislstm
name: in1k-lstm-6m16-e800res192-bialter-bilatflat-lr1e3-conv2d3-bias
stage_name: in1k
datasets:
  train:
    kind: cifar10
    split: train
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: random_resized_crop
        size: 192
        scale:
        - 0.08
        - 1.0
        interpolation: bicubic
      - kind: random_horizontal_flip
      - kind: transforms.three_augment
        blur_sigma:
        - 0.1
        - 2.0
      - kind: color_jitter
        brightness: 0.3
        contrast: 0.3
        saturation: 0.3
        hue: 0.0
      - kind: cifar10_norm
    - kind: one_hot_wrapper
    collators:
    - kind: mix_collator
      mixup_alpha: 0.8
      cutmix_alpha: 1.0
      mixup_p: 0.5
      cutmix_p: 0.5
      apply_mode: batch
      lamb_mode: batch
      shuffle_mode: flip
  val:
    kind: cifar10
    split: val
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: resize
        size: 192
        interpolation: bicubic
      - kind: center_crop
        size: 192
      - kind: cifar10_norm
model:
  kind: models.single.vislstm
  patch_size: 16
  dim: 192
  depth: 24
  bidirectional: false
  alternation: bidirectional
  conv1d_kernel_size: 3
  use_conv2d: true
  bias: true
  pos_embed_mode: learnable
  mode: classifier
  pooling:
    kind: bilateral
    aggregate: flatten
  optim:
    kind: adamw
    lr: 0.001
    betas:
    - 0.9
    - 0.999
    weight_decay: 0.05
    clip_grad_norm: 1.0
    schedule:
      kind: linear_warmup_cosine_decay_schedule
      warmup_epochs: 5
      end_value: 1.0e-06
    lr_scaler:
      kind: linear_lr_scaler
      divisor: 1024
trainer:
  kind: classification_trainer
  precision: bfloat16
  backup_precision: float16
  max_epochs: 800
  effective_batch_size: 2048
  log_every_n_epochs: 1
  use_torch_compile: true
  callbacks:
  - kind: checkpoint_callback
  - kind: checkpoint_callback
    every_n_epochs: 10
    save_weights: false
    save_latest_weights: true
    save_latest_optim: true
  - kind: offline_accuracy_callback
    every_n_epochs: 1
    dataset_key: val
05-02 02:43:01 I copied unresolved hp to /home/beknur.kalmakhanbet/save/in1k/0azvm0yh/hp_unresolved.yaml
05-02 02:43:01 I dumped resolved hp to /home/beknur.kalmakhanbet/save/in1k/0azvm0yh/hp_resolved.yaml
05-02 02:43:01 I ------------------
05-02 02:43:01 I training stage 'in1k'
05-02 02:43:01 I set seed to 0
05-02 02:43:01 I ------------------
05-02 02:43:01 I initializing datasets
05-02 02:43:01 I initializing train
05-02 02:43:02 I instantiating sample_wrapper x_transform_wrapper
05-02 02:43:02 I instantiating sample_wrapper one_hot_wrapper
05-02 02:43:02 I initializing val
05-02 02:43:03 I instantiating sample_wrapper x_transform_wrapper
05-02 02:43:03 I ------------------
05-02 02:43:03 I initializing trainer
05-02 02:43:03 I using precision: torch.bfloat16 (desired=bfloat16 backup=float16)
05-02 02:43:03 I main_sampler: RandomSampler(num_repeats=1)
05-02 02:43:03 I ------------------
05-02 02:43:03 I creating model
05-02 02:43:03 I input_shape: (3, 192, 192)
05-02 02:43:03 I pos_embed.is_learnable=True
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=384, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=192, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=384)
05-02 02:43:03 I drop_path_rate: 0.0
05-02 02:43:03 I model:
VisLSTM(
  (pooling): Bilateral(aggregate=flatten)
  (patch_embed): VitPatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_embed): VitPosEmbed2d()
  (xlstm): xLSTMBlockStack(
    (blocks): ModuleList(
      (0-23): 24 x mLSTMBlock(
        (drop_path1): DropPath(drop_prob=0.000)
        (xlstm_norm): LayerNorm()
        (xlstm): mLSTMLayer(
          (proj_up): Linear(in_features=192, out_features=768, bias=True)
          (conv1d): SequenceConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (conv_act_fn): SiLU()
          (mlstm_cell): mLSTMCell(
            (linear_i): Conv1d(384, 384, kernel_size=(1,), stride=(1,), groups=384, bias=False)
            (linear_f): Conv1d(384, 384, kernel_size=(1,), stride=(1,), groups=384, bias=False)
            (linear_h): Conv1d(384, 384, kernel_size=(1,), stride=(1,), groups=384, bias=False)
          )
          (ogate_act_fn): SiLU()
          (proj_down): Linear(in_features=384, out_features=192, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layerscale): Identity()
        )
      )
    )
    (post_blocks_norm): LayerNorm()
  )
  (head): Sequential(
    (0): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
    (1): Linear(in_features=384, out_features=10, bias=True)
  )
)
05-02 02:43:03 I vislstm initialize optimizer
05-02 02:43:03 I base lr: 1e-3
05-02 02:43:03 I scaled lr: 2e-3
05-02 02:43:03 I lr_scaler=LinearLrScaler(divisor=1024)
05-02 02:43:03 I lr_scale_factor=2048
05-02 02:43:03 I exclude_bias_from_wd=True exclude_norm_from_wd=True param_group_modifiers=[WeightDecayByNameModifier(name=pos_embed.embed)]
05-02 02:43:03 I using 2 param groups:
05-02 02:43:03 I len(params)=146
05-02 02:43:03 I weight_decay=0.0 len(params)=151
05-02 02:43:03 I added default DatasetStatsCallback
05-02 02:43:03 I added default ParamCountCallback
05-02 02:43:03 I added default CopyPreviousConfigCallback
05-02 02:43:03 I added default CopyPreviousSummaryCallback
05-02 02:43:03 I added default ProgressCallback(every_n_epochs=1)
05-02 02:43:03 I added default TrainTimeCallback(every_n_epochs=1)
05-02 02:43:03 I added default OnlineLossCallback(every_n_epochs=1)
05-02 02:43:03 I added default LrCallback(every_n_updates=50)
05-02 02:43:03 I added default FreezerCallback(every_n_updates=50)
05-02 02:43:03 I added default OnlineLossCallback(every_n_updates=50)
05-02 02:43:03 I wrapping model with torch.compile
05-02 02:43:04 I ------------------
05-02 02:43:04 I PREPARE TRAINER
05-02 02:43:04 I calculating batch_size and accumulation_steps (effective_batch_size=2048)
05-02 02:43:04 I torch.compile is used -> automatic batchsize not supported
05-02 02:43:04 I train_batches per epoch: 24 (world_size=1 batch_size=2048)
05-02 02:43:04 I initializing dataloader
05-02 02:43:04 I OfflineAccuracyCallback(every_n_epochs=1) registered InterleavedSamplerConfig(every_n_epochs=1) dataset_mode='x class'
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
05-02 02:43:04 I created dataloader (batch_size=2048 num_workers=4 pin_memory=True total_cpu_count=2 prefetch_factor=2)
05-02 02:43:04 I concatenated dataset properties:
05-02 02:43:04 I - mode='index x class' len=50000 root_dataset=Cifar10(split=train)
05-02 02:43:04 I - mode='x class' len=10000 root_dataset=Cifar10(split=val)
05-02 02:43:04 I ------------------
05-02 02:43:04 I BEFORE TRAINING
05-02 02:43:04 I train: 50000 samples
05-02 02:43:04 I val: 10000 samples
05-02 02:43:04 I parameter counts (trainable | frozen)
05-02 02:43:04 I 5,649,994 | 0 | vislstm
05-02 02:43:04 I estimated checkpoint size: 67.7MB
05-02 02:43:04 I estimated weight checkpoint size: 22.5MB
05-02 02:43:04 I estimated optim checkpoint size: 45.1MB
05-02 02:43:04 I estimated size for 1 checkpoints: 22.5MB
05-02 02:43:04 I estimated checkpoint size: 67.7MB
05-02 02:43:04 I estimated weight checkpoint size: 22.5MB
05-02 02:43:04 I estimated optim checkpoint size: 45.1MB
05-02 02:43:04 I estimated size for 81 checkpoints: 0.0B
05-02 02:43:04 I ------------------
05-02 02:43:04 I DatasetStatsCallback
05-02 02:43:04 I ParamCountCallback
05-02 02:43:04 I CopyPreviousConfigCallback
05-02 02:43:04 I CopyPreviousSummaryCallback
05-02 02:43:04 I ProgressCallback(every_n_epochs=1)
05-02 02:43:04 I TrainTimeCallback(every_n_epochs=1)
05-02 02:43:04 I OnlineLossCallback(every_n_epochs=1)
05-02 02:43:04 I LrCallback(every_n_updates=50)
05-02 02:43:04 I FreezerCallback(every_n_updates=50)
05-02 02:43:04 I OnlineLossCallback(every_n_updates=50)
05-02 02:43:04 I OnlineAccuracyCallback(every_n_updates=50)
05-02 02:43:04 I OnlineAccuracyCallback(every_n_epochs=1)
05-02 02:43:04 I CheckpointCallback()
05-02 02:43:04 I CheckpointCallback(every_n_epochs=10)
05-02 02:43:04 I OfflineAccuracyCallback(every_n_epochs=1)
05-02 02:43:04 I ------------------
05-02 02:43:04 I START TRAINING
05-02 02:43:04 I initializing dataloader workers
05-02 02:43:07 I initialized dataloader workers
Traceback (most recent call last):
  File "/home/beknur.kalmakhanbet/vision-lstm/src/main_train.py", line 9, in <module>
    main()
  File "/home/beknur.kalmakhanbet/vision-lstm/src/main_train.py", line 5, in main
    Runner().run()
  File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/runners/runner.py", line 46, in run
    run_managed(
  File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/distributed/run/managed.py", line 44, in run_managed
    _run_managed_singleprocess(accelerator, main)
  File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/distributed/run/managed.py", line 54, in _run_managed_singleprocess
    main(device=device)
  File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/runners/runner.py", line 312, in main
    trainer.train(model)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/kappaprofiler/__init__.py", line 20, in _profile
    return func(*args, **kwargs)
  File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/trainers/base/sgd_trainer.py", line 591, in train
    self._train(
  File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/trainers/base/sgd_trainer.py", line 681, in _train
    losses, update_outputs = self.update(
  File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/trainers/base/sgd_trainer.py", line 861, in update
    losses, outputs = ddp_model(batch, **forward_kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 465, in _fn
    return fn(*args, **kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 1269, in __call__
    return self._torchdynamo_orig_callable(
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 1064, in __call__
    result = self._inner_convert(
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 526, in __call__
    return _compile(
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 924, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 666, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_utils_internal.py", line 87, in wrapper_function
    return function(*args, **kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 699, in _compile_inner
    out_code = transform_code_object(code, transform)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py", line 1322, in transform_code_object
    transformations(instructions, code_options)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 219, in _fn
    return fn(*args, **kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 634, in transform
    tracer.run()
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 2796, in run
    super().run()
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 983, in run
    while self.step():
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 895, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 2987, in RETURN_VALUE
    self._return(inst)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 2972, in _return
    self.output.compile_subgraph(
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 1142, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 1369, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 1416, in call_user_compiler
    return self._call_user_compiler(gm)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 1451, in _call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 1446, in _call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/repro/after_dynamo.py", line 129, in __call__
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/__init__.py", line 2234, in __call__
    return compile_fx(model_, inputs_, config_patches=self.config)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 1521, in compile_fx
    return aot_autograd(
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_dynamo/backends/common.py", line 72, in __call__
    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1071, in aot_module_simplified
    compiled_fn = dispatch_and_compile()
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1056, in dispatch_and_compile
    compiled_fn, _ = create_aot_dispatcher_function(
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 522, in create_aot_dispatcher_function
    return _create_aot_dispatcher_function(
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 623, in _create_aot_dispatcher_function
    fw_metadata = run_functionalized_fw_and_collect_metadata(
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py", line 173, in inner
    flat_f_outs = f(*flat_f_args)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 859, in functional_call
    out = PropagateUnbackedSymInts(mod).run(
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/fx/interpreter.py", line 146, in run
    self.env[node] = self.run_node(node)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/fx/experimental/symbolic_shapes.py", line 5498, in run_node
    result = super().run_node(n)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/fx/interpreter.py", line 203, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/fx/interpreter.py", line 275, in call_function
    return target(*args, **kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/nn/functional.py", line 2900, in layer_norm
    return torch.layer_norm(
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_subclasses/functional_tensor.py", line 534, in __torch_dispatch__
    outs_unwrapped = func._op_dk(
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/utils/_stats.py", line 21, in wrapper
    return fn(*args, **kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py", line 1238, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py", line 1692, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py", line 1348, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py", line 1983, in _dispatch_impl
    op_impl_out = op_impl(self, func, *args, **kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_subclasses/fake_impls.py", line 147, in dispatch_to_op_implementations_dict
    return op_implementations_dict[func](fake_mode, func, *args, **kwargs)
  File "/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/_subclasses/fake_impls.py", line 386, in local_scalar_dense
    raise DataDependentOutputException(func)
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
DataDependentOutputException: aten._local_scalar_dense.default

While executing %input_1 : [num_users=1] = call_function[target=torch.nn.functional.layer_norm](args = (%x_101, (%l_self_modules_model_modules_head_modules_0_normalized_shape_0_,), %l_self_modules_model_modules_head_modules_0_parameters_weight_, %l_self_modules_model_modules_head_modules_0_parameters_bias_, 1e-06), kwargs = {})
Original traceback:
  File "/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/trainers/classification_trainer.py", line 66, in forward
    preds = self.model(x, **forward_kwargs)
  File "/home/beknur.kalmakhanbet/vision-lstm/src/vislstm/models/single/vislstm.py", line 238, in forward
    x = self.head(x)


Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

srun: error: gpu-51: task 0: Exited with exit code 1
