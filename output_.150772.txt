MASTER_ADDR: gpu-55
CUDA_VISIBLE_DEVICES=0,1,2,3
Fri Oct 31 13:39:33 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   27C    P0             50W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   27C    P0             50W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   25C    P0             51W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   25C    P0             50W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
torch: 2.5.1+cu121 cuda: 12.1 cuda available: True
10-31 13:39:42 I initializing rank=3 local_rank=3 nodes=1 hostname=gpu-55 master_addr=gpu-55 master_port=55555 (waiting for all 4 processes to connect)
10-31 13:39:42 I initializing rank=2 local_rank=2 nodes=1 hostname=gpu-55 master_addr=gpu-55 master_port=55555 (waiting for all 4 processes to connect)
10-31 13:39:42 I initializing rank=0 local_rank=0 nodes=1 hostname=gpu-55 master_addr=gpu-55 master_port=55555 (waiting for all 4 processes to connect)
10-31 13:39:42 I initializing rank=1 local_rank=1 nodes=1 hostname=gpu-55 master_addr=gpu-55 master_port=55555 (waiting for all 4 processes to connect)
[rank1]:[W1031 13:39:42.610836969 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W1031 13:39:42.930261081 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W1031 13:39:42.095581107 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W1031 13:39:42.097385678 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
10-31 13:39:43 I initialized process rank=1 local_rank=1 pid=3724623
10-31 13:39:43 I initialized process rank=3 local_rank=3 pid=3724625
10-31 13:39:43 I initialized process rank=0 local_rank=0 pid=3724622
10-31 13:39:43 I initialized process rank=2 local_rank=2 pid=3724624
10-31 13:39:43 I initialized 4 processes
10-31 13:39:43 W disabled cudnn benchmark
10-31 13:39:43 W enabled cudnn deterministic
10-31 13:39:43 I log file: /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/log.txt
10-31 13:39:43 I no seed specified -> using seed=0
10-31 13:39:43 I ------------------
10-31 13:39:43 I initializing wandb (mode=online)
10-31 13:39:43 I logging into wandb (host=https://api.wandb.ai/ rank=0)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
wandb: Currently logged in as: beka-kalmahanbet (ml710_project) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
10-31 13:39:44 I logged into wandb (host=https://api.wandb.ai/)
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/wandb/run-20251031_133944-5ezb8thj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run in1k-lstm-80m16-e400res192-bialter-bilatflat-lr1e3-conv2d3-bias/in1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/beka-kalmahanbet-mbzuai/minLSTM
wandb: üöÄ View run at https://wandb.ai/beka-kalmahanbet-mbzuai/minLSTM/runs/5ezb8thj
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
10-31 13:39:45 I ------------------
10-31 13:39:45 I stage_id: 5ezb8thj
10-31 13:39:45 I python main_train.py --hp src/vislstm/yamls/pretrain/vil/lstm_80M16_e400_bialter_bilatflat_conv2d3_lr1e3_res192_bias.yaml --resume_stage_id 6xt8x4ip --resume_checkpoint latest --num_workers 5
10-31 13:39:45 I ------------------
10-31 13:39:45 I VERSION CHECK
10-31 13:39:45 I executable: /home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/bin/python
10-31 13:39:45 I python version: 3.9.21
10-31 13:39:45 I torch version: 2.5.1+cu121
10-31 13:39:45 I torch.cuda version: 12.1
10-31 13:39:45 I torchvision.version: 0.20.1+cu121
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
10-31 13:39:45 I initialized process rank=3 local_rank=3 pid=3724625 hostname=gpu-55
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
10-31 13:39:45 I initialized process rank=2 local_rank=2 pid=3724624 hostname=gpu-55
10-31 13:39:46 I torchmetrics version: 1.6.2
10-31 13:39:46 I kappaschedules version: 0.0.31
10-31 13:39:46 I kappamodules version: 0.1.76
10-31 13:39:46 I ------------------
10-31 13:39:46 I SYSTEM INFO
10-31 13:39:46 I host name: gpu-55
10-31 13:39:46 I OS: Linux-5.15.161-ql-generic-13.0-14-x86_64-with-glibc2.35
10-31 13:39:46 I OS version: #1 SMP Wed Jun 26 16:19:39 UTC 2024
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
10-31 13:39:46 I initialized process rank=1 local_rank=1 pid=3724623 hostname=gpu-55
10-31 13:39:46 I CUDA version: 12.4
10-31 13:39:46 I current commit hash: 879894a2c4205819466aaff45b583fe3b517c036
fatal: No annotated tags can describe '879894a2c4205819466aaff45b583fe3b517c036'.
However, there were unannotated tags: try --tags.
10-31 13:39:46 I latest git tag: 
10-31 13:39:46 I initialized process rank=0 local_rank=0 pid=3724622 hostname=gpu-55
10-31 13:39:46 I total_cpu_count: 64
10-31 13:39:46 I ------------------
10-31 13:39:46 I STATIC CONFIG
10-31 13:39:46 I account_name: beknur.kalmakhanbet
10-31 13:39:46 I output_path: /home/beknur.kalmakhanbet/save
10-31 13:39:46 I ------------------
10-31 13:39:46 I CLI ARGS
10-31 13:39:46 I hp: src/vislstm/yamls/pretrain/vil/lstm_80M16_e400_bialter_bilatflat_conv2d3_lr1e3_res192_bias.yaml
10-31 13:39:46 I accelerator: gpu
10-31 13:39:46 I num_workers: 5
10-31 13:39:46 I testrun: False
10-31 13:39:46 I minmodelrun: False
10-31 13:39:46 I mindatarun: False
10-31 13:39:46 I mindurationrun: False
10-31 13:39:46 I static_config_uri: static_config.yaml
10-31 13:39:46 I resume_stage_id: 6xt8x4ip
10-31 13:39:46 I resume_checkpoint: latest
10-31 13:39:46 I ------------------
10-31 13:39:46 I DIST CONFIG
10-31 13:39:46 I rank: 0
10-31 13:39:46 I local_rank: 0
10-31 13:39:46 I world_size: 4
10-31 13:39:46 I nodes: 1
10-31 13:39:46 I backend: nccl
10-31 13:39:46 I slurm job id: 150772
10-31 13:39:46 I hostnames: gpu-55
10-31 13:39:46 I ------------------
master_factory_base_path: vislstm
stage_name: in1k
datasets:
  train:
    kind: imagenet1k
    split: train
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: random_resized_crop
        size: 192
        scale:
        - 0.08
        - 1.0
        interpolation: bicubic
      - kind: random_horizontal_flip
      - kind: transforms.three_augment
        blur_sigma:
        - 0.1
        - 2.0
      - kind: color_jitter
        brightness: 0.3
        contrast: 0.3
        saturation: 0.3
        hue: 0.0
      - kind: imagenet1k_norm
    - kind: one_hot_wrapper
    collators:
    - kind: mix_collator
      mixup_alpha: 0.8
      cutmix_alpha: 1.0
      mixup_p: 0.5
      cutmix_p: 0.5
      apply_mode: batch
      lamb_mode: batch
      shuffle_mode: flip
  val:
    kind: imagenet1k
    split: val
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: resize
        size: 192
        interpolation: bicubic
      - kind: center_crop
        size: 192
      - kind: imagenet1k_norm
model:
  kind: models.single.vislstm
  patch_size: 16
  dim: 768
  depth: 24
  bidirectional: false
  alternation: bidirectional
  conv1d_kernel_size: 3
  use_conv2d: true
  bias: true
  pos_embed_mode: learnable
  drop_path_rate: 0.2
  drop_path_decay: false
  mode: classifier
  pooling:
    kind: bilateral
    aggregate: flatten
  optim:
    kind: adamw
    lr: 0.001
    betas:
    - 0.9
    - 0.999
    weight_decay: 0.05
    clip_grad_norm: 1.0
    schedule:
      kind: linear_warmup_cosine_decay_schedule
      warmup_epochs: 5
      end_value: 1.0e-06
    lr_scaler:
      kind: linear_lr_scaler
      divisor: 1024
trainer:
  kind: classification_trainer
  precision: bfloat16
  backup_precision: float16
  max_epochs: 400
  effective_batch_size: 512
  log_every_n_epochs: 1
  use_torch_compile: true
  callbacks:
  - kind: checkpoint_callback
  - kind: checkpoint_callback
    every_n_epochs: 10
    save_weights: false
    save_latest_weights: true
    save_latest_optim: true
  - kind: offline_accuracy_callback
    every_n_epochs: 5
    dataset_key: val
  initializer:
    kind: resume_initializer
    stage_id: 6xt8x4ip
    checkpoint: latest
10-31 13:39:46 I copied unresolved hp to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/hp_unresolved.yaml
10-31 13:39:46 I dumped resolved hp to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/hp_resolved.yaml
10-31 13:39:46 I ------------------
10-31 13:39:46 I training stage 'in1k'
10-31 13:39:46 I using different seeds per process (seed+rank)
10-31 13:39:46 I set seed to 0
10-31 13:39:46 I ------------------
10-31 13:39:46 I initializing datasets
10-31 13:39:46 I initializing train
10-31 13:39:51 I instantiating sample_wrapper x_transform_wrapper
10-31 13:39:51 I instantiating sample_wrapper one_hot_wrapper
10-31 13:39:51 I initializing val
10-31 13:39:53 I instantiating sample_wrapper x_transform_wrapper
10-31 13:39:53 I ------------------
10-31 13:39:53 I initializing trainer
10-31 13:39:53 I using precision: torch.bfloat16 (desired=bfloat16 backup=float16)
10-31 13:39:53 I main_sampler: DistributedSampler(num_repeats=1, shuffle=True)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
10-31 13:39:53 I loaded checkpoint from trainer_state_dict: {'epoch': 150, 'update': 375300, 'sample': 192153600, 'callback_state_dicts': [None, None, None]}
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer_ckpt = torch.load(self._get_trainer_ckpt_file())
10-31 13:39:53 I ------------------
10-31 13:39:53 I creating model
10-31 13:39:53 I input_shape: (3, 192, 192)
10-31 13:39:53 I pos_embed.is_learnable=True
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
10-31 13:39:54 I drop_path_rate: 0.2
10-31 13:39:54 I model:
VisLSTM(
  (pooling): Bilateral(aggregate=flatten)
  (patch_embed): VitPatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_embed): VitPosEmbed2d()
  (xlstm): xLSTMBlockStack(
    (blocks): ModuleList(
      (0-23): 24 x mLSTMBlock(
        (drop_path1): DropPath(drop_prob=0.200)
        (xlstm_norm): LayerNorm()
        (xlstm): mLSTMLayer(
          (proj_up): Linear(in_features=768, out_features=3072, bias=True)
          (conv1d): SequenceConv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
          (conv_act_fn): SiLU()
          (mlstm_cell): mLSTMCell(
            (linear_h): FeedForward(
              (linear1): Linear(in_features=1536, out_features=8, bias=True)
              (activation): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=8, out_features=1536, bias=True)
            )
            (linear_i): FeedForward(
              (linear1): Linear(in_features=1536, out_features=8, bias=True)
              (activation): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=8, out_features=1536, bias=True)
            )
            (linear_f): FeedForward(
              (linear1): Linear(in_features=1536, out_features=8, bias=True)
              (activation): ReLU()
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=8, out_features=1536, bias=True)
            )
          )
          (ogate_act_fn): SiLU()
          (proj_down): Linear(in_features=1536, out_features=768, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layerscale): Identity()
        )
      )
    )
    (post_blocks_norm): LayerNorm()
  )
  (head): Sequential(
    (0): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
    (1): Linear(in_features=1536, out_features=1000, bias=True)
  )
)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
10-31 13:39:54 I vislstm initialize optimizer
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
10-31 13:39:54 I base lr: 1e-3
10-31 13:39:54 I scaled lr: 5e-4
10-31 13:39:54 I lr_scaler=LinearLrScaler(divisor=1024)
10-31 13:39:54 I lr_scale_factor=512
10-31 13:39:54 I exclude_bias_from_wd=True exclude_norm_from_wd=True param_group_modifiers=[WeightDecayByNameModifier(name=pos_embed.embed)]
10-31 13:39:54 I using 2 param groups:
10-31 13:39:54 I len(params)=218
10-31 13:39:54 I weight_decay=0.0 len(params)=295
10-31 13:39:54 I ------------------
10-31 13:39:54 I loading trainer/model state for resuming
10-31 13:39:54 I loading state from checkpoint 6xt8x4ip/in1k/latest
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  trainer.load_state_dict(torch.load(ckpt_uri))
10-31 13:39:54 I loaded trainer checkpoint /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/trainer cp=latest.th
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
10-31 13:39:56 I loaded weights of vislstm from /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest model.th
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
/home/beknur.kalmakhanbet/vision-lstm/src/ksuit/initializers/resume_initializer.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(ckpt_uri, map_location=model.device)
10-31 13:39:58 I loaded optimizer of vislstm from /home/beknur.kalmakhanbet/save/in1k/6xt8x4ip/checkpoints/vislstm cp=latest optim.th
10-31 13:39:58 I added default DatasetStatsCallback
10-31 13:39:58 I added default ParamCountCallback
10-31 13:39:58 I added default CopyPreviousConfigCallback
10-31 13:39:58 I added default CopyPreviousSummaryCallback
10-31 13:39:58 I added default ProgressCallback(every_n_epochs=1)
10-31 13:39:58 I added default TrainTimeCallback(every_n_epochs=1)
10-31 13:39:58 I added default OnlineLossCallback(every_n_epochs=1)
10-31 13:39:58 I added default LrCallback(every_n_updates=50)
10-31 13:39:58 I added default FreezerCallback(every_n_updates=50)
10-31 13:39:58 I added default OnlineLossCallback(every_n_updates=50)
10-31 13:39:58 I replacing BatchNorm layers with SyncBatchNorm
10-31 13:39:58 I wrapping model with torch.compile
10-31 13:40:00 I ------------------
10-31 13:40:00 I PREPARE TRAINER
10-31 13:40:00 I calculating batch_size and accumulation_steps (effective_batch_size=512)
10-31 13:40:00 I torch.compile is used -> automatic batchsize not supported
10-31 13:40:00 I train_batches per epoch: 2502 (world_size=4 batch_size=128)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=144, _num_blocks=1, _inner_embedding_dim=1536)
10-31 13:40:00 I initializing dataloader
10-31 13:40:00 I OfflineAccuracyCallback(every_n_epochs=5) registered InterleavedSamplerConfig(every_n_epochs=5) dataset_mode='x class'
10-31 13:40:00 I created dataloader (batch_size=128 num_workers=5 pin_memory=True total_cpu_count=64 prefetch_factor=2)
10-31 13:40:00 I concatenated dataset properties:
10-31 13:40:00 I - mode='index x class' len=1281167 root_dataset=<vislstm.datasets.imagenet1k.Imagenet1k object at 0x14c3e706de80>
10-31 13:40:00 I - mode='x class' len=50000 root_dataset=<vislstm.datasets.imagenet1k.Imagenet1k object at 0x14c1a1fc9f10>
10-31 13:40:00 I ------------------
10-31 13:40:00 I BEFORE TRAINING
10-31 13:40:00 I train: 1281167 samples
10-31 13:40:00 I val: 50000 samples
10-31 13:40:00 I parameter counts (trainable | frozen)
10-31 13:40:00 I 89,592,616 | 0 | vislstm
10-31 13:40:00 I estimated checkpoint size: 1.0GB
10-31 13:40:00 I estimated weight checkpoint size: 358.3MB
10-31 13:40:00 I estimated optim checkpoint size: 716.7MB
10-31 13:40:00 I estimated size for 1 checkpoints: 358.3MB
10-31 13:40:00 I estimated checkpoint size: 1.0GB
10-31 13:40:00 I estimated weight checkpoint size: 358.3MB
10-31 13:40:00 I estimated optim checkpoint size: 716.7MB
10-31 13:40:00 I estimated size for 41 checkpoints: 0.0B
10-31 13:40:00 I ------------------
10-31 13:40:00 I DatasetStatsCallback
10-31 13:40:00 I ParamCountCallback
10-31 13:40:00 I CopyPreviousConfigCallback
10-31 13:40:00 I CopyPreviousSummaryCallback
10-31 13:40:00 I ProgressCallback(every_n_epochs=1)
10-31 13:40:00 I TrainTimeCallback(every_n_epochs=1)
10-31 13:40:00 I OnlineLossCallback(every_n_epochs=1)
10-31 13:40:00 I LrCallback(every_n_updates=50)
10-31 13:40:00 I FreezerCallback(every_n_updates=50)
10-31 13:40:00 I OnlineLossCallback(every_n_updates=50)
10-31 13:40:00 I OnlineAccuracyCallback(every_n_updates=50)
10-31 13:40:00 I OnlineAccuracyCallback(every_n_epochs=1)
10-31 13:40:00 I CheckpointCallback()
10-31 13:40:00 I CheckpointCallback(every_n_epochs=10)
10-31 13:40:00 I OfflineAccuracyCallback(every_n_epochs=5)
10-31 13:40:00 I ------------------
10-31 13:40:00 I START TRAINING
10-31 13:40:00 I initializing dataloader workers
10-31 13:40:00 I initialized dataloader workers
[rank1]:W1031 13:40:01.425740 3724623 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W1031 13:40:01.428704 3724622 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank3]:W1031 13:40:01.441197 3724625 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:W1031 13:40:01.466630 3724624 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/beknur.kalmakhanbet/miniconda3/envs/minLSTM/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
10-31 13:41:22 I 0 unused parameters
10-31 13:50:17 I ------------------
10-31 13:50:17 I Epoch 151/400 (E151_U377802_S193434624)
10-31 13:50:17 I ETA: 10.31 14.07.16 estimated_duration: 00:27:15.93 time_since_last_log: 00:10:17.56 time_per_update: 00:00:00.00 
10-31 13:50:17 I data=[0.00, 0.00, 0.00, 0.00] update=[0.24, 0.24, 0.24, 0.24]
10-31 13:50:17 I loss/online/main/E1: 3.2207071781158447
10-31 13:50:17 I loss/online/total/E1: 3.2207071781158447
10-31 13:50:17 I accuracy1/online/main/E1: 0.482262
10-31 13:59:12 I ------------------
10-31 13:59:12 I Epoch 152/400 (E152_U380304_S194715648)
10-31 13:59:12 I ETA: 10.31 14.13.51 estimated_duration: 00:23:33.56 time_since_last_log: 00:08:54.95 time_per_update: 00:00:00.21 
10-31 13:59:12 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 13:59:12 I loss/online/main/E1: 3.2146406173706055
10-31 13:59:12 I loss/online/total/E1: 3.2146406173706055
10-31 13:59:12 I accuracy1/online/main/E1: 0.483876
10-31 14:08:09 I ------------------
10-31 14:08:09 I Epoch 153/400 (E153_U382806_S195996672)
10-31 14:08:09 I ETA: 10.31 14.37.10 estimated_duration: 00:46:53.00 time_since_last_log: 00:08:56.66 time_per_update: 00:00:00.21 
10-31 14:08:09 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 14:08:09 I loss/online/main/E1: 3.219964027404785
10-31 14:08:09 I loss/online/total/E1: 3.219964027404785
10-31 14:08:09 I accuracy1/online/main/E1: 0.484182
10-31 14:17:07 I ------------------
10-31 14:17:07 I Epoch 154/400 (E154_U385308_S197277696)
10-31 14:17:07 I ETA: 10.31 15.00.15 estimated_duration: 01:09:57.88 time_since_last_log: 00:08:58.09 time_per_update: 00:00:00.21 
10-31 14:17:07 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 14:17:07 I loss/online/main/E1: 3.2096524238586426
10-31 14:17:07 I loss/online/total/E1: 3.2096524238586426
10-31 14:17:07 I accuracy1/online/main/E1: 0.483688
10-31 14:26:02 I ------------------
10-31 14:26:02 I Epoch 155/400 (E155_U387810_S198558720)
10-31 14:26:02 I ETA: 10.31 15.22.54 estimated_duration: 01:32:37.20 time_since_last_log: 00:08:55.17 time_per_update: 00:00:00.21 
10-31 14:26:02 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 14:26:02 I loss/online/main/E1: 3.2174534797668457
10-31 14:26:02 I loss/online/total/E1: 3.2174534797668457
10-31 14:26:02 I accuracy1/online/main/E1: 0.485011
10-31 14:26:19 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 14:26:19 I accuracy1/val/main: 0.717360
10-31 14:26:19 I loss/val/main: 1.1875
10-31 14:35:15 I ------------------
10-31 14:35:15 I Epoch 156/400 (E156_U390312_S199839744)
10-31 14:35:15 I ETA: 10.31 15.46.01 estimated_duration: 01:55:44.00 time_since_last_log: 00:09:12.66 time_per_update: 00:00:00.22 
10-31 14:35:15 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 14:35:15 I loss/online/main/E1: 3.20546293258667
10-31 14:35:15 I loss/online/total/E1: 3.20546293258667
10-31 14:35:15 I accuracy1/online/main/E1: 0.485837
10-31 14:44:10 I ------------------
10-31 14:44:10 I Epoch 157/400 (E157_U392814_S201120768)
10-31 14:44:10 I ETA: 10.31 16.08.05 estimated_duration: 02:17:48.17 time_since_last_log: 00:08:55.12 time_per_update: 00:00:00.21 
10-31 14:44:10 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 14:44:10 I loss/online/main/E1: 3.205686092376709
10-31 14:44:10 I loss/online/total/E1: 3.205686092376709
10-31 14:44:10 I accuracy1/online/main/E1: 0.485275
10-31 14:53:07 I ------------------
10-31 14:53:07 I Epoch 158/400 (E158_U395316_S202401792)
10-31 14:53:07 I ETA: 10.31 16.29.57 estimated_duration: 02:39:39.85 time_since_last_log: 00:08:56.84 time_per_update: 00:00:00.21 
10-31 14:53:07 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 14:53:07 I loss/online/main/E1: 3.196523904800415
10-31 14:53:07 I loss/online/total/E1: 3.196523904800415
10-31 14:53:07 I accuracy1/online/main/E1: 0.488226
10-31 15:02:02 I ------------------
10-31 15:02:02 I Epoch 159/400 (E159_U397818_S203682816)
10-31 15:02:02 I ETA: 10.31 16.51.27 estimated_duration: 03:01:10.10 time_since_last_log: 00:08:54.93 time_per_update: 00:00:00.21 
10-31 15:02:02 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 15:02:02 I loss/online/main/E1: 3.1912527084350586
10-31 15:02:02 I loss/online/total/E1: 3.1912527084350586
10-31 15:02:02 I accuracy1/online/main/E1: 0.488522
10-31 15:10:57 I ------------------
10-31 15:10:57 I Epoch 160/400 (E160_U400320_S204963840)
10-31 15:10:57 I ETA: 10.31 17.12.42 estimated_duration: 03:22:24.79 time_since_last_log: 00:08:55.20 time_per_update: 00:00:00.21 
10-31 15:10:57 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 15:10:57 I loss/online/main/E1: 3.1795763969421387
10-31 15:10:57 I loss/online/total/E1: 3.1795763969421387
10-31 15:10:57 I accuracy1/online/main/E1: 0.489238
10-31 15:10:58 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/vislstm cp=latest model.th
10-31 15:10:59 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/vislstm cp=latest optim.th
10-31 15:10:59 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/trainer cp=latest.th
10-31 15:11:16 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 15:11:16 I accuracy1/val/main: 0.719600
10-31 15:11:16 I loss/val/main: 1.15625
10-31 15:20:11 I ------------------
10-31 15:20:11 I Epoch 161/400 (E161_U402822_S206244864)
10-31 15:20:11 I ETA: 10.31 17.34.29 estimated_duration: 03:44:11.43 time_since_last_log: 00:09:14.40 time_per_update: 00:00:00.22 
10-31 15:20:11 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 15:20:11 I loss/online/main/E1: 3.185035228729248
10-31 15:20:11 I loss/online/total/E1: 3.185035228729248
10-31 15:20:11 I accuracy1/online/main/E1: 0.488714
10-31 15:29:08 I ------------------
10-31 15:29:08 I Epoch 162/400 (E162_U405324_S207525888)
10-31 15:29:08 I ETA: 10.31 17.55.16 estimated_duration: 04:04:58.54 time_since_last_log: 00:08:56.92 time_per_update: 00:00:00.21 
10-31 15:29:08 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 15:29:08 I loss/online/main/E1: 3.188593864440918
10-31 15:29:08 I loss/online/total/E1: 3.188593864440918
10-31 15:29:08 I accuracy1/online/main/E1: 0.489174
10-31 15:38:04 I ------------------
10-31 15:38:04 I Epoch 163/400 (E163_U407826_S208806912)
10-31 15:38:04 I ETA: 10.31 18.15.46 estimated_duration: 04:25:28.46 time_since_last_log: 00:08:56.20 time_per_update: 00:00:00.21 
10-31 15:38:04 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 15:38:04 I loss/online/main/E1: 3.172579765319824
10-31 15:38:04 I loss/online/total/E1: 3.172579765319824
10-31 15:38:04 I accuracy1/online/main/E1: 0.490659
10-31 15:47:00 I ------------------
10-31 15:47:00 I Epoch 164/400 (E164_U410328_S210087936)
10-31 15:47:00 I ETA: 10.31 18.35.58 estimated_duration: 04:45:41.33 time_since_last_log: 00:08:55.40 time_per_update: 00:00:00.21 
10-31 15:47:00 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 15:47:00 I loss/online/main/E1: 3.166457414627075
10-31 15:47:00 I loss/online/total/E1: 3.166457414627075
10-31 15:47:00 I accuracy1/online/main/E1: 0.491501
10-31 15:55:56 I ------------------
10-31 15:55:56 I Epoch 165/400 (E165_U412830_S211368960)
10-31 15:55:56 I ETA: 10.31 18.55.58 estimated_duration: 05:05:41.19 time_since_last_log: 00:08:56.13 time_per_update: 00:00:00.21 
10-31 15:55:56 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 15:55:56 I loss/online/main/E1: 3.170034170150757
10-31 15:55:56 I loss/online/total/E1: 3.170034170150757
10-31 15:55:56 I accuracy1/online/main/E1: 0.490864
10-31 15:56:13 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 15:56:13 I accuracy1/val/main: 0.719580
10-31 15:56:13 I loss/val/main: 1.1328125
10-31 16:05:09 I ------------------
10-31 16:05:09 I Epoch 166/400 (E166_U415332_S212649984)
10-31 16:05:09 I ETA: 10.31 19.16.25 estimated_duration: 05:26:08.12 time_since_last_log: 00:09:13.34 time_per_update: 00:00:00.22 
10-31 16:05:09 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 16:05:09 I loss/online/main/E1: 3.159065008163452
10-31 16:05:09 I loss/online/total/E1: 3.159065008163452
10-31 16:05:09 I accuracy1/online/main/E1: 0.493353
10-31 16:14:06 I ------------------
10-31 16:14:06 I Epoch 167/400 (E167_U417834_S213931008)
10-31 16:14:06 I ETA: 10.31 19.35.58 estimated_duration: 05:45:40.96 time_since_last_log: 00:08:56.99 time_per_update: 00:00:00.21 
10-31 16:14:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 16:14:06 I loss/online/main/E1: 3.1541709899902344
10-31 16:14:06 I loss/online/total/E1: 3.1541709899902344
10-31 16:14:06 I accuracy1/online/main/E1: 0.495332
10-31 16:23:01 I ------------------
10-31 16:23:01 I Epoch 168/400 (E168_U420336_S215212032)
10-31 16:23:01 I ETA: 10.31 19.55.13 estimated_duration: 06:04:55.47 time_since_last_log: 00:08:55.19 time_per_update: 00:00:00.21 
10-31 16:23:01 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 16:23:01 I loss/online/main/E1: 3.142873525619507
10-31 16:23:01 I loss/online/total/E1: 3.142873525619507
10-31 16:23:01 I accuracy1/online/main/E1: 0.495258
10-31 16:31:58 I ------------------
10-31 16:31:58 I Epoch 169/400 (E169_U422838_S216493056)
10-31 16:31:58 I ETA: 10.31 20.14.16 estimated_duration: 06:23:58.71 time_since_last_log: 00:08:56.24 time_per_update: 00:00:00.21 
10-31 16:31:58 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 16:31:58 I loss/online/main/E1: 3.1338417530059814
10-31 16:31:58 I loss/online/total/E1: 3.1338417530059814
10-31 16:31:58 I accuracy1/online/main/E1: 0.495758
10-31 16:40:56 I ------------------
10-31 16:40:56 I Epoch 170/400 (E170_U425340_S217774080)
10-31 16:40:56 I ETA: 10.31 20.33.11 estimated_duration: 06:42:53.50 time_since_last_log: 00:08:58.38 time_per_update: 00:00:00.21 
10-31 16:40:56 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 16:40:56 I loss/online/main/E1: 3.139617681503296
10-31 16:40:56 I loss/online/total/E1: 3.139617681503296
10-31 16:40:56 I accuracy1/online/main/E1: 0.495438
10-31 16:40:57 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/vislstm cp=latest model.th
10-31 16:40:58 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/vislstm cp=latest optim.th
10-31 16:40:58 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/trainer cp=latest.th
10-31 16:41:15 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 16:41:15 I accuracy1/val/main: 0.722740
10-31 16:41:15 I loss/val/main: 1.140625
10-31 16:50:11 I ------------------
10-31 16:50:11 I Epoch 171/400 (E171_U427842_S219055104)
10-31 16:50:11 I ETA: 10.31 20.52.30 estimated_duration: 07:02:13.16 time_since_last_log: 00:09:14.67 time_per_update: 00:00:00.22 
10-31 16:50:11 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 16:50:11 I loss/online/main/E1: 3.1368842124938965
10-31 16:50:11 I loss/online/total/E1: 3.1368842124938965
10-31 16:50:11 I accuracy1/online/main/E1: 0.496386
10-31 16:59:07 I ------------------
10-31 16:59:07 I Epoch 172/400 (E172_U430344_S220336128)
10-31 16:59:07 I ETA: 10.31 21.10.54 estimated_duration: 07:20:37.01 time_since_last_log: 00:08:56.57 time_per_update: 00:00:00.21 
10-31 16:59:07 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 16:59:07 I loss/online/main/E1: 3.1219170093536377
10-31 16:59:07 I loss/online/total/E1: 3.1219170093536377
10-31 16:59:07 I accuracy1/online/main/E1: 0.499241
10-31 17:08:05 I ------------------
10-31 17:08:05 I Epoch 173/400 (E173_U432846_S221617152)
10-31 17:08:05 I ETA: 10.31 21.29.07 estimated_duration: 07:38:50.04 time_since_last_log: 00:08:57.44 time_per_update: 00:00:00.21 
10-31 17:08:05 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 17:08:05 I loss/online/main/E1: 3.136554718017578
10-31 17:08:05 I loss/online/total/E1: 3.136554718017578
10-31 17:08:05 I accuracy1/online/main/E1: 0.496537
10-31 17:17:01 I ------------------
10-31 17:17:01 I Epoch 174/400 (E174_U435348_S222898176)
10-31 17:17:01 I ETA: 10.31 21.47.04 estimated_duration: 07:56:46.99 time_since_last_log: 00:08:55.94 time_per_update: 00:00:00.21 
10-31 17:17:01 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 17:17:01 I loss/online/main/E1: 3.1204166412353516
10-31 17:17:01 I loss/online/total/E1: 3.1204166412353516
10-31 17:17:01 I accuracy1/online/main/E1: 0.498333
10-31 17:25:57 I ------------------
10-31 17:25:57 I Epoch 175/400 (E175_U437850_S224179200)
10-31 17:25:57 I ETA: 10.31 22.04.50 estimated_duration: 08:14:32.62 time_since_last_log: 00:08:56.40 time_per_update: 00:00:00.21 
10-31 17:25:57 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 17:25:57 I loss/online/main/E1: 3.1283364295959473
10-31 17:25:57 I loss/online/total/E1: 3.1283364295959473
10-31 17:25:57 I accuracy1/online/main/E1: 0.497797
10-31 17:26:14 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 17:26:14 I accuracy1/val/main: 0.725040
10-31 17:26:14 I loss/val/main: 1.1171875
10-31 17:35:09 I ------------------
10-31 17:35:09 I Epoch 176/400 (E176_U440352_S225460224)
10-31 17:35:09 I ETA: 10.31 22.22.59 estimated_duration: 08:32:42.34 time_since_last_log: 00:09:12.31 time_per_update: 00:00:00.22 
10-31 17:35:09 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 17:35:09 I loss/online/main/E1: 3.1171553134918213
10-31 17:35:09 I loss/online/total/E1: 3.1171553134918213
10-31 17:35:09 I accuracy1/online/main/E1: 0.500244
10-31 17:44:06 I ------------------
10-31 17:44:06 I Epoch 177/400 (E177_U442854_S226741248)
10-31 17:44:06 I ETA: 10.31 22.40.21 estimated_duration: 08:50:03.85 time_since_last_log: 00:08:56.51 time_per_update: 00:00:00.21 
10-31 17:44:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 17:44:06 I loss/online/main/E1: 3.109642267227173
10-31 17:44:06 I loss/online/total/E1: 3.109642267227173
10-31 17:44:06 I accuracy1/online/main/E1: 0.501561
10-31 17:53:01 I ------------------
10-31 17:53:01 I Epoch 178/400 (E178_U445356_S228022272)
10-31 17:53:01 I ETA: 10.31 22.57.28 estimated_duration: 09:07:11.06 time_since_last_log: 00:08:55.38 time_per_update: 00:00:00.21 
10-31 17:53:01 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 17:53:01 I loss/online/main/E1: 3.1160964965820312
10-31 17:53:01 I loss/online/total/E1: 3.1160964965820312
10-31 17:53:01 I accuracy1/online/main/E1: 0.500914
10-31 18:01:57 I ------------------
10-31 18:01:57 I Epoch 179/400 (E179_U447858_S229303296)
10-31 18:01:57 I ETA: 10.31 23.14.25 estimated_duration: 09:24:08.24 time_since_last_log: 00:08:56.06 time_per_update: 00:00:00.21 
10-31 18:01:57 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 18:01:57 I loss/online/main/E1: 3.099188804626465
10-31 18:01:57 I loss/online/total/E1: 3.099188804626465
10-31 18:01:57 I accuracy1/online/main/E1: 0.503670
10-31 18:10:55 I ------------------
10-31 18:10:55 I Epoch 180/400 (E180_U450360_S230584320)
10-31 18:10:55 I ETA: 10.31 23.31.14 estimated_duration: 09:40:56.85 time_since_last_log: 00:08:57.31 time_per_update: 00:00:00.21 
10-31 18:10:55 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 18:10:55 I loss/online/main/E1: 3.1078290939331055
10-31 18:10:55 I loss/online/total/E1: 3.1078290939331055
10-31 18:10:55 I accuracy1/online/main/E1: 0.501166
10-31 18:10:56 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/vislstm cp=latest model.th
10-31 18:10:57 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/vislstm cp=latest optim.th
10-31 18:10:57 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/trainer cp=latest.th
10-31 18:11:14 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 18:11:14 I accuracy1/val/main: 0.731360
10-31 18:11:14 I loss/val/main: 1.09375
10-31 18:20:12 I ------------------
10-31 18:20:12 I Epoch 181/400 (E181_U452862_S231865344)
10-31 18:20:12 I ETA: 10.31 23.48.35 estimated_duration: 09:58:17.56 time_since_last_log: 00:09:16.85 time_per_update: 00:00:00.22 
10-31 18:20:12 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 18:20:12 I loss/online/main/E1: 3.0854077339172363
10-31 18:20:12 I loss/online/total/E1: 3.0854077339172363
10-31 18:20:12 I accuracy1/online/main/E1: 0.505845
10-31 18:29:08 I ------------------
10-31 18:29:08 I Epoch 182/400 (E182_U455364_S233146368)
10-31 18:29:08 I ETA: 11.01 00.05.00 estimated_duration: 10:14:42.76 time_since_last_log: 00:08:56.89 time_per_update: 00:00:00.21 
10-31 18:29:08 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 18:29:08 I loss/online/main/E1: 3.09706974029541
10-31 18:29:08 I loss/online/total/E1: 3.09706974029541
10-31 18:29:08 I accuracy1/online/main/E1: 0.503915
10-31 18:38:05 I ------------------
10-31 18:38:05 I Epoch 183/400 (E183_U457866_S234427392)
10-31 18:38:05 I ETA: 11.01 00.21.14 estimated_duration: 10:30:56.89 time_since_last_log: 00:08:56.77 time_per_update: 00:00:00.21 
10-31 18:38:05 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 18:38:05 I loss/online/main/E1: 3.081376075744629
10-31 18:38:05 I loss/online/total/E1: 3.081376075744629
10-31 18:38:05 I accuracy1/online/main/E1: 0.506785
10-31 18:47:03 I ------------------
10-31 18:47:03 I Epoch 184/400 (E184_U460368_S235708416)
10-31 18:47:03 I ETA: 11.01 00.37.19 estimated_duration: 10:47:02.23 time_since_last_log: 00:08:57.62 time_per_update: 00:00:00.21 
10-31 18:47:03 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 18:47:03 I loss/online/main/E1: 3.0737414360046387
10-31 18:47:03 I loss/online/total/E1: 3.0737414360046387
10-31 18:47:03 I accuracy1/online/main/E1: 0.507430
10-31 18:56:00 I ------------------
10-31 18:56:00 I Epoch 185/400 (E185_U462870_S236989440)
10-31 18:56:00 I ETA: 11.01 00.53.14 estimated_duration: 11:02:56.80 time_since_last_log: 00:08:57.50 time_per_update: 00:00:00.21 
10-31 18:56:00 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 18:56:00 I loss/online/main/E1: 3.0705811977386475
10-31 18:56:00 I loss/online/total/E1: 3.0705811977386475
10-31 18:56:00 I accuracy1/online/main/E1: 0.506919
10-31 18:56:17 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 18:56:17 I accuracy1/val/main: 0.731360
10-31 18:56:17 I loss/val/main: 1.09375
10-31 19:05:14 I ------------------
10-31 19:05:14 I Epoch 186/400 (E186_U465372_S238270464)
10-31 19:05:14 I ETA: 11.01 01.09.32 estimated_duration: 11:19:15.29 time_since_last_log: 00:09:13.37 time_per_update: 00:00:00.22 
10-31 19:05:14 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 19:05:14 I loss/online/main/E1: 3.073810577392578
10-31 19:05:14 I loss/online/total/E1: 3.073810577392578
10-31 19:05:14 I accuracy1/online/main/E1: 0.506755
10-31 19:14:10 I ------------------
10-31 19:14:10 I Epoch 187/400 (E187_U467874_S239551488)
10-31 19:14:10 I ETA: 11.01 01.25.03 estimated_duration: 11:34:45.88 time_since_last_log: 00:08:55.95 time_per_update: 00:00:00.21 
10-31 19:14:10 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 19:14:10 I loss/online/main/E1: 3.0681164264678955
10-31 19:14:10 I loss/online/total/E1: 3.0681164264678955
10-31 19:14:10 I accuracy1/online/main/E1: 0.506651
10-31 19:23:06 I ------------------
10-31 19:23:06 I Epoch 188/400 (E188_U470376_S240832512)
10-31 19:23:06 I ETA: 11.01 01.40.25 estimated_duration: 11:50:07.62 time_since_last_log: 00:08:56.46 time_per_update: 00:00:00.21 
10-31 19:23:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 19:23:06 I loss/online/main/E1: 3.0650134086608887
10-31 19:23:06 I loss/online/total/E1: 3.0650134086608887
10-31 19:23:06 I accuracy1/online/main/E1: 0.509625
10-31 19:32:04 I ------------------
10-31 19:32:04 I Epoch 189/400 (E189_U472878_S242113536)
10-31 19:32:04 I ETA: 11.01 01.55.39 estimated_duration: 12:05:21.66 time_since_last_log: 00:08:57.46 time_per_update: 00:00:00.21 
10-31 19:32:04 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 19:32:04 I loss/online/main/E1: 3.064222812652588
10-31 19:32:04 I loss/online/total/E1: 3.064222812652588
10-31 19:32:04 I accuracy1/online/main/E1: 0.509841
10-31 19:40:59 I ------------------
10-31 19:40:59 I Epoch 190/400 (E190_U475380_S243394560)
10-31 19:40:59 I ETA: 11.01 02.10.39 estimated_duration: 12:20:22.27 time_since_last_log: 00:08:55.68 time_per_update: 00:00:00.21 
10-31 19:40:59 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 19:40:59 I loss/online/main/E1: 3.049208164215088
10-31 19:40:59 I loss/online/total/E1: 3.049208164215088
10-31 19:40:59 I accuracy1/online/main/E1: 0.512300
10-31 19:41:00 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/vislstm cp=latest model.th
10-31 19:41:02 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/vislstm cp=latest optim.th
10-31 19:41:02 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/trainer cp=latest.th
10-31 19:41:19 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 19:41:19 I accuracy1/val/main: 0.733760
10-31 19:41:19 I loss/val/main: 1.09375
10-31 19:50:14 I ------------------
10-31 19:50:14 I Epoch 191/400 (E191_U477882_S244675584)
10-31 19:50:14 I ETA: 11.01 02.26.11 estimated_duration: 12:35:53.69 time_since_last_log: 00:09:14.86 time_per_update: 00:00:00.22 
10-31 19:50:14 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 19:50:14 I loss/online/main/E1: 3.0510923862457275
10-31 19:50:14 I loss/online/total/E1: 3.0510923862457275
10-31 19:50:14 I accuracy1/online/main/E1: 0.510545
10-31 19:59:10 I ------------------
10-31 19:59:10 I Epoch 192/400 (E192_U480384_S245956608)
10-31 19:59:10 I ETA: 11.01 02.40.52 estimated_duration: 12:50:34.84 time_since_last_log: 00:08:55.47 time_per_update: 00:00:00.21 
10-31 19:59:10 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 19:59:10 I loss/online/main/E1: 3.036175489425659
10-31 19:59:10 I loss/online/total/E1: 3.036175489425659
10-31 19:59:10 I accuracy1/online/main/E1: 0.513410
10-31 20:08:06 I ------------------
10-31 20:08:06 I Epoch 193/400 (E193_U482886_S247237632)
10-31 20:08:06 I ETA: 11.01 02.55.26 estimated_duration: 13:05:08.79 time_since_last_log: 00:08:56.42 time_per_update: 00:00:00.21 
10-31 20:08:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 20:08:06 I loss/online/main/E1: 3.055356025695801
10-31 20:08:06 I loss/online/total/E1: 3.055356025695801
10-31 20:08:06 I accuracy1/online/main/E1: 0.511140
10-31 20:17:02 I ------------------
10-31 20:17:02 I Epoch 194/400 (E194_U485388_S248518656)
10-31 20:17:02 I ETA: 11.01 03.09.49 estimated_duration: 13:19:31.78 time_since_last_log: 00:08:55.50 time_per_update: 00:00:00.21 
10-31 20:17:02 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 20:17:02 I loss/online/main/E1: 3.0275800228118896
10-31 20:17:02 I loss/online/total/E1: 3.0275800228118896
10-31 20:17:02 I accuracy1/online/main/E1: 0.514907
10-31 20:25:57 I ------------------
10-31 20:25:57 I Epoch 195/400 (E195_U487890_S249799680)
10-31 20:25:57 I ETA: 11.01 03.24.03 estimated_duration: 13:33:45.88 time_since_last_log: 00:08:55.50 time_per_update: 00:00:00.21 
10-31 20:25:57 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 20:25:57 I loss/online/main/E1: 3.0255117416381836
10-31 20:25:57 I loss/online/total/E1: 3.0255117416381836
10-31 20:25:57 I accuracy1/online/main/E1: 0.515563
10-31 20:26:14 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 20:26:14 I accuracy1/val/main: 0.736940
10-31 20:26:14 I loss/val/main: 1.0625
10-31 20:35:10 I ------------------
10-31 20:35:10 I Epoch 196/400 (E196_U490392_S251080704)
10-31 20:35:10 I ETA: 11.01 03.38.44 estimated_duration: 13:48:27.27 time_since_last_log: 00:09:13.12 time_per_update: 00:00:00.22 
10-31 20:35:10 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 20:35:10 I loss/online/main/E1: 3.033613920211792
10-31 20:35:10 I loss/online/total/E1: 3.033613920211792
10-31 20:35:10 I accuracy1/online/main/E1: 0.513621
10-31 20:44:09 I ------------------
10-31 20:44:09 I Epoch 197/400 (E197_U492894_S252361728)
10-31 20:44:09 I ETA: 11.01 03.52.47 estimated_duration: 14:02:30.28 time_since_last_log: 00:08:58.68 time_per_update: 00:00:00.21 
10-31 20:44:09 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 20:44:09 I loss/online/main/E1: 3.0288655757904053
10-31 20:44:09 I loss/online/total/E1: 3.0288655757904053
10-31 20:44:09 I accuracy1/online/main/E1: 0.515182
10-31 20:53:04 I ------------------
10-31 20:53:04 I Epoch 198/400 (E198_U495396_S253642752)
10-31 20:53:04 I ETA: 11.01 04.06.35 estimated_duration: 14:16:17.98 time_since_last_log: 00:08:55.35 time_per_update: 00:00:00.21 
10-31 20:53:04 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 20:53:04 I loss/online/main/E1: 3.022843599319458
10-31 20:53:04 I loss/online/total/E1: 3.022843599319458
10-31 20:53:04 I accuracy1/online/main/E1: 0.515562
10-31 21:02:01 I ------------------
10-31 21:02:01 I Epoch 199/400 (E199_U497898_S254923776)
10-31 21:02:01 I ETA: 11.01 04.20.16 estimated_duration: 14:29:59.26 time_since_last_log: 00:08:56.32 time_per_update: 00:00:00.21 
10-31 21:02:01 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 21:02:01 I loss/online/main/E1: 3.013489246368408
10-31 21:02:01 I loss/online/total/E1: 3.013489246368408
10-31 21:02:01 I accuracy1/online/main/E1: 0.517133
10-31 21:10:58 I ------------------
10-31 21:10:58 I Epoch 200/400 (E200_U500400_S256204800)
10-31 21:10:58 I ETA: 11.01 04.33.51 estimated_duration: 14:43:33.93 time_since_last_log: 00:08:57.13 time_per_update: 00:00:00.21 
10-31 21:10:58 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 21:10:58 I loss/online/main/E1: 2.997997522354126
10-31 21:10:58 I loss/online/total/E1: 2.997997522354126
10-31 21:10:58 I accuracy1/online/main/E1: 0.520253
10-31 21:10:59 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/vislstm cp=latest model.th
10-31 21:11:00 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/vislstm cp=latest optim.th
10-31 21:11:00 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/trainer cp=latest.th
10-31 21:11:17 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 21:11:17 I accuracy1/val/main: 0.740620
10-31 21:11:17 I loss/val/main: 1.0546875
10-31 21:20:13 I ------------------
10-31 21:20:13 I Epoch 201/400 (E201_U502902_S257485824)
10-31 21:20:13 I ETA: 11.01 04.47.54 estimated_duration: 14:57:36.99 time_since_last_log: 00:09:15.45 time_per_update: 00:00:00.22 
10-31 21:20:13 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 21:20:13 I loss/online/main/E1: 3.007295846939087
10-31 21:20:13 I loss/online/total/E1: 3.007295846939087
10-31 21:20:13 I accuracy1/online/main/E1: 0.518976
10-31 21:29:09 I ------------------
10-31 21:29:09 I Epoch 202/400 (E202_U505404_S258766848)
10-31 21:29:09 I ETA: 11.01 05.01.09 estimated_duration: 15:10:52.16 time_since_last_log: 00:08:55.55 time_per_update: 00:00:00.21 
10-31 21:29:09 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 21:29:09 I loss/online/main/E1: 3.0100464820861816
10-31 21:29:09 I loss/online/total/E1: 3.0100464820861816
10-31 21:29:09 I accuracy1/online/main/E1: 0.517164
10-31 21:38:04 I ------------------
10-31 21:38:04 I Epoch 203/400 (E203_U507906_S260047872)
10-31 21:38:04 I ETA: 11.01 05.14.17 estimated_duration: 15:23:59.43 time_since_last_log: 00:08:55.54 time_per_update: 00:00:00.21 
10-31 21:38:04 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 21:38:04 I loss/online/main/E1: 2.9988608360290527
10-31 21:38:04 I loss/online/total/E1: 2.9988608360290527
10-31 21:38:04 I accuracy1/online/main/E1: 0.519322
10-31 21:47:02 I ------------------
10-31 21:47:02 I Epoch 204/400 (E204_U510408_S261328896)
10-31 21:47:02 I ETA: 11.01 05.27.21 estimated_duration: 15:37:03.85 time_since_last_log: 00:08:58.03 time_per_update: 00:00:00.21 
10-31 21:47:02 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 21:47:02 I loss/online/main/E1: 2.9957873821258545
10-31 21:47:02 I loss/online/total/E1: 2.9957873821258545
10-31 21:47:02 I accuracy1/online/main/E1: 0.520016
10-31 21:55:59 I ------------------
10-31 21:55:59 I Epoch 205/400 (E205_U512910_S262609920)
10-31 21:55:59 I ETA: 11.01 05.40.15 estimated_duration: 15:49:57.39 time_since_last_log: 00:08:56.40 time_per_update: 00:00:00.21 
10-31 21:55:59 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 21:55:59 I loss/online/main/E1: 2.98873233795166
10-31 21:55:59 I loss/online/total/E1: 2.98873233795166
10-31 21:55:59 I accuracy1/online/main/E1: 0.520606
10-31 21:56:16 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 21:56:16 I accuracy1/val/main: 0.742980
10-31 21:56:16 I loss/val/main: 1.0625
10-31 22:05:12 I ------------------
10-31 22:05:12 I Epoch 206/400 (E206_U515412_S263890944)
10-31 22:05:12 I ETA: 11.01 05.53.34 estimated_duration: 16:03:16.88 time_since_last_log: 00:09:13.61 time_per_update: 00:00:00.22 
10-31 22:05:12 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 22:05:12 I loss/online/main/E1: 2.9883174896240234
10-31 22:05:12 I loss/online/total/E1: 2.9883174896240234
10-31 22:05:12 I accuracy1/online/main/E1: 0.521479
10-31 22:14:08 I ------------------
10-31 22:14:08 I Epoch 207/400 (E207_U517914_S265171968)
10-31 22:14:08 I ETA: 11.01 06.06.11 estimated_duration: 16:15:54.21 time_since_last_log: 00:08:55.86 time_per_update: 00:00:00.21 
10-31 22:14:08 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 22:14:08 I loss/online/main/E1: 2.975433826446533
10-31 22:14:08 I loss/online/total/E1: 2.975433826446533
10-31 22:14:08 I accuracy1/online/main/E1: 0.522698
10-31 22:23:06 I ------------------
10-31 22:23:06 I Epoch 208/400 (E208_U520416_S266452992)
10-31 22:23:06 I ETA: 11.01 06.18.45 estimated_duration: 16:28:27.63 time_since_last_log: 00:08:57.62 time_per_update: 00:00:00.21 
10-31 22:23:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 22:23:06 I loss/online/main/E1: 2.981017589569092
10-31 22:23:06 I loss/online/total/E1: 2.981017589569092
10-31 22:23:06 I accuracy1/online/main/E1: 0.522950
10-31 22:32:02 I ------------------
10-31 22:32:02 I Epoch 209/400 (E209_U522918_S267734016)
10-31 22:32:02 I ETA: 11.01 06.31.08 estimated_duration: 16:40:50.38 time_since_last_log: 00:08:55.83 time_per_update: 00:00:00.21 
10-31 22:32:02 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 22:32:02 I loss/online/main/E1: 2.9773478507995605
10-31 22:32:02 I loss/online/total/E1: 2.9773478507995605
10-31 22:32:02 I accuracy1/online/main/E1: 0.522818
10-31 22:40:57 I ------------------
10-31 22:40:57 I Epoch 210/400 (E210_U525420_S269015040)
10-31 22:40:57 I ETA: 11.01 06.43.23 estimated_duration: 16:53:05.87 time_since_last_log: 00:08:55.76 time_per_update: 00:00:00.21 
10-31 22:40:57 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 22:40:57 I loss/online/main/E1: 2.9688565731048584
10-31 22:40:57 I loss/online/total/E1: 2.9688565731048584
10-31 22:40:57 I accuracy1/online/main/E1: 0.525178
10-31 22:40:58 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/vislstm cp=latest model.th
10-31 22:41:00 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/vislstm cp=latest optim.th
10-31 22:41:00 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/trainer cp=latest.th
10-31 22:41:17 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 22:41:17 I accuracy1/val/main: 0.743960
10-31 22:41:17 I loss/val/main: 1.046875
10-31 22:50:13 I ------------------
10-31 22:50:13 I Epoch 211/400 (E211_U527922_S270296064)
10-31 22:50:13 I ETA: 11.01 06.56.09 estimated_duration: 17:05:51.44 time_since_last_log: 00:09:15.27 time_per_update: 00:00:00.22 
10-31 22:50:13 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 22:50:13 I loss/online/main/E1: 2.9638662338256836
10-31 22:50:13 I loss/online/total/E1: 2.9638662338256836
10-31 22:50:13 I accuracy1/online/main/E1: 0.524832
10-31 22:59:10 I ------------------
10-31 22:59:10 I Epoch 212/400 (E212_U530424_S271577088)
10-31 22:59:10 I ETA: 11.01 07.08.13 estimated_duration: 17:17:56.12 time_since_last_log: 00:08:57.48 time_per_update: 00:00:00.21 
10-31 22:59:10 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 22:59:10 I loss/online/main/E1: 2.963010549545288
10-31 22:59:10 I loss/online/total/E1: 2.963010549545288
10-31 22:59:10 I accuracy1/online/main/E1: 0.526329
10-31 23:08:06 I ------------------
10-31 23:08:06 I Epoch 213/400 (E213_U532926_S272858112)
10-31 23:08:06 I ETA: 11.01 07.20.09 estimated_duration: 17:29:51.60 time_since_last_log: 00:08:56.23 time_per_update: 00:00:00.21 
10-31 23:08:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 23:08:06 I loss/online/main/E1: 2.9509706497192383
10-31 23:08:06 I loss/online/total/E1: 2.9509706497192383
10-31 23:08:06 I accuracy1/online/main/E1: 0.526576
10-31 23:17:03 I ------------------
10-31 23:17:03 I Epoch 214/400 (E214_U535428_S274139136)
10-31 23:17:03 I ETA: 11.01 07.31.58 estimated_duration: 17:41:40.97 time_since_last_log: 00:08:56.56 time_per_update: 00:00:00.21 
10-31 23:17:03 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 23:17:03 I loss/online/main/E1: 2.9501707553863525
10-31 23:17:03 I loss/online/total/E1: 2.9501707553863525
10-31 23:17:03 I accuracy1/online/main/E1: 0.528279
10-31 23:25:59 I ------------------
10-31 23:25:59 I Epoch 215/400 (E215_U537930_S275420160)
10-31 23:25:59 I ETA: 11.01 07.43.40 estimated_duration: 17:53:22.81 time_since_last_log: 00:08:56.07 time_per_update: 00:00:00.21 
10-31 23:25:59 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 23:25:59 I loss/online/main/E1: 2.9386587142944336
10-31 23:25:59 I loss/online/total/E1: 2.9386587142944336
10-31 23:25:59 I accuracy1/online/main/E1: 0.530030
10-31 23:26:16 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
10-31 23:26:16 I accuracy1/val/main: 0.745900
10-31 23:26:16 I loss/val/main: 1.046875
10-31 23:35:14 I ------------------
10-31 23:35:14 I Epoch 216/400 (E216_U540432_S276701184)
10-31 23:35:14 I ETA: 11.01 07.55.51 estimated_duration: 18:05:33.65 time_since_last_log: 00:09:15.22 time_per_update: 00:00:00.22 
10-31 23:35:14 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 23:35:14 I loss/online/main/E1: 2.931180953979492
10-31 23:35:14 I loss/online/total/E1: 2.931180953979492
10-31 23:35:14 I accuracy1/online/main/E1: 0.531156
10-31 23:44:10 I ------------------
10-31 23:44:10 I Epoch 217/400 (E217_U542934_S277982208)
10-31 23:44:10 I ETA: 11.01 08.07.18 estimated_duration: 18:17:01.33 time_since_last_log: 00:08:55.51 time_per_update: 00:00:00.21 
10-31 23:44:10 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 23:44:10 I loss/online/main/E1: 2.9303464889526367
10-31 23:44:10 I loss/online/total/E1: 2.9303464889526367
10-31 23:44:10 I accuracy1/online/main/E1: 0.531625
10-31 23:53:06 I ------------------
10-31 23:53:06 I Epoch 218/400 (E218_U545436_S279263232)
10-31 23:53:06 I ETA: 11.01 08.18.41 estimated_duration: 18:28:24.11 time_since_last_log: 00:08:56.30 time_per_update: 00:00:00.21 
10-31 23:53:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
10-31 23:53:06 I loss/online/main/E1: 2.9208569526672363
10-31 23:53:06 I loss/online/total/E1: 2.9208569526672363
10-31 23:53:06 I accuracy1/online/main/E1: 0.532224
11-01 00:02:02 I ------------------
11-01 00:02:02 I Epoch 219/400 (E219_U547938_S280544256)
11-01 00:02:02 I ETA: 11.01 08.29.58 estimated_duration: 18:39:40.40 time_since_last_log: 00:08:56.18 time_per_update: 00:00:00.21 
11-01 00:02:02 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-01 00:02:02 I loss/online/main/E1: 2.924792528152466
11-01 00:02:02 I loss/online/total/E1: 2.924792528152466
11-01 00:02:02 I accuracy1/online/main/E1: 0.530948
11-01 00:10:59 I ------------------
11-01 00:10:59 I Epoch 220/400 (E220_U550440_S281825280)
11-01 00:10:59 I ETA: 11.01 08.41.09 estimated_duration: 18:50:52.05 time_since_last_log: 00:08:57.02 time_per_update: 00:00:00.21 
11-01 00:10:59 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-01 00:10:59 I loss/online/main/E1: 2.9230222702026367
11-01 00:10:59 I loss/online/total/E1: 2.9230222702026367
11-01 00:10:59 I accuracy1/online/main/E1: 0.532380
11-01 00:11:00 I saved vislstm to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/vislstm cp=latest model.th
11-01 00:11:02 I saved vislstm optim to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/vislstm cp=latest optim.th
11-01 00:11:02 I saved trainer state_dict to /home/beknur.kalmakhanbet/save/in1k/5ezb8thj/checkpoints/trainer cp=latest.th
11-01 00:11:19 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-01 00:11:19 I accuracy1/val/main: 0.747440
11-01 00:11:19 I loss/val/main: 1.03125
11-01 00:20:15 I ------------------
11-01 00:20:15 I Epoch 221/400 (E221_U552942_S283106304)
11-01 00:20:15 I ETA: 11.01 08.52.48 estimated_duration: 19:02:31.20 time_since_last_log: 00:09:15.55 time_per_update: 00:00:00.22 
11-01 00:20:15 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-01 00:20:15 I loss/online/main/E1: 2.913557767868042
11-01 00:20:15 I loss/online/total/E1: 2.913557767868042
11-01 00:20:15 I accuracy1/online/main/E1: 0.533041
11-01 00:29:11 I ------------------
11-01 00:29:11 I Epoch 222/400 (E222_U555444_S284387328)
11-01 00:29:11 I ETA: 11.01 09.03.47 estimated_duration: 19:13:29.56 time_since_last_log: 00:08:56.46 time_per_update: 00:00:00.21 
11-01 00:29:11 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-01 00:29:11 I loss/online/main/E1: 2.914895534515381
11-01 00:29:11 I loss/online/total/E1: 2.914895534515381
11-01 00:29:11 I accuracy1/online/main/E1: 0.533254
11-01 00:38:08 I ------------------
11-01 00:38:08 I Epoch 223/400 (E223_U557946_S285668352)
11-01 00:38:08 I ETA: 11.01 09.14.39 estimated_duration: 19:24:22.01 time_since_last_log: 00:08:56.47 time_per_update: 00:00:00.21 
11-01 00:38:08 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-01 00:38:08 I loss/online/main/E1: 2.9148411750793457
11-01 00:38:08 I loss/online/total/E1: 2.9148411750793457
11-01 00:38:08 I accuracy1/online/main/E1: 0.532141
11-01 00:47:03 I ------------------
11-01 00:47:03 I Epoch 224/400 (E224_U560448_S286949376)
11-01 00:47:03 I ETA: 11.01 09.25.24 estimated_duration: 19:35:07.18 time_since_last_log: 00:08:55.67 time_per_update: 00:00:00.21 
11-01 00:47:03 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-01 00:47:03 I loss/online/main/E1: 2.8991498947143555
11-01 00:47:03 I loss/online/total/E1: 2.8991498947143555
11-01 00:47:03 I accuracy1/online/main/E1: 0.536473
11-01 00:55:59 I ------------------
11-01 00:55:59 I Epoch 225/400 (E225_U562950_S288230400)
11-01 00:55:59 I ETA: 11.01 09.36.04 estimated_duration: 19:45:47.08 time_since_last_log: 00:08:55.95 time_per_update: 00:00:00.21 
11-01 00:55:59 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-01 00:55:59 I loss/online/main/E1: 2.8950443267822266
11-01 00:55:59 I loss/online/total/E1: 2.8950443267822266
11-01 00:55:59 I accuracy1/online/main/E1: 0.537159
11-01 00:56:16 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.17
11-01 00:56:17 I accuracy1/val/main: 0.749600
11-01 00:56:17 I loss/val/main: 1.015625
11-01 01:05:12 I ------------------
11-01 01:05:12 I Epoch 226/400 (E226_U565452_S289511424)
11-01 01:05:12 I ETA: 11.01 09.47.08 estimated_duration: 19:56:51.18 time_since_last_log: 00:09:12.80 time_per_update: 00:00:00.22 
11-01 01:05:12 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-01 01:05:12 I loss/online/main/E1: 2.8910014629364014
11-01 01:05:12 I loss/online/total/E1: 2.8910014629364014
11-01 01:05:12 I accuracy1/online/main/E1: 0.537668
11-01 01:14:10 I ------------------
11-01 01:14:10 I Epoch 227/400 (E227_U567954_S290792448)
11-01 01:14:10 I ETA: 11.01 09.57.40 estimated_duration: 20:07:22.68 time_since_last_log: 00:08:57.67 time_per_update: 00:00:00.21 
11-01 01:14:10 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-01 01:14:10 I loss/online/main/E1: 2.8950629234313965
11-01 01:14:10 I loss/online/total/E1: 2.8950629234313965
11-01 01:14:10 I accuracy1/online/main/E1: 0.535518
11-01 01:23:06 I ------------------
11-01 01:23:06 I Epoch 228/400 (E228_U570456_S292073472)
11-01 01:23:06 I ETA: 11.01 10.08.02 estimated_duration: 20:17:45.28 time_since_last_log: 00:08:55.77 time_per_update: 00:00:00.21 
11-01 01:23:06 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-01 01:23:06 I loss/online/main/E1: 2.885434627532959
11-01 01:23:06 I loss/online/total/E1: 2.885434627532959
11-01 01:23:06 I accuracy1/online/main/E1: 0.537305
11-01 01:32:02 I ------------------
11-01 01:32:02 I Epoch 229/400 (E229_U572958_S293354496)
11-01 01:32:02 I ETA: 11.01 10.18.21 estimated_duration: 20:28:03.36 time_since_last_log: 00:08:56.30 time_per_update: 00:00:00.21 
11-01 01:32:02 I data=[0.00, 0.00, 0.00, 0.00] update=[0.21, 0.21, 0.21, 0.21]
11-01 01:32:02 I loss/online/main/E1: 2.8858470916748047
11-01 01:32:02 I loss/online/total/E1: 2.8858470916748047
11-01 01:32:02 I accuracy1/online/main/E1: 0.537760
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd-gpu-55: error: *** STEP 150772.0 ON gpu-55 CANCELLED AT 2025-11-01T01:39:31 DUE TO TIME LIMIT ***
slurmstepd-gpu-55: error: *** JOB 150772 ON gpu-55 CANCELLED AT 2025-11-01T01:39:31 DUE TO TIME LIMIT ***
